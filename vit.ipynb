{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer (ViT) from Scratch\n",
        "\n",
        "Let's dive into one of the most significant contribution in the field of Computer Vision: the Vision Transformer (ViT)."
      ],
      "metadata": {
        "id": "k16_mZAoLVFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import inspect\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange"
      ],
      "metadata": {
        "id": "rkiYGpqxL6sD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Ad5RdiaSL-g7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Transformer Encoder"
      ],
      "metadata": {
        "id": "jyjmDRK9Lx8C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3yhgGeHnLObY"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    The Attention Layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head, dropout=0.1):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert n_embed % n_head == 0, \"n_embed must be divisible by n_head\"\n",
        "        self.n_embed= n_embed\n",
        "        self.n_head = n_head\n",
        "        self.d_head = n_embed // n_head\n",
        "        # query, key, value projections in a single batch\n",
        "        self.c_attn= nn.Linear(n_embed, 3 * n_embed)\n",
        "        # output projection\n",
        "        self.o_proj= nn.Linear(n_embed, n_embed)\n",
        "        # regularization\n",
        "        self.dropout= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C= x.size() # batch_size, sequence length, embedding dim (d_model)\n",
        "        assert C == self.n_embed, \"Input embedding dimension must match model embedding dimension\"\n",
        "        # 1. calculate query, key, values for all heads\n",
        "        qkv= self.c_attn(x)\n",
        "        q, k, v= qkv.split(self.n_embed, dim=2) # q,k,v -> (B, T, C)\n",
        "        # 2. reshape for Multi-Head Attention\n",
        "        q= q.view(B, T, self.n_head, self.d_head).transpose(1, 2) # q,k,v view   -> (B, T, nh, dh)\n",
        "        k= k.view(B, T, self.n_head, self.d_head).transpose(1, 2) # q,k,v transp -> (B, nh, T, dh)\n",
        "        v= v.view(B, T, self.n_head, self.d_head).transpose(1, 2)\n",
        "        # 3. Attention - the 'scaled dot product'\n",
        "        attn= (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.d_head))\n",
        "        # normalize Attention scores\n",
        "        attn= F.softmax(attn, dim=-1)\n",
        "        attn= self.dropout(attn)\n",
        "        # 4. compute Attention output\n",
        "        y= attn @ v # (B, nh, T, dh)\n",
        "        # 5. concatenate multi-head outputs -- re-assembly all head outputs side by side\n",
        "        y= y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        # 6. output projection\n",
        "        return self.o_proj(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    The Feed Forward Network (FFN).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, d_ff, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.ffn= nn.Sequential(\n",
        "            nn.Linear(n_embed, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(d_ff, n_embed),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.ffn(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Yf-v31TtL_1Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    The Ecoder Block (pre-normalization version).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head, d_ff, dropout=0.1) -> None:\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.ln_1= nn.LayerNorm(n_embed)\n",
        "        self.attn= MultiHeadSelfAttention(n_embed, n_head, dropout)\n",
        "        self.dropout1= nn.Dropout(p=dropout)\n",
        "        self.ln_2= nn.LayerNorm(n_embed)\n",
        "        self.ffn = FeedForward(n_embed, d_ff, dropout)\n",
        "        self.dropout2= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm = self.ln_1(x)\n",
        "        x= x + self.dropout1(self.attn(x_norm))\n",
        "        x_norm = self.ln_2(x)\n",
        "        x= x + self.dropout2(self.ffn(x_norm))\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "KBERV_gyL_4T"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the Attention Layer and Feed Forward Network in place, we can assemble a Transformer Encoder. The Transformer Encoder is essentially a stack of N Encoder Blocks.\n",
        "\n",
        "Remember, Transformers are like Legos â€” the input dimension is the same as the output dimension, so you can stack as many blocks as you want (or as your memory allows)."
      ],
      "metadata": {
        "id": "riOivS7dUSEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Transformer Encoder is essentially a stack of N Encoder Blocks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed=512, n_layer=6, n_head=8, d_ff=2048, dropout=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.transformer= nn.ModuleList([\n",
        "            EncoderBlock(n_embed, n_head, d_ff, dropout) for _ in range(n_layer)\n",
        "        ])\n",
        "        self.ln_final= nn.LayerNorm(n_embed)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.transformer:\n",
        "            x= layer(x)\n",
        "\n",
        "        return self.ln_final(x)\n"
      ],
      "metadata": {
        "id": "c5pTN4cwL_7P"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= TransformerEncoder().to(device)\n",
        "data= torch.randn(16, 128, 512).to(device)\n",
        "model.eval()\n",
        "model(data).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P29NIa7JMABN",
        "outputId": "d71597fa-ecf1-430c-a705-e4eb931fe580"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 128, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the final ViT\n",
        "\n",
        "We mainly need to add 3 components:\n",
        "\n",
        "- Converting the image into patches, and then vectors.\n",
        "- Add positional embedding.\n",
        "- Add the CLS token.\n",
        "\n",
        "We need to check that we are correctly splitting the image into a number of patches that is an integer. In other words, we need to check that image_height and image_width are divisible by patch_dimension.\n",
        "\n",
        "Next step is to convert the patch into embeddings. Remember that here an image has C = 3 dimensions. We need to unfold this dimension, and compress each patch of dimension patch_size x patch_size x c.\n",
        "\n",
        "Then we need to define the CLS token and the positional embedding. The CLS Token is useful to represent the whole image into a single vector, and the positional embedding is what helps the model to have spatial awareness of the tokens. They are both learned parameters (randomly initialized).\n",
        "\n",
        "Finally, we just have to define the transformer layer that we have defined before, and add a classification head."
      ],
      "metadata": {
        "id": "z32LfnrKVStB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Initializes a Vision Transformer (ViT) model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_size, patch_size, channels, num_classes, pool='cls',\n",
        "                 n_embed=512, n_layer=6, n_head=8, d_ff=2048, dropout=0.1):\n",
        "        super(ViT, self).__init__()\n",
        "        image_height, image_width= self.pair(image_size)\n",
        "        patch_height, patch_width= self.pair(patch_size)\n",
        "        # ensure that the image dimensions are divisible by the patch sizes\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        # calculate the number of patches and the dimension of each patch\n",
        "        num_patches= (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim= channels * patch_height * patch_width\n",
        "        # ensure the pooling strategy is valid\n",
        "        assert pool in {'cls', 'mean'}, 'Pool type must be either cls (cls token) or mean (mean pooling).'\n",
        "        # pooling strategy (CLS token or mean of patches)\n",
        "        self.pool= pool\n",
        "\n",
        "        self.patch_embed= nn.Sequential(\n",
        "            # unfold the image into patches of shape (batch_size, num_patches, patch_dim)\n",
        "            Rearrange('b c (h ph) (w pw) -> b (h w) (ph pw c)', ph=patch_height, pw=patch_width),\n",
        "            nn.LayerNorm(patch_dim),       # normalize each patch\n",
        "            nn.Linear(patch_dim, n_embed), # project patches to embedding dimension\n",
        "            nn.LayerNorm(n_embed),         # normalize the embedding\n",
        "        ) # embedding shape (batch_size, num_patches, n_embed)\n",
        "\n",
        "        # define CLS token and positional embeddings\n",
        "        self.cls_token= nn.Parameter(torch.randn(1, 1, n_embed)) # learnable class (CLS) token\n",
        "        self.pos_embed= nn.Parameter(torch.randn(1, num_patches +1, n_embed))\n",
        "        self.dropout= nn.Dropout(p=dropout)\n",
        "\n",
        "        # define the transformer encoder\n",
        "        self.encoder= TransformerEncoder(n_embed, n_layer, n_head, d_ff, dropout)\n",
        "\n",
        "        # identity layer (no change to the tensor)\n",
        "        self.latent_space= nn.Identity()\n",
        "        # classification head\n",
        "        self.lm_head= nn.Linear(n_embed, num_classes)\n",
        "\n",
        "        # initialize parameters with Glorot / fan_avg\n",
        "        for p in self.parameters():\n",
        "            if p.dim()> 1:\n",
        "                nn.init.xavier_normal_(p)\n",
        "\n",
        "\n",
        "    def pair(self, x):\n",
        "        \"\"\"\n",
        "        Utility function: Converts a single value into a tuple of two values.\n",
        "        If x is already a tuple, it is returned as is.\n",
        "        \"\"\"\n",
        "\n",
        "        return x if isinstance(x, tuple) else (x, x)\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        x= self.patch_embed(img)\n",
        "        B, P, C= x.size() # (batch_size, num_patches, n_embed)\n",
        "        # repeat class token (CLS) for each image in the batch\n",
        "        cls_token= repeat(self.cls_token, '1 1 d -> b 1 d', b=B)\n",
        "        # concatenate class token (CLS) with patch embeddings\n",
        "        x= torch.cat((cls_token, x), dim=1)\n",
        "        # add positional embedding to the input\n",
        "        x += self.pos_embed[:, :(P + 1)]\n",
        "        x= self.dropout(x)\n",
        "\n",
        "        # forward the the transformer encoder\n",
        "        x= self.encoder(x) # (batch_size, num_patches + 1, n_embed)\n",
        "\n",
        "        # extract class token and feature map\n",
        "        cls_token= x[:, 0]\n",
        "        feature_map= x[:, 1:]\n",
        "        # apply pooling operation: 'cls' token or mean of patches\n",
        "        pool_output= cls_token if self.pool == 'cls' else feature_map.mean(dim=1)\n",
        "\n",
        "        # apply the identity transformation (no change to the tensor)\n",
        "        pool_output= self.latent_space(pool_output)\n",
        "        # forward the classifier\n",
        "        logits= self.lm_head(pool_output)\n",
        "\n",
        "        # return CLS token, patch embeddings, and classification results\n",
        "        return cls_token, feature_map, logits\n"
      ],
      "metadata": {
        "id": "NCHgRrjhMAET"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward pass:** We have initialized all the components of our ViT, now we just have to call them in the right order for the forward pass.\n",
        "\n",
        "- We first convert the input image into patches, and unfold each patch into a vector.\n",
        "\n",
        "- Then we repeat CLS tokens (along the batch dimension), and we concatenate it on the dimension 1, which is the sequence length. Indeed we learn the parameters for one vector, but it needs to be concatenated to each image, this is why we expand one dimension.\n",
        "\n",
        "- Then we add the position embedding to each token.\n",
        "\n",
        "Next we apply the Transformer Encoder. We then mainly use it to build an output containing 3 things:\n",
        "\n",
        "- The CLS Token (a single vector representation of the image).\n",
        "\n",
        "- The Feature Map (A vectorized representation of each patch of the image)\n",
        "\n",
        "- Classification Head Logits (Optional): This is used in the case of classification task. Note that Vision Transformer can be trained with different tasks, but classification is the task that was originally used."
      ],
      "metadata": {
        "id": "ekVSF73glJv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ViT Large hyperparameters config ---\n",
        "image_size= 224\n",
        "patch_size= 16\n",
        "channels=3\n",
        "num_classes= 1000\n",
        "pool='cls'\n",
        "n_embed= 1024\n",
        "n_layer= 24\n",
        "n_head= 16\n",
        "d_ff= 4 * n_embed\n",
        "\n",
        "model= ViT(image_size, patch_size, channels, num_classes, pool,\n",
        "           n_embed, n_layer, n_head, d_ff).to(device)\n",
        "\n",
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}\\n')\n",
        "\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWCBAOGDMAHW",
        "outputId": "4c8c6161-1e02-4dc9-b4a0-28d905caafac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 304330216\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViT(\n",
              "  (patch_embed): Sequential(\n",
              "    (0): Rearrange('b c (h ph) (w pw) -> b (h w) (ph pw c)', ph=16, pw=16)\n",
              "    (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): Linear(in_features=768, out_features=1024, bias=True)\n",
              "    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (encoder): TransformerEncoder(\n",
              "    (transformer): ModuleList(\n",
              "      (0-23): 24 x EncoderBlock(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): MultiHeadSelfAttention(\n",
              "          (c_attn): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (ffn): FeedForward(\n",
              "          (ffn): Sequential(\n",
              "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.1, inplace=False)\n",
              "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (ln_final): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (latent_space): Identity()\n",
              "  (lm_head): Linear(in_features=1024, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img= [np.random.randn(3, 224, 224) for _ in range(32)]\n",
        "img= torch.tensor(np.array(img), dtype=torch.float32).to(device)\n",
        "\n",
        "model.eval()\n",
        "cls_token, feature_map, logits= model(img)\n",
        "\n",
        "print(f'CLS Token Shape: {cls_token.shape}')\n",
        "print(f'Feature Map Shape: {feature_map.shape}')\n",
        "print(f'Classification Head Logits Shape: {logits.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIHUmqOO3SH8",
        "outputId": "16b6853e-7d1f-4348-cb71-9817a682db80"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLS Token Shape: torch.Size([32, 1024])\n",
            "Feature Map Shape: torch.Size([32, 196, 1024])\n",
            "Classification Head Logits Shape: torch.Size([32, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://towardsdatascience.com/how-to-train-a-vision-transformer-vit-from-scratch-f26641f26af2"
      ],
      "metadata": {
        "id": "u1_Wj-sY4C1O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the ViT model from scratch\n",
        "\n",
        "TODO"
      ],
      "metadata": {
        "id": "2kmHw50E5sYo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qg7bvUacPwOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4bNAYseXPwbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hRh_7GlxPwf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w8kL9CbzPwjG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}