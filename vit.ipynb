{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer (ViT) from Scratch\n",
        "\n",
        "Here, one of the most significant contribution in the field of Computer Vision: the Vision Transformer (ViT).\n",
        "\n",
        "With Self-Attention, each part of the image can \"talk\" to every other part, instantly understanding its relationship to the whole. ViTs treat an image as a collection of \"words\" — not individual pixels, but meaningful chunks, like the different elements in a painting. These chunks are called patches. The magic of Self-Attention lets the model understand the relationships between these patches.\n",
        "\n",
        "CNNs, like detectives, focus on local details, meticulously examining each pixel and its immediate surroundings. They excel at recognizing simple patterns and shapes. ViTs, on the other hand, act like art critics, taking in the whole picture, understanding the relationships between elements, and interpreting the artist's intent. They excel at capturing complex relationships and dependencies across the image.\n",
        "\n",
        "- **Accuracy:** In many cases, ViTs achieve higher accuracy than CNNs, especially on tasks that require understanding complex relationships or recognizing nuanced features.\n",
        "- **Scalability:** ViTs exhibit faster neural scaling laws. This means they become more accurate as the model size increases, while CNNs tend to plateau in performance."
      ],
      "metadata": {
        "id": "k16_mZAoLVFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import inspect\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange"
      ],
      "metadata": {
        "id": "rkiYGpqxL6sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Ad5RdiaSL-g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Transformer Encoder"
      ],
      "metadata": {
        "id": "jyjmDRK9Lx8C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yhgGeHnLObY"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    The Attention Layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head, dropout=0.1) -> None:\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert n_embed % n_head == 0, \"n_embed must be divisible by n_head\"\n",
        "        self.n_embed= n_embed\n",
        "        self.n_head = n_head\n",
        "        self.d_head = n_embed // n_head\n",
        "        # query, key, value projections in a single batch\n",
        "        self.c_attn= nn.Linear(n_embed, 3 * n_embed)\n",
        "        # output projection\n",
        "        self.o_proj= nn.Linear(n_embed, n_embed)\n",
        "        # regularization\n",
        "        self.dropout= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C= x.size() # batch_size, sequence length, embedding dim (d_model)\n",
        "        assert C == self.n_embed, \"Input embedding dimension must match model embedding dimension\"\n",
        "        # 1. calculate query, key, values for all heads\n",
        "        qkv= self.c_attn(x)\n",
        "        q, k, v= qkv.split(self.n_embed, dim=2) # q,k,v -> (B, T, C)\n",
        "        # 2. reshape for Multi-Head Attention\n",
        "        q= q.view(B, T, self.n_head, self.d_head).transpose(1, 2) # q,k,v view   -> (B, T, nh, dh)\n",
        "        k= k.view(B, T, self.n_head, self.d_head).transpose(1, 2) # q,k,v transp -> (B, nh, T, dh)\n",
        "        v= v.view(B, T, self.n_head, self.d_head).transpose(1, 2)\n",
        "        # 3. Attention - the 'scaled dot product'\n",
        "        attn= (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.d_head))\n",
        "        # normalize Attention scores\n",
        "        attn= F.softmax(attn, dim=-1)\n",
        "        attn= self.dropout(attn)\n",
        "        # 4. compute Attention output\n",
        "        y= attn @ v # (B, nh, T, dh)\n",
        "        # 5. concatenate multi-head outputs -- re-assembly all head outputs side by side\n",
        "        y= y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        # 6. output projection\n",
        "        return self.o_proj(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    The Feed Forward Network (FFN).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, d_ff, dropout=0.1) -> None:\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.ffn= nn.Sequential(\n",
        "            nn.Linear(n_embed, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(d_ff, n_embed),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.ffn(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Yf-v31TtL_1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    The Ecoder Block (pre-normalization version).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head, d_ff, dropout=0.1) -> None:\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.ln_1= nn.LayerNorm(n_embed)\n",
        "        self.attn= MultiHeadSelfAttention(n_embed, n_head, dropout)\n",
        "        self.dropout1= nn.Dropout(p=dropout)\n",
        "        self.ln_2= nn.LayerNorm(n_embed)\n",
        "        self.ffn = FeedForward(n_embed, d_ff, dropout)\n",
        "        self.dropout2= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm= self.ln_1(x)\n",
        "        x= x + self.dropout1(self.attn(x_norm))\n",
        "        x_norm= self.ln_2(x)\n",
        "        x= x + self.dropout2(self.ffn(x_norm))\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "KBERV_gyL_4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the Attention Layer and Feed Forward Network in place, we can assemble a Transformer Encoder. The Transformer Encoder is essentially a stack of N Encoder Blocks. Remember, Transformers are like Legos — the input dimension is the same as the output dimension, so you can stack as many blocks as you want (or as your memory allows)."
      ],
      "metadata": {
        "id": "riOivS7dUSEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Transformer Encoder is essentially a stack of N Encoder Blocks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed=512, n_layer=6, n_head=8, d_ff=2048, dropout=0.1) -> None:\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.transformer= nn.ModuleList([\n",
        "            EncoderBlock(n_embed, n_head, d_ff, dropout) for _ in range(n_layer)\n",
        "        ])\n",
        "        self.ln_final= nn.LayerNorm(n_embed)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.transformer:\n",
        "            x= layer(x)\n",
        "\n",
        "        return self.ln_final(x)\n"
      ],
      "metadata": {
        "id": "c5pTN4cwL_7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= TransformerEncoder().to(device)\n",
        "data= torch.randn(16, 128, 512).to(device)\n",
        "model.eval()\n",
        "model(data).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P29NIa7JMABN",
        "outputId": "4ec61491-d4ca-4e7a-9e7f-6c959acdab10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 128, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the final ViT\n",
        "\n",
        "ViTs start by dividing the image into a grid of smaller, rectangular pieces, like individual puzzle pieces. These pieces are the patches. The size of the patch, determined by a parameter called \"patch size,\" decides how much detail each piece captures. A larger patch size means more detail, but fewer pieces overall, like a puzzle with fewer, larger pieces.\n",
        "\n",
        "Once the image is divided into patches, the real magic begins — the patches are embedded into vectors. This process converts each patch, a visual chunk of information, into a mathematical representation, a sequence of numbers. We can think of this as translating the image into a language the model can understand.\n",
        "\n",
        "To preserve the spatial information within the image, the positional encoding comes in. Imagine each patch has a unique address on the image grid. Positional encoding adds a numerical \"address\" to each patch, allowing the model to understand where it is relative to other patches. Now, we have a sequence of \"words\" with their spatial addresses. This sequence is fed into the heart of the ViT model — the Transformer encoder.\n",
        "\n",
        "A special \"classification token\" is added to the beginning of the patch sequence. Think of it as a placeholder for the final answer. This token gathers information from the entire patch sequence, becoming a representation of the whole image, and summarizing the important features and relationships. The classification token is then fed through a final layer, where the model ultimately makes its prediction, be it a category label, an object detection, or any other task.\n",
        "\n",
        "In summary, we mainly need 3 components to build the Input Embedding:\n",
        "\n",
        "- Convert the image into patches, and then vectors.\n",
        "- Add positional encoding.\n",
        "- Add the CLS token."
      ],
      "metadata": {
        "id": "z32LfnrKVStB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Initializes the Embedding module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_height, image_width, patch_height, patch_width, n_embed,\n",
        "                 dropout=0.1) -> None:\n",
        "        super(Embedding, self).__init__()\n",
        "        # calculate the number of patches and the dimension of each patch\n",
        "        num_patches= (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim= channels * patch_height * patch_width\n",
        "\n",
        "        # unfold images of shape (batch_size, channels, image_height, image_width)\n",
        "        # into patches of shape  (batch_size, num_patches, patch_dim)\n",
        "        self.patch_embed= nn.Sequential(\n",
        "            Rearrange('b c (h ph) (w pw) -> b (h w) (ph pw c)', ph=patch_height, pw=patch_width),\n",
        "            nn.LayerNorm(patch_dim),       # normalize each patch\n",
        "            nn.Linear(patch_dim, n_embed), # project patches to embedding dimension\n",
        "            nn.LayerNorm(n_embed),         # normalize the embedding\n",
        "        ) # embedding shape (batch_size, num_patches, n_embed)\n",
        "\n",
        "        # define CLS token and positional embeddings -- both as learnable parameters\n",
        "        self.cls_token= nn.Parameter(torch.randn(1, 1, n_embed))\n",
        "        self.pos_embed= nn.Parameter(torch.randn(1, num_patches +1, n_embed))\n",
        "        self.dropout= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        x= self.patch_embed(img)\n",
        "        B, P, C= x.size()  # (batch_size, num_patches, n_embed)\n",
        "        # repeat class token (CLS) for each image in the batch\n",
        "        cls_token= repeat(self.cls_token, '1 1 d -> b 1 d', b=B)\n",
        "        # concatenate class token (CLS) with patch embeddings\n",
        "        x= torch.cat((cls_token, x), dim=1)\n",
        "        # add positional embedding to the input\n",
        "        x += self.pos_embed[:, :(P + 1)] # (batch_size, num_patches + 1, n_embed)\n",
        "\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "xjjf0XiJqbzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to check that we are correctly splitting the image into a number of patches that is an integer. In other words, we need to check that **image_height** and **image_width** are divisible by **patch_dimension**. Next step is to convert the patch into embeddings. Remember that here an image has $C = 3$ dimensions. We need to unfold this dimension, and compress each patch of dimension $patch\\_size \\times patch\\_size \\times C$. Then we need to define the **CLS** token and the positional embedding. They are both learned parameters (randomly initialized).\n",
        "\n",
        "Finally, we have to define the Transformer encoder that we have defined before, and add a classification head."
      ],
      "metadata": {
        "id": "QH1EeXhAuzwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Initializes a Vision Transformer (ViT) model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_size, patch_size, channels, num_classes, pool='cls',\n",
        "                 n_embed=512, n_layer=6, n_head=8, d_ff=2048, dropout=0.1) -> None:\n",
        "        super(ViT, self).__init__()\n",
        "        image_height, image_width= self.pair(image_size)\n",
        "        patch_height, patch_width= self.pair(patch_size)\n",
        "        # ensure that the image dimensions are divisible by the patch sizes\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "        # ensure the pooling strategy is valid\n",
        "        assert pool in {'cls', 'mean'}, 'Pool type must be either cls (cls token) or mean (mean pooling).'\n",
        "        # pooling strategy (CLS token or mean of patches)\n",
        "        self.pool= pool\n",
        "\n",
        "        # define the patch, CLS token, and positional embeddings\n",
        "        self.embedding= Embedding(\n",
        "            image_height, image_width, patch_height, patch_width, n_embed, dropout\n",
        "        )\n",
        "        # define the transformer encoder\n",
        "        self.encoder= TransformerEncoder(n_embed, n_layer, n_head, d_ff, dropout)\n",
        "        # identity layer (no change to the tensor)\n",
        "        self.latent_space= nn.Identity()\n",
        "        # classification head\n",
        "        self.lm_head= nn.Linear(n_embed, num_classes, bias=False)\n",
        "\n",
        "        # initialize parameters with Glorot / fan_avg\n",
        "        for p in self.parameters():\n",
        "            if p.dim()> 1:\n",
        "                nn.init.xavier_normal_(p)\n",
        "\n",
        "\n",
        "    def pair(self, x):\n",
        "        \"\"\"\n",
        "        Utility function: Converts a single value into a tuple of two values.\n",
        "        If x is already a tuple, it is returned as is.\n",
        "        \"\"\"\n",
        "\n",
        "        return x if isinstance(x, tuple) else (x, x)\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        # img(batch_size, channels, image_height, image_width)\n",
        "        x= self.embedding(img)  # x(batch_size, num_patches + 1, n_embed)\n",
        "        # forward the the transformer encoder\n",
        "        x= self.encoder(x)\n",
        "\n",
        "        # extract class token and feature map\n",
        "        cls_token= x[:, 0]\n",
        "        feature_map= x[:, 1:]\n",
        "        # apply pooling operation: 'cls' token or mean of patches\n",
        "        pool_output= cls_token if self.pool == 'cls' else feature_map.mean(dim=1)\n",
        "\n",
        "        # apply the identity transformation (no change to the tensor)\n",
        "        pool_output= self.latent_space(pool_output)\n",
        "        # forward the classifier\n",
        "        logits= self.lm_head(pool_output)\n",
        "\n",
        "        # return CLS token, patch embeddings, and classification results\n",
        "        return cls_token, feature_map, logits\n"
      ],
      "metadata": {
        "id": "NCHgRrjhMAET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward pass:** We have initialized all the components of our ViT, now we just have to call them in the right order for the forward pass.\n",
        "\n",
        "- We first convert the input image into patches, and unfold each patch into a vector.\n",
        "- Then we repeat CLS tokens (along the batch dimension), and we concatenate it on the dimension 1, which is the sequence length. Indeed we learn the parameters for one vector, but it needs to be concatenated to each image, this is why we expand one dimension.\n",
        "- Then we add the position embedding to each token.\n",
        "\n",
        "Next we apply the Transformer Encoder. We then mainly use it to build an output containing 3 things:\n",
        "\n",
        "- The CLS Token (a single vector representation of the image).\n",
        "- The Feature Map (A vectorized representation of each patch of the image)\n",
        "- Classification Head Logits (Optional): This is used in the case of classification task. Note that Vision Transformer can be trained with different tasks, but classification is the task that was originally used.\n",
        "\n",
        "The math reveals a fascinating aspect of ViTs: they rely on data-driven learning, unlike CNNs which rely on fixed, pre-defined filters. ViTs learn to \"see\" patterns in images based on the data they're trained on, demonstrating a unique ability to adapt to different visual styles and complexities. This explains why they perform exceptionally well on large datasets, as they can capture more subtle and nuanced relationships."
      ],
      "metadata": {
        "id": "ekVSF73glJv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ViT Large hyperparameters config ---\n",
        "image_size= 224\n",
        "patch_size= 16\n",
        "channels=3\n",
        "num_classes= 1000\n",
        "pool='cls'\n",
        "n_embed= 1024\n",
        "n_layer= 24\n",
        "n_head= 16\n",
        "d_ff= 4 * n_embed\n",
        "\n",
        "model= ViT(image_size, patch_size, channels, num_classes, pool,\n",
        "           n_embed, n_layer, n_head, d_ff).to(device)\n",
        "\n",
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}\\n')\n",
        "\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWCBAOGDMAHW",
        "outputId": "0f0a0166-14ab-4091-b8f5-d3669c81c39b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 304329216\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViT(\n",
              "  (embedding): Embedding(\n",
              "    (patch_embed): Sequential(\n",
              "      (0): Rearrange('b c (h ph) (w pw) -> b (h w) (ph pw c)', ph=16, pw=16)\n",
              "      (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (2): Linear(in_features=768, out_features=1024, bias=True)\n",
              "      (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): TransformerEncoder(\n",
              "    (transformer): ModuleList(\n",
              "      (0-23): 24 x EncoderBlock(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): MultiHeadSelfAttention(\n",
              "          (c_attn): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (ffn): FeedForward(\n",
              "          (ffn): Sequential(\n",
              "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.1, inplace=False)\n",
              "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (ln_final): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (latent_space): Identity()\n",
              "  (lm_head): Linear(in_features=1024, out_features=1000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img= [np.random.randn(3, 224, 224) for _ in range(32)]\n",
        "img= torch.tensor(np.array(img), dtype=torch.float32).to(device)\n",
        "\n",
        "model.eval()\n",
        "cls_token, feature_map, logits= model(img)\n",
        "\n",
        "print(f'CLS Token Shape: {cls_token.shape}')\n",
        "print(f'Feature Map Shape: {feature_map.shape}')\n",
        "print(f'Classification Head Logits Shape: {logits.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIHUmqOO3SH8",
        "outputId": "e2df0e9d-e951-4efd-8700-1c403249e1d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLS Token Shape: torch.Size([32, 1024])\n",
            "Feature Map Shape: torch.Size([32, 196, 1024])\n",
            "Classification Head Logits Shape: torch.Size([32, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://medium.com/@cristianleo120/the-math-behind-vision-transformers-95a64a6f0c1a\n",
        "# https://towardsdatascience.com/how-to-train-a-vision-transformer-vit-from-scratch-f26641f26af2"
      ],
      "metadata": {
        "id": "u1_Wj-sY4C1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the ViT model from scratch\n",
        "\n",
        "TODO"
      ],
      "metadata": {
        "id": "2kmHw50E5sYo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qg7bvUacPwOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4bNAYseXPwbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hRh_7GlxPwf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d6CGaMTR_QL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vftETRI0_QRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViTs also have some limitations:\n",
        "\n",
        "- **Data Requirements:** ViTs generally need larger datasets than CNNs to reach their full potential, as their data-driven approach requires more examples to learn complex patterns.\n",
        "- **Computational Cost:** For smaller models, ViTs can be more computationally expensive than CNNs, especially for inference, as their global attention mechanism requires more computations.\n",
        "\n",
        "So, when should we use ViTs and when should we stick with CNNs?\n",
        "\n",
        "- **Large Datasets:** If you have a large amount of training data and need to achieve high accuracy, ViTs are a powerful option.\n",
        "- **Complex Relationships:** For tasks requiring the understanding of intricate relationships across the entire image, ViTs are a better choice.\n",
        "- **Scalability:** If you need a model that scales well, meaning its performance improves as its size increases, ViTs are the way to go."
      ],
      "metadata": {
        "id": "k-V2b-1vBfJu"
      }
    }
  ]
}