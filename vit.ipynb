{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k16_mZAoLVFe"
      },
      "source": [
        "# Vision Transformer (ViT) from Scratch\n",
        "\n",
        "Here, one of the most significant contribution in the field of Computer Vision: the Vision Transformer (ViT).\n",
        "\n",
        "With Self-Attention, each part of the image can \"talk\" to every other part, instantly understanding its relationship to the whole. ViTs treat an image as a collection of \"words\" — not individual pixels, but meaningful chunks, like the different elements in a painting. These chunks are called patches. The magic of Self-Attention lets the model understand the relationships between these patches.\n",
        "\n",
        "CNNs, like detectives, focus on local details, meticulously examining each pixel and its immediate surroundings. They excel at recognizing simple patterns and shapes. ViTs, on the other hand, act like art critics, taking in the whole picture, understanding the relationships between elements, and interpreting the artist's intent. They excel at capturing complex relationships and dependencies across the image.\n",
        "\n",
        "- **Accuracy:** In many cases, ViTs achieve higher accuracy than CNNs, especially on tasks that require understanding complex relationships or recognizing nuanced features.\n",
        "- **Scalability:** ViTs exhibit faster neural scaling laws. This means they become more accurate as the model size increases, while CNNs tend to plateau in performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rkiYGpqxL6sD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import inspect\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ad5RdiaSL-g7"
      },
      "outputs": [],
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rruAqz68Nz6F"
      },
      "outputs": [],
      "source": [
        "# count how many trainable weights the model has\n",
        "def count_parameters(model) -> None:\n",
        "    total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'Number of parameters: {total_params}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyjmDRK9Lx8C"
      },
      "source": [
        "# The Transformer Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3yhgGeHnLObY"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    The Attention Layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head, dropout=0.1) -> None:\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert n_embed % n_head == 0, \"n_embed must be divisible by n_head\"\n",
        "        self.n_embed= n_embed\n",
        "        self.n_head = n_head\n",
        "        self.d_head = n_embed // n_head\n",
        "        # query, key, value projections in a single batch\n",
        "        self.c_attn= nn.Linear(n_embed, 3 * n_embed)\n",
        "        # output projection\n",
        "        self.o_proj= nn.Linear(n_embed, n_embed)\n",
        "        # regularization\n",
        "        self.dropout= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C= x.size() # batch_size, seq_length, embedding dim (d_model)\n",
        "        assert C == self.n_embed, \"Input embedding dimension must match model embedding dimension\"\n",
        "        # 1. calculate query, key, values for all heads\n",
        "        qkv= self.c_attn(x)\n",
        "        q, k, v= qkv.split(self.n_embed, dim=2) # q,k,v -> (B, T, C)\n",
        "        # 2. reshape for Multi-Head Attention\n",
        "        q= q.view(B, -1, self.n_head, self.d_head).transpose(1, 2) # q,k,v view   -> (B, T, nh, dh)\n",
        "        k= k.view(B, -1, self.n_head, self.d_head).transpose(1, 2) # q,k,v transp -> (B, nh, T, dh)\n",
        "        v= v.view(B, -1, self.n_head, self.d_head).transpose(1, 2)\n",
        "        # 3. Attention - the 'scaled dot product'\n",
        "        attn= (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.d_head))\n",
        "        # normalize Attention scores\n",
        "        attn= F.softmax(attn, dim=-1)\n",
        "        attn= self.dropout(attn)\n",
        "        # 4. compute Attention output\n",
        "        y= attn @ v # (B, nh, T, dh)\n",
        "        # 5. concatenate multi-head outputs -- re-assembly all head outputs side by side\n",
        "        y= y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        # 6. output projection\n",
        "        return self.o_proj(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Yf-v31TtL_1Q"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    The Feed Forward Network (FFN).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, d_ff, dropout=0.1) -> None:\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.ffn= nn.Sequential(\n",
        "            nn.Linear(n_embed, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(d_ff, n_embed),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.ffn(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KBERV_gyL_4T"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    The Ecoder Block (pre-normalization version).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head, d_ff, dropout=0.1) -> None:\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.ln_1= nn.LayerNorm(n_embed)\n",
        "        self.attn= MultiHeadSelfAttention(n_embed, n_head, dropout)\n",
        "        self.dropout1= nn.Dropout(p=dropout)\n",
        "        self.ln_2= nn.LayerNorm(n_embed)\n",
        "        self.ffn = FeedForward(n_embed, d_ff, dropout)\n",
        "        self.dropout2= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm= self.ln_1(x)\n",
        "        x= x + self.dropout1(self.attn(x_norm))\n",
        "        x_norm= self.ln_2(x)\n",
        "        x= x + self.dropout2(self.ffn(x_norm))\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riOivS7dUSEq"
      },
      "source": [
        "With the Attention Layer and Feed Forward Network in place, we can assemble a Transformer Encoder. The Transformer Encoder is essentially a stack of N Encoder Blocks. Remember, Transformers are like Legos — the input dimension is the same as the output dimension, so you can stack as many blocks as you want (or as your memory allows)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c5pTN4cwL_7P"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Transformer Encoder is essentially a stack of N Encoder Blocks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed=512, n_layer=6, n_head=8, d_ff=2048, dropout=0.1) -> None:\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.transformer= nn.ModuleList([\n",
        "            EncoderBlock(n_embed, n_head, d_ff, dropout) for _ in range(n_layer)\n",
        "        ])\n",
        "        self.ln_final= nn.LayerNorm(n_embed)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block in self.transformer:\n",
        "            x= block(x)\n",
        "\n",
        "        return self.ln_final(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P29NIa7JMABN",
        "outputId": "c8bbeea4-abb4-48cd-cab5-34180c651a93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 128, 512])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model= TransformerEncoder().to(device)\n",
        "data= torch.randn(16, 128, 512).to(device)\n",
        "model.eval()\n",
        "model(data).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z32LfnrKVStB"
      },
      "source": [
        "# Building the final ViT\n",
        "\n",
        "ViTs start by dividing the image into a grid of smaller, rectangular pieces, like individual puzzle pieces. These pieces are the patches. The size of the patch, determined by a parameter called \"patch size,\" decides how much detail each piece captures. A larger patch size means more detail, but fewer pieces overall, like a puzzle with fewer, larger pieces.\n",
        "\n",
        "Once the image is divided into patches, the real magic begins — the patches are embedded into vectors. This process converts each patch, a visual chunk of information, into a mathematical representation, a sequence of numbers. We can think of this as translating the image into a language the model can understand.\n",
        "\n",
        "To preserve the spatial information within the image, the positional encoding comes in. Imagine each patch has a unique address on the image grid. Positional encoding adds a numerical \"address\" to each patch, allowing the model to understand where it is relative to other patches. Now, we have a sequence of \"words\" with their spatial addresses. This sequence is fed into the heart of the ViT model — the Transformer encoder.\n",
        "\n",
        "A special \"classification token\" is added to the beginning of the patch sequence. Think of it as a placeholder for the final answer. This token gathers information from the entire patch sequence, becoming a representation of the whole image, and summarizing the important features and relationships. The classification token is then fed through a final layer, where the model ultimately makes its prediction, be it a category label, an object detection, or any other task.\n",
        "\n",
        "In summary, we mainly need 3 components to build the Input Embedding:\n",
        "\n",
        "- Convert the image into patches, and then vectors.\n",
        "- Add positional encoding.\n",
        "- Add the CLS token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xjjf0XiJqbzf"
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Initializes the Embedding module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_height, image_width, patch_height, patch_width, n_embed,\n",
        "                 dropout=0.1) -> None:\n",
        "        super(Embedding, self).__init__()\n",
        "        # calculate the number of patches and the dimension of each patch\n",
        "        num_patches= (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim= channels * patch_height * patch_width\n",
        "\n",
        "        # unfold images of shape (batch_size, channels, image_height, image_width)\n",
        "        # into patches of shape  (batch_size, num_patches, patch_dim)\n",
        "        self.patch_embed= nn.Sequential(\n",
        "            Rearrange('b c (h ph) (w pw) -> b (h w) (ph pw c)', ph=patch_height, pw=patch_width),\n",
        "            nn.LayerNorm(patch_dim),       # normalize each patch\n",
        "            nn.Linear(patch_dim, n_embed), # project patches to embedding dimension\n",
        "        ) # embedding shape (batch_size, num_patches, n_embed)\n",
        "\n",
        "        # define CLS token and positional embeddings -- both as learnable parameters\n",
        "        self.cls_token= nn.Parameter(torch.zeros(1, 1, n_embed))\n",
        "        self.pos_embed= nn.Parameter(torch.zeros(1, num_patches +1, n_embed))\n",
        "        self.dropout= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        x= self.patch_embed(img)\n",
        "        B, P, C= x.size()  # (batch_size, num_patches, n_embed)\n",
        "        # repeat class token (CLS) for each image in the batch\n",
        "        cls_token= repeat(self.cls_token, '1 1 d -> b 1 d', b=B)\n",
        "        # concatenate class token (CLS) with patch embeddings\n",
        "        x= torch.cat((cls_token, x), dim=1)\n",
        "        # add positional embedding to the input\n",
        "        x= x + self.pos_embed[:, :(P + 1)]  # (batch_size, num_patches + 1, n_embed)\n",
        "\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH1EeXhAuzwY"
      },
      "source": [
        "We need to check that we are correctly splitting the image into a number of patches that is an integer. In other words, we need to check that **image_height** and **image_width** are divisible by **patch_dimension**. Next step is to convert the patch into embeddings. Remember that here an image has $C = 3$ dimensions. We need to unfold this dimension, and compress each patch of dimension $patch\\_size \\times patch\\_size \\times C$. Then we need to define the **CLS** token and the positional embedding. They are both learned parameters (randomly initialized)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GD_tjCnHAlaZ"
      },
      "outputs": [],
      "source": [
        "class MLPHead(nn.Module):\n",
        "    \"\"\"\n",
        "    The MLP (classification) head.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, d_ff, num_classes, dropout=0.1, fine_tune=False) -> None:\n",
        "        super(MLPHead, self).__init__()\n",
        "        if fine_tune:\n",
        "            self.c_head= nn.Linear(n_embed, num_classes)\n",
        "        else:\n",
        "            self.c_head= nn.Sequential(\n",
        "                nn.Linear(n_embed, d_ff),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(p=dropout),\n",
        "                nn.Linear(d_ff, num_classes),\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.c_head(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXS68FBABMBY"
      },
      "source": [
        "Finally, we have to define the Transformer encoder that we have defined before, and add a classification head. The MLP head, also known as the classification head, is the final part of the Vision Transformer (ViT) pipeline. It is responsible for processing the output of the Transformer encoder (particularly the class token) to produce the final predictions for a classification task.\n",
        "\n",
        "- During fine-tuning, the MLP head may be replaced with a single linear layer (no hidden layers). This reduces complexity since the model already has rich feature representations from pretraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NCHgRrjhMAET"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Initializes a Vision Transformer (ViT) model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_size, patch_size, channels, num_classes, pool='cls',\n",
        "                 n_embed=512, n_layer=6, n_head=8, d_ff=2048, dropout=0.1, fine_tune=False) -> None:\n",
        "        super(ViT, self).__init__()\n",
        "        image_height, image_width= self.pair(image_size)\n",
        "        patch_height, patch_width= self.pair(patch_size)\n",
        "        # ensure that the image dimensions are divisible by the patch sizes\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, \\\n",
        "            'Image dimensions must be divisible by the patch size.'\n",
        "        # ensure the pooling strategy is valid\n",
        "        assert pool in {'cls', 'mean'}, 'Pool type must be either cls (cls token) or mean (mean pooling).'\n",
        "        # pooling strategy (CLS token or mean of patches)\n",
        "        self.pool= pool\n",
        "\n",
        "        # define the patch, CLS token, and positional embeddings\n",
        "        self.embedding= Embedding(\n",
        "            image_height, image_width, patch_height, patch_width, n_embed, dropout\n",
        "        )\n",
        "        # define the transformer encoder\n",
        "        self.encoder= TransformerEncoder(n_embed, n_layer, n_head, d_ff, dropout)\n",
        "        # identity layer (no change to the tensor)\n",
        "        self.latent_space= nn.Identity()\n",
        "        # classification head\n",
        "        self.lm_head= MLPHead(n_embed, d_ff, num_classes, dropout, fine_tune)\n",
        "\n",
        "        # initialize parameters with Glorot / fan_avg\n",
        "        for p in self.parameters():\n",
        "            if p.dim()> 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "\n",
        "    def pair(self, x):\n",
        "        \"\"\"\n",
        "        Utility function: Converts a single value into a tuple of two values.\n",
        "        If x is already a tuple, it is returned as is.\n",
        "        \"\"\"\n",
        "        return x if isinstance(x, tuple) else (x, x)\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        # img(batch_size, channels, image_height, image_width)\n",
        "        x= self.embedding(img)  # x(batch_size, num_patches + 1, n_embed)\n",
        "        # forward the the transformer encoder\n",
        "        x= self.encoder(x)\n",
        "\n",
        "        # extract class token and feature map\n",
        "        cls_token= x[:, 0]\n",
        "        feature_map= x[:, 1:]\n",
        "        # apply pooling operation: 'cls' token or mean of patches\n",
        "        pool_output= cls_token if self.pool == 'cls' else feature_map.mean(dim=1)\n",
        "\n",
        "        # apply the identity transformation (no change to the tensor)\n",
        "        pool_output= self.latent_space(pool_output)\n",
        "        # forward the classifier\n",
        "        logits= self.lm_head(pool_output)\n",
        "\n",
        "        # return CLS token, patch embeddings, and classification results\n",
        "        return cls_token, feature_map, logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekVSF73glJv8"
      },
      "source": [
        "**Forward pass:** We have initialized all the components of our ViT, now we just have to call them in the right order for the forward pass.\n",
        "\n",
        "- We first convert the input image into patches, and unfold each patch into a vector.\n",
        "- Then we repeat CLS tokens (along the batch dimension), and we concatenate it on the dimension 1, which is the sequence length. Indeed we learn the parameters for one vector, but it needs to be concatenated to each image, this is why we expand one dimension.\n",
        "- Then we add the position embedding to each token.\n",
        "\n",
        "Next we apply the Transformer Encoder. We then mainly use it to build an output containing 3 things:\n",
        "\n",
        "- The CLS Token (a single vector representation of the image).\n",
        "- The Feature Map (A vectorized representation of each patch of the image)\n",
        "- Classification Head Logits (Optional): This is used in the case of classification task. Note that Vision Transformer can be trained with different tasks, but classification is the task that was originally used.\n",
        "\n",
        "The math reveals a fascinating aspect of ViTs: they rely on data-driven learning, unlike CNNs which rely on fixed, pre-defined filters. ViTs learn to \"see\" patterns in images based on the data they're trained on, demonstrating a unique ability to adapt to different visual styles and complexities. This explains why they perform exceptionally well on large datasets, as they can capture more subtle and nuanced relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWCBAOGDMAHW",
        "outputId": "897e0adf-4572-4f6d-9a21-aaf717b5d352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 311598568\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViT(\n",
              "  (embedding): Embedding(\n",
              "    (patch_embed): Sequential(\n",
              "      (0): Rearrange('b c (h ph) (w pw) -> b (h w) (ph pw c)', ph=16, pw=16)\n",
              "      (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (2): Linear(in_features=768, out_features=1024, bias=True)\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): TransformerEncoder(\n",
              "    (transformer): ModuleList(\n",
              "      (0-23): 24 x EncoderBlock(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): MultiHeadSelfAttention(\n",
              "          (c_attn): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (ffn): FeedForward(\n",
              "          (ffn): Sequential(\n",
              "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Dropout(p=0.1, inplace=False)\n",
              "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (ln_final): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (latent_space): Identity()\n",
              "  (lm_head): MLPHead(\n",
              "    (c_head): Sequential(\n",
              "      (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "      (1): GELU(approximate='none')\n",
              "      (2): Dropout(p=0.1, inplace=False)\n",
              "      (3): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# --- ViT Large hyperparameters config ---\n",
        "image_size= 224\n",
        "patch_size= 16\n",
        "channels= 3\n",
        "num_classes= 1000  # ImageNet has 1000 classes\n",
        "pool='cls'\n",
        "n_embed= 1024\n",
        "n_layer= 24\n",
        "n_head= 16\n",
        "d_ff= 4 * n_embed\n",
        "\n",
        "model= ViT(image_size, patch_size, channels, num_classes, pool,\n",
        "           n_embed, n_layer, n_head, d_ff).to(device)\n",
        "\n",
        "count_parameters(model)\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIHUmqOO3SH8",
        "outputId": "5de017de-40a3-4d5a-aae7-854019f32851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLS Token Shape: torch.Size([64, 1024])\n",
            "Feature Map Shape: torch.Size([64, 196, 1024])\n",
            "Classification Head Logits Shape: torch.Size([64, 1000])\n",
            "Forward Pass Memory: 25235.53 MB\n"
          ]
        }
      ],
      "source": [
        "import torch.cuda as cuda\n",
        "\n",
        "img= [np.random.randn(3, 224, 224) for _ in range(64)]\n",
        "img= torch.tensor(np.array(img), dtype=torch.float32).to(device)\n",
        "\n",
        "cuda.reset_peak_memory_stats()\n",
        "# Run the model\n",
        "model.eval()\n",
        "cls_token, feature_map, logits= model(img)\n",
        "# Measure peak memory usage\n",
        "peak_memory= cuda.max_memory_allocated()\n",
        "\n",
        "print(f'CLS Token Shape: {cls_token.shape}')\n",
        "print(f'Feature Map Shape: {feature_map.shape}')\n",
        "print(f'Classification Head Logits Shape: {logits.shape}')\n",
        "print(f\"Forward Pass Memory: {np.round(peak_memory / 10**6, decimals=2)} MB\")\n",
        "\n",
        "del img, cls_token, feature_map, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1_Wj-sY4C1O"
      },
      "outputs": [],
      "source": [
        "# https://medium.com/@cristianleo120/the-math-behind-vision-transformers-95a64a6f0c1a\n",
        "# https://towardsdatascience.com/how-to-train-a-vision-transformer-vit-from-scratch-f26641f26af2\n",
        "# https://medium.com/@dancerworld60/building-vision-transformers-from-scratch-a-comprehensive-guide-dd244abaad15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kmHw50E5sYo"
      },
      "source": [
        "# Training the ViT model from scratch\n",
        "\n",
        "torchvision: Provides utilities for handling vision-related tasks, including datasets and transformations.\n",
        "\n",
        "Before feeding images into the Vision Transformer, we need to preprocess them. The model expects images of size 224x224, so we'll resize our CIFAR-10 images and normalize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Qg7bvUacPwOB"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bNAYseXPwbL",
        "outputId": "745b8fc0-48b5-46aa-ced9-392e398e586c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:01<00:00, 101MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# data preparation -- define transformations for the dataset\n",
        "transform= transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# load the CIFAR-10 dataset\n",
        "train_dataset= datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
        "\n",
        "# create data loaders\n",
        "train_size= int(0.9 * len(train_dataset))\n",
        "val_size  = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset= random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "batch_size= 64\n",
        "train_loader= DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader  = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kgti6QNVDVJv",
        "outputId": "62f7b3ac-664c-4096-c5eb-d8af9741fb3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(704, 79)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(train_loader), len(val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZDlKe77CSIR"
      },
      "source": [
        "# The Trainer Function and Cosine LR Decay\n",
        "\n",
        "TODO:\n",
        "- Training using data augmentation (Transformers are data hungry).\n",
        "- Logging Learning Rate.\n",
        "- Metric Tracking: adding accuracy, precision, or other metrics to monitor training and validation performance.\n",
        "- Early Stopping: based on validation loss to prevent overfitting.\n",
        "- More epochs of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yGjgxh2yqi7U"
      },
      "outputs": [],
      "source": [
        "class CosineLRDecay:\n",
        "    \"\"\"\n",
        "    Modulates learning rate (LR) based on the iteration (step) number which LR there should be.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, min_lr, max_lr=3e-4, warmup_steps=10,\n",
        "                 max_steps=50) -> None:\n",
        "        self.optimizer= optimizer\n",
        "        self.min_lr= min_lr\n",
        "        self.max_lr= max_lr\n",
        "        self.warmup_steps= warmup_steps\n",
        "        self.max_steps= max_steps\n",
        "        self.last_step= 0\n",
        "        self.last_lr= None\n",
        "\n",
        "\n",
        "    def get_last_lr(self):\n",
        "        return self.last_lr\n",
        "\n",
        "\n",
        "    def get_lr(self, it):\n",
        "        # 1) linear warmup for warmup_iters steps\n",
        "        if it< self.warmup_steps:\n",
        "            return self.max_lr * (it+1) / self.warmup_steps\n",
        "        # 2) if it > lr_decay_iters, return min learning rate\n",
        "        if it> self.max_steps:\n",
        "            return self.min_lr\n",
        "        # 3) in between, use cosine decay down to min learning rate\n",
        "        decay_ratio= (it - self.warmup_steps) / (self.max_steps - self.warmup_steps)\n",
        "        assert 0 <= decay_ratio <= 1\n",
        "        # coeff starts at 1 and goes to 0\n",
        "        coeff= 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "\n",
        "        return self.min_lr + coeff * (self.max_lr - self.min_lr)\n",
        "\n",
        "\n",
        "    def step(self):\n",
        "        self.last_lr= self.get_lr(self.last_step)\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr']= self.last_lr\n",
        "\n",
        "        self.last_step += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hRh_7GlxPwf6"
      },
      "outputs": [],
      "source": [
        "def trainer(model, train_loader, val_loader, optimizer, criterion, scheduler, epochs,\n",
        "            device, eval_interval=10, verbose=False):\n",
        "\n",
        "    tr_loss_hist= []\n",
        "    vl_loss_hist= []\n",
        "\n",
        "    # --- training loop ---\n",
        "    for epoch in range(epochs):\n",
        "        batch_loss= []\n",
        "        start= time.time()\n",
        "\n",
        "        # --- training steps ---\n",
        "        # iterating over all batches\n",
        "        for step, (images, labels) in enumerate(train_loader):\n",
        "            # --- minibatch construction ---\n",
        "            images= images.to(device, non_blocking=True)\n",
        "            labels= labels.to(device, non_blocking=True)\n",
        "\n",
        "            # --- forward pass and get loss ---\n",
        "            model.train()\n",
        "            _, _, logits= model(images)\n",
        "            loss= criterion(logits, labels)\n",
        "            batch_loss.append(loss.item())\n",
        "\n",
        "            # --- backward pass to calculate the gradients ---\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # --- update the parameters using the gradient ---\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        # --- evaluation and track stats ---\n",
        "        tr_loss_hist.append(np.mean(batch_loss))\n",
        "\n",
        "        if epoch% eval_interval== 0 or epoch== epochs-1:\n",
        "            model.eval()\n",
        "            val_loss= []\n",
        "            with torch.no_grad():\n",
        "                for images, labels in val_loader:\n",
        "                    images, labels= images.to(device), labels.to(device)\n",
        "                    _, _, logits= model(images)\n",
        "                    loss_v= criterion(logits, labels)\n",
        "                    val_loss.append(loss_v.item())\n",
        "\n",
        "            val_loss= np.mean(val_loss)\n",
        "            end= time.time()\n",
        "            dt= end - start\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Epoch: {epoch} | Train Loss: {tr_loss_hist[-1]:.4f} | \"\n",
        "                      f\"Val Loss: {val_loss:.4f} | dt/epoch: {dt*1000:.2f}ms\")\n",
        "\n",
        "        vl_loss_hist.append(val_loss)\n",
        "\n",
        "    return tr_loss_hist, vl_loss_hist\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device, verbose=False):\n",
        "    model.eval()\n",
        "    correct= 0\n",
        "    total= 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels= images.to(device), labels.to(device)\n",
        "            _, _, logits= model(images)\n",
        "            y_pred= torch.argmax(logits, dim=1)\n",
        "            correct += (y_pred == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    acc= correct / total\n",
        "    if verbose:\n",
        "        print(f\"Accuracy: {(acc * 100):.2f}%\")\n",
        "\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "d2XfyaznRZ16"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgHGvEIaCntB"
      },
      "source": [
        "# Training setup using TF32 and Fused AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1usqKj0ExsO",
        "outputId": "d3465f31-f90f-4c1f-a809-bbbbe7131a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 88193290\n",
            "Using fused AdamW: True\n"
          ]
        }
      ],
      "source": [
        "use_fused= False\n",
        "\n",
        "if device== 'cuda': # TF32 computationally more efficient (slightly the same precision of FP32)\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "    # create AdamW optimizer and use the fused version of it is available\n",
        "    fused_available= 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "    # fused is a lot faster when it is available and when running on cuda\n",
        "    use_fused= fused_available\n",
        "\n",
        "# --- ViT Base hyperparameters config ---\n",
        "image_size= 224\n",
        "patch_size= 16\n",
        "channels= 3\n",
        "num_classes= 10 # CIFAR-10 has 10 classes\n",
        "pool='cls'\n",
        "n_embed= 768\n",
        "n_layer= 12\n",
        "n_head= 12\n",
        "d_ff= 4 * n_embed\n",
        "\n",
        "vit_model= ViT(image_size, patch_size, channels, num_classes, pool, n_embed, n_layer,\n",
        "               n_head, d_ff).to(device)\n",
        "\n",
        "count_parameters(vit_model)\n",
        "\n",
        "\n",
        "epochs= 10\n",
        "# train_loader has size 704, so 10 epochs have 7,040 steps\n",
        "steps= len(train_loader) * epochs\n",
        "learning_rate= 1e-4\n",
        "\n",
        "max_lr= learning_rate\n",
        "min_lr= 5e-5\n",
        "warmup_steps= 400\n",
        "max_steps= steps\n",
        "\n",
        "optimizer= torch.optim.AdamW(\n",
        "    vit_model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=0.01,\n",
        "    fused=use_fused\n",
        ")\n",
        "print(f\"Using fused AdamW: {use_fused}\")\n",
        "criterion= nn.CrossEntropyLoss()\n",
        "scheduler= CosineLRDecay(optimizer, min_lr, max_lr, warmup_steps, max_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr_loss, vl_loss= trainer(vit_model, train_loader, val_loader, optimizer, criterion, scheduler,\n",
        "                          epochs, device, eval_interval=1, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g17bBiLydZ86",
        "outputId": "5f91fc51-c98f-45a4-b2f7-a0573d43a303"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Train Loss: 1.7988 | Val Loss: 1.6816 | dt/epoch: 116103.33ms\n",
            "Epoch: 1 | Train Loss: 1.4559 | Val Loss: 1.3620 | dt/epoch: 115888.64ms\n",
            "Epoch: 2 | Train Loss: 1.3060 | Val Loss: 1.3676 | dt/epoch: 115825.14ms\n",
            "Epoch: 3 | Train Loss: 1.1999 | Val Loss: 1.1766 | dt/epoch: 115784.35ms\n",
            "Epoch: 4 | Train Loss: 1.0951 | Val Loss: 1.1654 | dt/epoch: 115565.79ms\n",
            "Epoch: 5 | Train Loss: 1.0075 | Val Loss: 1.1288 | dt/epoch: 115502.80ms\n",
            "Epoch: 6 | Train Loss: 0.9090 | Val Loss: 1.0910 | dt/epoch: 115716.98ms\n",
            "Epoch: 7 | Train Loss: 0.8070 | Val Loss: 1.0584 | dt/epoch: 115650.64ms\n",
            "Epoch: 8 | Train Loss: 0.7086 | Val Loss: 1.0989 | dt/epoch: 115530.33ms\n",
            "Epoch: 9 | Train Loss: 0.6206 | Val Loss: 1.1396 | dt/epoch: 115805.50ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "plt.plot(tr_loss, label='Train Loss')\n",
        "plt.plot(vl_loss, label='Validation Loss')\n",
        "plt.title('Losses')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "mbYnSiBgdaGh",
        "outputId": "4fd46bee-1d66-48cc-d791-04f885254dab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vit_acc= evaluate_model(vit_model, test_loader, device, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H11bfZmrdaO1",
        "outputId": "5d7a63b2-5163-4ce4-d363-158d2385d054"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 63.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-V2b-1vBfJu"
      },
      "source": [
        "ViTs also have some limitations:\n",
        "\n",
        "- **Data Requirements:** ViTs generally need larger datasets than CNNs to reach their full potential, as their data-driven approach requires more examples to learn complex patterns.\n",
        "- **Computational Cost:** For smaller models, ViTs can be more computationally expensive than CNNs, especially for inference, as their global attention mechanism requires more computations.\n",
        "\n",
        "So, when should we use ViTs and when should we stick with CNNs?\n",
        "\n",
        "- **Large Datasets:** If you have a large amount of training data and need to achieve high accuracy, ViTs are a powerful option.\n",
        "- **Complex Relationships:** For tasks requiring the understanding of intricate relationships across the entire image, ViTs are a better choice.\n",
        "- **Scalability:** If you need a model that scales well, meaning its performance improves as its size increases, ViTs are the way to go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFYkFDeFwdfO"
      },
      "outputs": [],
      "source": [
        "# https://github.com/kyegomez/Vit-RGTS/blob/main/train.py\n",
        "# https://github.com/FrancoisPorcher/vit-pytorch/tree/5a105d7df3b22815c322d7d386494a49e6c08825"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}