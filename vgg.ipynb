{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# VGG Network from Scratch in PyTorch\n",
        "\n",
        "VGG stands for the Vision Geometry Group from the University of Oxford. The VGG net is a Deep Learning model based on Convolutional Neural Networks (CNN) and defined in the paper \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" (https://arxiv.org/abs/1409.1556).\n",
        "\n",
        "In the paper, the authors introduced not one but six different network configurations for the VGG neural network models. Each of them has a different neural network architecture. The VGG-16 has 13 convolutional and 3 fully-connected layers, carrying with them the ReLU tradition from AlexNet. This network stacks more layers onto AlexNet, and use smaller size filters ($2 \\times 2$ and $3 \\times 3$). It consists of 138M parameters and takes up about 500MB of storage space. There is also a deeper variant, VGG-19, and the simplest of all the configurations, VGG-11."
      ],
      "metadata": {
        "id": "JEp4yctuhvPs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CrJUjh6Zhs7U"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import inspect\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "AmW_2isVi6nh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count how many trainable weights the model has\n",
        "def count_parameters(model) -> None:\n",
        "    total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'Number of parameters: {total_params}')"
      ],
      "metadata": {
        "id": "Mm7RGyXRi6qs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vgg_= (num conv channels, norm presence, pool presence)\n",
        "model_parameters= {}\n",
        "model_parameters['vgg11']= (\n",
        "    [64,128,256,256,512,512,512,512],  # Channels\n",
        "    [1,1,1,1,1,1,1,1],                 # BatchNorm presence  (1 for yes, 0 for no)\n",
        "    [1,1,0,1,0,1,0,1]                  # MaxPooling presence (1 for yes, 0 for no)\n",
        ")\n",
        "model_parameters['vgg13']= (\n",
        "    [64,64,128,128,256,256,512,512,512,512],\n",
        "    [1]*10,\n",
        "    [0,1,0,1,0,1,0,1,0,1]\n",
        ")\n",
        "model_parameters['vgg16']= (\n",
        "    [64,64,128,128,256,256,256,512,512,512,512,512,512],\n",
        "    [1]*13,\n",
        "    [0,1,0,1,0,0,1,0,0,1,0,0,1]\n",
        ")\n",
        "model_parameters['vgg19']= (\n",
        "    [64,64,128,128,256,256,256,256,512,512,512,512,512,512,512,512],\n",
        "    [1]*16,\n",
        "    [0,1,0,1,0,0,0,1,0,0,0,1,0,0,0,1]\n",
        ")"
      ],
      "metadata": {
        "id": "D12UdMM8v21O"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements one customizable CNN layer.\n",
        "    VGG-style: Input -> Conv2d -> BatchNorm2d -> ReLU -> MaxPool2d -> Output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, norm=True, activation=None, pool=True) -> None:\n",
        "        super(ConvLayer, self).__init__()\n",
        "        self.conv= nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        # Batch Normalization -- not introduced when VGG models came out\n",
        "        self.norm= nn.BatchNorm2d(out_channels) if norm else None\n",
        "        # Activation function -- ReLU is default in VGG\n",
        "        self.activation= nn.ReLU(inplace=True) if activation is None else activation\n",
        "        # Max pooling layer -- halves the feature maps each time\n",
        "        self.pool= nn.MaxPool2d(kernel_size=2, stride=2) if pool else None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.conv(x)\n",
        "        if self.norm is not None:\n",
        "            x= self.norm(x)\n",
        "        x= self.activation(x)\n",
        "        if self.pool is not None:\n",
        "            x= self.pool(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "kl_tY9MFi6uG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements one customizable MLP layer.\n",
        "    AlexNet/VGG-style: Input -> Dropout -> Linear -> ReLU -> Output\n",
        "    Dropout is placed before the FC layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, activation=None, dropout=0.0) -> None:\n",
        "        super(MLPLayer, self).__init__()\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout= nn.Dropout(p=dropout) if dropout> 0.0 else None\n",
        "        # Fully connected (FC) layer\n",
        "        self.fc= nn.Linear(in_dim, out_dim)\n",
        "        # Activation function -- ReLU is the default in VGG\n",
        "        self.activation= activation\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.dropout is not None:\n",
        "            x= self.dropout(x)\n",
        "        x= self.fc(x)\n",
        "        if self.activation is not None:\n",
        "            x= self.activation(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ig9E3q6FBVav"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "    \"\"\"\n",
        "    Initializes the VGG architecture based on the provided variant.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vgg_type, in_channels, num_classes, activation=None, dropout=0.1) -> None:\n",
        "        super(VGG, self).__init__()\n",
        "        # Channels along with BatchNorm2d and MaxPool2d presence for each ConvLayer\n",
        "        channels= vgg_type[0]\n",
        "        norms   = vgg_type[1]\n",
        "        maxpools= vgg_type[2]\n",
        "        # Define the activation function -- ReLU is default in VGG\n",
        "        activation= nn.ReLU(inplace=True) if activation is None else activation\n",
        "\n",
        "        # The convolutional feature extractor\n",
        "        self.conv_layers= nn.ModuleList(\n",
        "            [ConvLayer(in_channels, channels[0], norms[0], activation, maxpools[0])]\n",
        "        )\n",
        "        for i in range(1, len(channels)):\n",
        "            self.conv_layers.append(\n",
        "                ConvLayer(channels[i-1], channels[i], norms[i], activation, maxpools[i])\n",
        "            )\n",
        "        # Flatten the 2D feature maps into 1D feature vectors\n",
        "        self.flatten= nn.Flatten(start_dim=1)\n",
        "        # The classification head -- FC linear layers\n",
        "        self.fc = MLPLayer(7*7*512, 4096, activation, dropout)\n",
        "        self.fc1= MLPLayer(4096, 4096, activation, dropout)\n",
        "        self.fc2= MLPLayer(4096, num_classes, activation=None, dropout=0.0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Processes the input through the convolutional layers\n",
        "        for layer in self.conv_layers:\n",
        "            x= layer(x)\n",
        "        x= self.flatten(x)\n",
        "        # Runs the feature vector through the classification head to generate predictions\n",
        "        x= self.fc2(self.fc1(self.fc(x)))\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "GYhAblBjkVrZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img= torch.randn(1, 3, 224, 224).to(device)\n",
        "model= VGG(model_parameters['vgg16'], in_channels=3, num_classes=1000, dropout=0.1).to(device)\n",
        "count_parameters(model)\n",
        "print(model(img).shape)\n",
        "\n",
        "model"
      ],
      "metadata": {
        "id": "EZQSL-9lkVvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0770e9cc-e86a-478b-d0d1-e731bb6e7f44"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 138365992\n",
            "torch.Size([1, 1000])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (conv_layers): ModuleList(\n",
              "    (0): ConvLayer(\n",
              "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activation): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): ConvLayer(\n",
              "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activation): ReLU(inplace=True)\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (2): ConvLayer(\n",
              "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activation): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): ConvLayer(\n",
              "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activation): ReLU(inplace=True)\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (4): ConvLayer(\n",
              "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activation): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): ConvLayer(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activation): ReLU(inplace=True)\n",
              "    )\n",
              "    (6): ConvLayer(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activation): ReLU(inplace=True)\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (7): ConvLayer(\n",
              "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activation): ReLU(inplace=True)\n",
              "    )\n",
              "    (8): ConvLayer(\n",
              "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activation): ReLU(inplace=True)\n",
              "    )\n",
              "    (9): ConvLayer(\n",
              "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activation): ReLU(inplace=True)\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (10-11): 2 x ConvLayer(\n",
              "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activation): ReLU(inplace=True)\n",
              "    )\n",
              "    (12): ConvLayer(\n",
              "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (activation): ReLU(inplace=True)\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "  )\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (fc): MLPLayer(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (fc): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (activation): ReLU(inplace=True)\n",
              "  )\n",
              "  (fc1): MLPLayer(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (fc): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (activation): ReLU(inplace=True)\n",
              "  )\n",
              "  (fc2): MLPLayer(\n",
              "    (fc): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "boWMAldFkVyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n2kDQQg6i6xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mQwj1z0Ti61X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ssrsb68fi67J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1pWCyWZki6_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://medium.com/@ilaslanduzgun/create-vgg-from-scratch-in-pytorch-aa194c269b55\n",
        "# https://www.digitalocean.com/community/tutorials/vgg-from-scratch-pytorch\n",
        "# https://debuggercafe.com/implementing-vgg11-from-scratch-using-pytorch/"
      ],
      "metadata": {
        "id": "v_0dNRbyi7CL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}