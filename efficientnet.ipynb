{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# EfficientNet - Implementation from scratch in PyTorch"
      ],
      "metadata": {
        "id": "SANbdWXTGcow"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4WrSniUGCqM"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import inspect\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "bBnrVnkhGnVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count how many trainable weights the model has\n",
        "def count_parameters(model) -> None:\n",
        "    total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'Number of parameters: {total_params}')"
      ],
      "metadata": {
        "id": "5Mezx6HsGnZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define configs for different EfficientNet versions"
      ],
      "metadata": {
        "id": "oQoPH7TdHgXP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZDdk1tsxHfUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture Implementation"
      ],
      "metadata": {
        "id": "KbDsc3epHrT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements one customizable CNN layer.\n",
        "    EfficientNet-style: Input -> Conv2d -> BatchNorm2d -> SiLU -> Output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups=1,\n",
        "                 bias=False, activation=None) -> None:\n",
        "        super(ConvLayer, self).__init__()\n",
        "        self.conv= nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=bias\n",
        "        )\n",
        "        # Batch Normalization to stabilize training\n",
        "        self.norm= nn.BatchNorm2d(out_channels)\n",
        "        # Activation function -- SiLU is the default in EfficientNet\n",
        "        self.activation= nn.SiLU() if activation is None else activation\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.conv(x)\n",
        "        x= self.norm(x)\n",
        "        x= self.activation(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "meSgQrUdHfX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SqueezeExcitation(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a Squeeze-and-Excitation module.\n",
        "    It squeezes global spatial information into a channel descriptor and re-scales the channels.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, squeezed_dim, activation=None) -> None:\n",
        "        super(SqueezeExcitation, self).__init__()\n",
        "        # Global average pooling: C x H x W -> C x 1 x 1\n",
        "        self.average_pool= nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "        # 1x1 convolution reduces the channel dimension\n",
        "        self.conv1= nn.Conv2d(in_channels, squeezed_dim, kernel_size=1)\n",
        "        # Activation function -- SiLU is the default in EfficientNet\n",
        "        self.activation= nn.SiLU() if activation is None else activation\n",
        "        # 1x1 convolution restores the channel dimension\n",
        "        self.conv2= nn.Conv2d(squeezed_dim, in_channels, kernel_size=1)\n",
        "        # Sigmoid activation to obtain channel-wise weights between 0 and 1\n",
        "        self.sigmoid= nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        se= self.average_pool(x)\n",
        "        se= self.activation(self.conv1(se))\n",
        "        se= self.sigmoid(self.conv2(se))\n",
        "\n",
        "        return x * se\n"
      ],
      "metadata": {
        "id": "Uk-Ul6e-HffJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MBConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a Mobile Inverted Residual Block.\n",
        "    Residual (skip) connection is used if stride==1 and input/output channels match.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, expand_ratio,\n",
        "                 reduction=4, survival_prob=0.8, bias=False, activation=None) -> None:\n",
        "        super(MBConv, self).__init__()\n",
        "        # Activation function -- SiLU is the default in EfficientNet\n",
        "        activation= nn.SiLU() if activation is None else activation\n",
        "        hidden_dim= in_channels * expand_ratio\n",
        "        # For squeeze and excitation module\n",
        "        reduced_dim= in_channels // reduction\n",
        "\n",
        "        self.use_residual= in_channels == out_channels and stride == 1\n",
        "        # Determine if expansion is needed\n",
        "        self.expand= in_channels != hidden_dim\n",
        "        # For stochastic depth\n",
        "        self.survival_prob= survival_prob\n",
        "\n",
        "        if self.expand:\n",
        "            # Optional expansion phase (1x1 conv + BN + Activation)\n",
        "            self.expand_conv= ConvLayer(\n",
        "                in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=bias,\n",
        "                activation=activation\n",
        "            )\n",
        "\n",
        "        self.conv= nn.Sequential(\n",
        "            # Depthwise convolution\n",
        "            ConvLayer(\n",
        "                hidden_dim, hidden_dim, kernel_size, stride, padding, groups=hidden_dim,\n",
        "                bias=bias, activation=activation\n",
        "            ),\n",
        "            # Squeeze-and-Excitation\n",
        "            SqueezeExcitation(hidden_dim, reduced_dim, activation=activation),\n",
        "            # Projection phase (1x1 conv + BN) -- reduce channels to out_channels\n",
        "            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, bias=bias),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "\n",
        "\n",
        "    def stochastic_depth(self, x):\n",
        "        \"\"\"\n",
        "        Implements stochastic depth regularization.\n",
        "        During training, randomly drops the output of the block with probability\n",
        "        (1 - survival_prob).\n",
        "        \"\"\"\n",
        "        if not self.training:\n",
        "            return x\n",
        "        # Binary tensor with the same batch size and shape (broadcasted over spatial dimensions)\n",
        "        binary_tensor= torch.rand(\n",
        "            x.shape[0], 1, 1, 1, device=x.device, dtype=x.dtype\n",
        "        ) < self.survival_prob\n",
        "        # Scale the output to maintain expected value and apply the mask\n",
        "        return torch.div(x, self.survival_prob) * binary_tensor\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.expand:\n",
        "            x= self.expand_conv(x)\n",
        "        if self.use_residual:\n",
        "            return x + self.stochastic_depth(self.conv(x))\n",
        "\n",
        "        return self.conv(x)\n"
      ],
      "metadata": {
        "id": "2RRnDqy6HfcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the EfficientNet"
      ],
      "metadata": {
        "id": "6Zc2Zm66HWWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EfficientNet(nn.Module):\n",
        "    \"\"\"\n",
        "    WIP\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_version, num_classes, bias=False, activation=None) -> None:\n",
        "        super(EfficientNet, self).__init__()\n",
        "        # Activation function -- SiLU is the default in EfficientNet\n",
        "        activation= nn.SiLU() if activation is None else activation\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ZwzrxsDYGncE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eeJ2eiH9IMk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bPrxk3oAIMpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a EfficientNet model from scratch"
      ],
      "metadata": {
        "id": "vLd8fcA4HRDg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6xZhJyB-Gnfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lhiMtAjRIVRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x2upZwRFIVU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer Function"
      ],
      "metadata": {
        "id": "QR0fYm-gHNuW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qntwlcedGnjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IbrOlZeFH8NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rxm606euH8RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training setup using TF32 and Fused AdamW"
      ],
      "metadata": {
        "id": "nXVDS6aEHJ74"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Kh7NAWPGnn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ez85I5piH87u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k-FVthQ0H8_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "99AS9CzTIYN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://medium.com/@aniketthomas27/efficientnet-implementation-from-scratch-in-pytorch-a-step-by-step-guide-a7bb96f2bdaa\n",
        "# https://medium.com/technological-singularity/efficientnet-revolutionizing-deep-learning-through-model-efficiency-0ed5485f9a6f"
      ],
      "metadata": {
        "id": "--FLwsEqGnrC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}