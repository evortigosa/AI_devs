{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Build the LLaMA 3 Architecture from Scratch Using PyTorch"
      ],
      "metadata": {
        "id": "o1jyU47dnHdd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ucl6l-i5mpv8"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Tuple, List\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "1pPwP8jxnr9S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count how many trainable weights the model has\n",
        "def count_parameters(model) -> None:\n",
        "    total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'Number of parameters: {total_params}')"
      ],
      "metadata": {
        "id": "s25vWwlA6lWe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - The Input Block\n",
        "\n",
        "The input to the model should always be in number format as it is unable to process text. Tokenizer helps to convert these texts/prompts into token-ids (which is an index number representation of tokens in vocabulary). We'll be using a character-level tokenizer for our model building."
      ],
      "metadata": {
        "id": "1Pr7PYN20gjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Embeds a batch of token_ids generated from texts or prompts using tokenizers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size:int, embed_dim:int) -> None:\n",
        "        super(InputBlock, self).__init__()\n",
        "        # init embedding class from the input block\n",
        "        self.embeddings= nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h= self.embeddings(x)\n",
        "\n",
        "        return h # x[bsz,seq_len] -> h[bsz,seq_len,embed_dim]\n"
      ],
      "metadata": {
        "id": "JqPRKTd7ndUx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - The Decoder Block\n",
        "\n",
        "The embedding vector has many dimensions (4096 dim in LLaMA 3-8B) and there is always a chance of having values in different ranges. This can cause model gradients to explode or vanish hence resulting in slow convergence or even divergence."
      ],
      "metadata": {
        "id": "OBCWo0xV0l8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 - RMSNorm\n",
        "\n",
        "The **RMSNorm** brings these values into a certain range which helps to stabilize and accelerate the training process. This makes gradients have more consistent magnitudes and that results in making models converge more quickly.\n",
        "\n",
        "$$\\hat{a}_i = \\frac{a_i}{RMS(a)}g_i \\text{, where } RMS(a) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n{a_i^2}}$$\n",
        "\n",
        "Specifically, Pre-normalization using RMSNorm helps LLMs prioritize which parts of the text are more critical for understanding the context and meaning. It assigns higher weights to essential elements and lower weights to less crucial ones, ensuring the model focuses its attention where it's most needed for accurate comprehension."
      ],
      "metadata": {
        "id": "qOrEKi2NeEsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Root Mean Square normalization (RMSNorm).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim:int, eps:float=1e-6) -> None:\n",
        "        super(RMSNorm, self).__init__()\n",
        "        # scaling parameter gamma initialized with ones and the amount of parameters equal to dim\n",
        "        self.gamma= nn.Parameter(torch.ones(dim))\n",
        "        self.eps= eps\n",
        "\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm= self._norm(x.float()).type_as(x) # x[bs,seq,dim] -> x_norm[bs,seq,dim]\n",
        "\n",
        "        return x_norm * self.gamma\n"
      ],
      "metadata": {
        "id": "0tg31CbsFS4V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Test: RMSNorm Code --\n",
        "x= torch.randn((10, 256, 512), device=device)\n",
        "rms_norm= RMSNorm(512).to(device)\n",
        "x_norm= rms_norm(x)\n",
        "\n",
        "print(f\"Shape of x: {x.shape}\")\n",
        "print(f\"Shape of x_norm: {x_norm.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QZgqrYrFTDL",
        "outputId": "0e86bbdd-1667-49d4-8329-f94cb42fbd63"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x: torch.Size([10, 256, 512])\n",
            "Shape of x_norm: torch.Size([10, 256, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 - RoPE\n",
        "\n",
        "**Rotary Positional Encoding (RoPE):** Let's say the input text is \"I love apple\" or \"apple love I\", the model will still treat both sentences as the same and learn it as the same. Because there is no order defined in the embeddings for the model to learn. Hence, the order is very important for any language model. In LLaMA 3 model architecture, RoPE is used to define the position of each token in the sentences that maintain not only the order but also maintains the relative position of tokens in the sentences.\n",
        "\n",
        "So, what is Rotary Positional Encoding and how does it work? As mentioned in the why section above, RoPE is a type of position encoding that encodes the embeddings which maintains the order of tokens in the sentences by adding absolute positional information as well as incorporates the relative position information among the tokens. It performs the encoding action by rotating a given embedding by a special matrix called the rotation matrix. This simple yet very powerful mathematical derivation using rotation matrix is the heart of RoPE.\n",
        "\n",
        "**Note:** the rotation matrix needs to be converted to polar form and the embedding vector needs to converted to complex before performing rotation. After rotation is completed, the rotated embeddings need to be converted back to real for attention operation. Also, RoPE is applied to Query and Key embedding only. It doesn't apply to Value embedding."
      ],
      "metadata": {
        "id": "QbzU4iHaS1Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QK_RoPE(nn.Module):\n",
        "    \"\"\"\n",
        "    Define Rotary Positional Encoding (RoPE) functions for Query and Key embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, head_dim:int, max_seq_len:int, theta:float=10000.0) -> None:\n",
        "        super(QK_RoPE, self).__init__()\n",
        "        assert head_dim % 2 == 0, \"head_dim must be even for RoPE\"\n",
        "        self.head_dim= head_dim\n",
        "        self.max_seq_len= max_seq_len\n",
        "        self.theta= theta\n",
        "        self.device= None\n",
        "\n",
        "\n",
        "    def precompute_freqs_cis(self, dim, max_len):\n",
        "        # computing theta value for each dim pair which is dim/2\n",
        "        freqs= 1.0 / (self.theta ** (\n",
        "            torch.arange(0, dim, 2, device=self.device)[: (dim // 2)].float() / dim\n",
        "        ))\n",
        "        # computing range of positions(m) in the sequence\n",
        "        t= torch.arange(max_len, dtype=torch.float32, device=self.device)\n",
        "        # freqs gives all the theta value range for all the position of tokens in the sequence\n",
        "        freqs= torch.outer(t, freqs).to(self.device)\n",
        "        # the rotation matrix needs to be converted to Polar coordinates form in order to perform\n",
        "        # rotation to embedding\n",
        "        freqs_cis= torch.polar(torch.ones_like(freqs).to(self.device), freqs).to(self.device)\n",
        "\n",
        "        return freqs_cis\n",
        "\n",
        "\n",
        "    def reshape_for_broadcast(self, freqs_cis, x):\n",
        "        ndim= x.ndim\n",
        "        assert 0<= 1< ndim\n",
        "        assert freqs_cis.shape== (x.shape[1], x.shape[-1]), \"The last two dimension of freqs_cis and x must match\"\n",
        "        shape= [d if i==1 or i==ndim-1 else 1 for i,d in enumerate(x.shape)]\n",
        "\n",
        "        return freqs_cis.view(*shape)\n",
        "\n",
        "\n",
        "    def forward(self, q, k, start_pos, inference) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        if self.device is None:\n",
        "            self.device= q.device\n",
        "\n",
        "        bsz, seq_len, _, _= q.shape\n",
        "\n",
        "        if inference:\n",
        "            # compute rotation matrix for each position in the sequence\n",
        "            freqs_cis= self.precompute_freqs_cis(self.head_dim, self.max_seq_len * 2)\n",
        "            # during inference we should only take the rotation matrix range from the current\n",
        "            # position of the tokens\n",
        "            freqs_cis= freqs_cis[start_pos : start_pos + seq_len]\n",
        "        else:\n",
        "            # compute rotation matrix to query and key for training\n",
        "            freqs_cis= self.precompute_freqs_cis(self.head_dim, self.max_seq_len)\n",
        "\n",
        "        \"\"\"\n",
        "        Applying rotary positional encoding to both query and key embedding together.\n",
        "        - First: The last dimension of q and k embedding needs to be reshaped to make it a\n",
        "        pair. As rotation matrix is applied to each pair of dim.\n",
        "        - Next: convert both q and k to complex number as the rotation matrix is only\n",
        "        applicable to complex number.\n",
        "        \"\"\"\n",
        "        # q/k_ci[bsz,seq_len,n_heads,head_dim/2]\n",
        "        q_ci= torch.view_as_complex(q.float().reshape(*q.shape[:-1], -1, 2)).to(self.device)\n",
        "        k_ci= torch.view_as_complex(k.float().reshape(*k.shape[:-1], -1, 2)).to(self.device)\n",
        "\n",
        "        \"\"\"\n",
        "        The rotation matrix(freqs_cis) dimensions across seq_len(dim=1) and head_dim(dim=3) should\n",
        "        match with the embedding. Also, the shape freqs_cis should be the same with q and k,\n",
        "        hence change the shape of freqs_cis: [seq_len, head_dim] -> freqs_cis: [1, seq_len, 1, head_dim]\n",
        "        \"\"\"\n",
        "        freqs_cis= self.reshape_for_broadcast(freqs_cis, q_ci)\n",
        "\n",
        "        \"\"\"\n",
        "        Finally, perform rotation operation by multiplying with freqs_cis.\n",
        "        After the rotation is completed, convert both q_out and k_out back to real number and return.\n",
        "        \"\"\"\n",
        "        # q/k_out[bsz,seq_len,n_heads,head_dim]\n",
        "        q_out= torch.view_as_real(q_ci * freqs_cis).flatten(3).to(self.device)\n",
        "        k_out= torch.view_as_real(k_ci * freqs_cis).flatten(3).to(self.device)\n",
        "\n",
        "        return q_out.type_as(q), k_out.type_as(k)\n"
      ],
      "metadata": {
        "id": "kjDBjQ28kjDf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Test: RoPE Code --\n",
        "# Note: x_norm is calculated during RMSNorm and is being used for testing here\n",
        "head_dim= 512//8\n",
        "rope= QK_RoPE(head_dim, 256).to(device)\n",
        "\n",
        "wq= nn.Linear(512, 8 * head_dim, bias=False).to(device)\n",
        "wk= nn.Linear(512, 4 * head_dim, bias=False).to(device)\n",
        "xq= wq(x_norm)\n",
        "xk= wk(x_norm)\n",
        "print(f\"xq.shape: {xq.shape}\")\n",
        "print(f\"xk.shape: {xk.shape}\")\n",
        "\n",
        "xq= xq.view(xq.shape[0],xq.shape[1], 8, head_dim)\n",
        "xk= xk.view(xk.shape[0],xk.shape[1], 4, head_dim)\n",
        "print(f\"xq.re-shape: {xq.shape}\")\n",
        "print(f\"xk.re-shape: {xk.shape}\")\n",
        "\n",
        "freqs_cis= rope.precompute_freqs_cis(dim=head_dim, max_len=256)\n",
        "print(f\"freqs_cis.shape: {freqs_cis.shape}\")\n",
        "\n",
        "xq_rotate, xk_rotate= rope(xq, xk, 0, False)\n",
        "print(f\"xq_rotate.shape: {xq_rotate.shape}\")\n",
        "print(f\"xk_rotate.shape: {xk_rotate.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgl63GmxFTMH",
        "outputId": "1706307c-9c47-48a1-a440-4467742894d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xq.shape: torch.Size([10, 256, 512])\n",
            "xk.shape: torch.Size([10, 256, 256])\n",
            "xq.re-shape: torch.Size([10, 256, 8, 64])\n",
            "xk.re-shape: torch.Size([10, 256, 4, 64])\n",
            "freqs_cis.shape: torch.Size([256, 32])\n",
            "xq_rotate.shape: torch.Size([10, 256, 8, 64])\n",
            "xk_rotate.shape: torch.Size([10, 256, 4, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.3 - KV Cache\n",
        "\n",
        "**KV Cache (Only required at Inferencing):** In LLaMA 3 architecture, at the time of inferencing, the concept of KV-Cache is introduced to store previously generated tokens in the form of Key and Value cache. These caches will be used to calculate self-attention to generate the next token. Only key and value tokens are cached whereas query tokens are not cached, hence the term KV Cache."
      ],
      "metadata": {
        "id": "su0_aoSZh7NI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KV_Cache(nn.Module):\n",
        "    \"\"\"\n",
        "    KV Cache Implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_batch_size:int, max_seq_len:int, n_kv_heads:int, head_dim:int) -> None:\n",
        "        super(KV_Cache, self).__init__()\n",
        "        self.device= None\n",
        "        # initialize caches to store Key, Value at start\n",
        "        self.cache_k= torch.zeros((max_batch_size, max_seq_len, n_kv_heads, head_dim))\n",
        "        self.cache_v= torch.zeros((max_batch_size, max_seq_len, n_kv_heads, head_dim))\n",
        "\n",
        "\n",
        "    def cache_invalidation(self, batch_idx=None):\n",
        "\n",
        "        if batch_idx is None:\n",
        "            # invalidate the entire cache\n",
        "            self.cache_k.zero_()\n",
        "            self.cache_v.zero_()\n",
        "        else:\n",
        "            # invalidate only the cache for the given batch_idx\n",
        "            self.cache_k[batch_idx].zero_()\n",
        "            self.cache_v[batch_idx].zero_()\n",
        "\n",
        "\n",
        "    def forward(self, q, k, v, start_pos):\n",
        "        if self.device is None:\n",
        "            self.device = q.device\n",
        "            self.cache_k= self.cache_k.to(self.device)\n",
        "            self.cache_v= self.cache_v.to(self.device)\n",
        "\n",
        "        bsz, seq_len, _, _= v.shape\n",
        "\n",
        "        self.cache_k= self.cache_k.to(q)\n",
        "        self.cache_v= self.cache_v.to(q)\n",
        "        # store Key and Value token embeddings into their respective caches\n",
        "        self.cache_k[:bsz, start_pos : start_pos + seq_len]= k\n",
        "        self.cache_v[:bsz, start_pos : start_pos + seq_len]= v\n",
        "        # assign all the previous tokens embeddings upto current tokens position to Key and\n",
        "        # Value variable for Attention calculation\n",
        "        key  = self.cache_k[:bsz, : start_pos + seq_len]\n",
        "        value= self.cache_v[:bsz, : start_pos + seq_len]\n",
        "\n",
        "        return key, value\n"
      ],
      "metadata": {
        "id": "I1teG0o7Juw2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://towardsdatascience.com/transformers-key-value-kv-caching-explained-4d71de62d22d"
      ],
      "metadata": {
        "id": "laG1qp1WeJWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.4 - Group Query Attention\n",
        "\n",
        "**Group Query Attention** is the same as Muilt-Head Attention which was used in previous models such as LLaMA 1 with the only difference being in the use of separate heads for query and separate heads for key/value. Usually, the number of heads assigned to query is n-times to that of key, and value heads.\n",
        "\n",
        "Since Multi-Head Attention is already so good, why do we need Group query attention? To answer this, we need to go back to KV Cache for a while. The KV cache helps reduce computation resources greatly. However, as KV Cache stores more and more previous tokens, the memory resources will increase significantly. This is not a good thing for the model performance point of view as well as the financial point of view. Hence, Group query attention is introduced.\n",
        "\n",
        "Reducing the number of heads for K and V decreases the number of parameters to be stored, and hence, less memory is being used. Various test results have proven that the model accuracy remains in the same ranges with this approach."
      ],
      "metadata": {
        "id": "F2jRYz5NlB6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    The Group Query Self Attention Block [RoPE, KV Cache, and Group Query Attention].\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim:int, n_heads:int, n_kv_heads:int, max_batch_size:int,\n",
        "                 max_seq_len:int, rope_theta=10000.0) -> None:\n",
        "        super(MultiHeadedSelfAttention, self).__init__()\n",
        "        assert dim % n_heads == 0, \"model dim must be divisible by n_heads\"\n",
        "        # embedding dimension\n",
        "        self.dim= dim\n",
        "        # number of heads assigned to Query\n",
        "        self.n_heads= n_heads\n",
        "        # number of heads assigned to Key and Value. If None, the number will be same as Query\n",
        "        self.n_kv_heads= n_heads if n_kv_heads is None else n_kv_heads\n",
        "        # dimension of each head relative to model dimension\n",
        "        self.head_dim= dim // n_heads\n",
        "        # number of repetition in order to make time Key, Value heads to match Query heads number\n",
        "        self.n_rep= n_heads // n_kv_heads\n",
        "\n",
        "        # initialize Query, Key, Value and Oupt\n",
        "        # notice that the out_feature value of weight for q and kv are based on it's heads\n",
        "        self.q_proj= nn.Linear(dim,    n_heads * self.head_dim, bias=False)\n",
        "        self.k_proj= nn.Linear(dim, n_kv_heads * self.head_dim, bias=False)\n",
        "        self.v_proj= nn.Linear(dim, n_kv_heads * self.head_dim, bias=False)\n",
        "        self.o_proj= nn.Linear(n_heads * self.head_dim,    dim, bias=False)\n",
        "        # initialize Rotary Positional Encoding\n",
        "        self.rotary_emb= QK_RoPE(self.head_dim, max_seq_len, rope_theta)\n",
        "        # initialize KV Cache to store Key and Value\n",
        "        self.kv_cache= KV_Cache(max_batch_size, max_seq_len, n_kv_heads, self.head_dim)\n",
        "\n",
        "\n",
        "    def repeat_kv(self, x, n_rep):\n",
        "        # if the number of key/value heads is less than query heads, this function expands the\n",
        "        # key/value embeddings with the required number of repetition\n",
        "        bsz, seq_len, n_kv_heads, head_dim= x.shape\n",
        "\n",
        "        if n_rep== 1:\n",
        "            return x\n",
        "\n",
        "        return (\n",
        "            x[:,:,:,None,:]\n",
        "             .expand(bsz, seq_len,  n_kv_heads,  n_rep,  head_dim)\n",
        "            .reshape(bsz, seq_len, (n_kv_heads * n_rep), head_dim)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, start_pos, inference=True):\n",
        "        bsz, seq_len, dim= x.shape  # shape of the input embedding: [bsz, seq_len, dim]\n",
        "        assert dim == self.dim, \"Input embedding dimension must match model embedding dimension\"\n",
        "        # mask will be used during 'Training' and not for 'Inference' due to the use of KV cache\n",
        "        mask= None\n",
        "\n",
        "        q= self.q_proj(x) # x[bsz,seq_len,dim]*wq[dim,   n_heads*head_dim] -> q[bsz,seq_len,   n_heads*head_dim]\n",
        "        k= self.k_proj(x) # x[bsz,seq_len,dim]*wk[dim,n_kv_heads*head_dim] -> k[bsz,seq_len,n_kv_heads*head_dim]\n",
        "        v= self.v_proj(x) # x[bsz,seq_len,dim]*wv[dim,n_kv_heads*head_dim] -> v[bsz,seq_len,n_kv_heads*head_dim]\n",
        "        # reshaping Query, Key and Value by their number of heads\n",
        "        # (Group Query Attention Implementation)\n",
        "        q= q.view(bsz, seq_len, self.n_heads,    self.head_dim)\n",
        "        k= k.view(bsz, seq_len, self.n_kv_heads, self.head_dim)\n",
        "        v= v.view(bsz, seq_len, self.n_kv_heads, self.head_dim)\n",
        "\n",
        "        # apply RoPE to Query and Key embeddings\n",
        "        q, k= self.rotary_emb(q, k, start_pos, inference)\n",
        "\n",
        "        if inference:\n",
        "        # ----- Inference Mode: KV-Cache is enabled at inference mode only -----\n",
        "            k, v= self.kv_cache(q, k, v, start_pos)\n",
        "        else:\n",
        "        # ----- Training mode: KV-Cache is not enabled and the triangular masking is enabled -----\n",
        "            # performs Masked Attention on the outputs, so that positions depend on the past only\n",
        "            # create a lower triangular matrix (2-D tensor)\n",
        "            mask= torch.tril(torch.ones((seq_len, seq_len), device=x.device)).view(1, 1, seq_len, seq_len)\n",
        "\n",
        "        # at this point, Key and Value shapes aren't same with Query embedding which has to be\n",
        "        # in order to computer Attention scores\n",
        "        # use repeat_kv function to make Key/Value shape same as Query shape\n",
        "        k= self.repeat_kv(k, self.n_rep) # k[bsz,seq_len,n_heads,head_dim]\n",
        "        v= self.repeat_kv(v, self.n_rep) # v[bsz,seq_len,n_heads,head_dim]\n",
        "\n",
        "        # to compute Attention, we'll need to perform a transpose operation to reshape all\n",
        "        # Query, Key, and Value, bring heads at dim 1 and seq at dim 2\n",
        "        q= q.transpose(1, 2)  # q[bsz,n_heads,seq_len,head_dim]\n",
        "        k= k.transpose(1, 2)  # k[bsz,n_heads,seq_len,head_dim]\n",
        "        v= v.transpose(1, 2)  # v[bsz,n_heads,seq_len,head_dim]\n",
        "\n",
        "        # computing Attention scores - the 'scaled dot product'\n",
        "        attn= (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
        "        # apply Attention mask (when the mask is not None)\n",
        "        if mask is not None:\n",
        "            attn= attn.masked_fill(mask== 0, float('-inf'))\n",
        "        # apply softmax to normalize the Attention scores\n",
        "        attn= F.softmax(attn, dim=-1)\n",
        "        # compute Attention output -> y[bsz,n_heads,seq_len,head_dim]\n",
        "        y= attn @ v\n",
        "        # we get the contextual embedding for each head, then all heads need to be reshaped and\n",
        "        # combined to give a single contextual Attention output\n",
        "        y= y.transpose(1, 2).contiguous().view(bsz, seq_len, -1)\n",
        "        # output projection\n",
        "        y= self.o_proj(y) # y[bsz,seq_len,dim]\n",
        "\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "RQ-ahz5lFTGX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Test: Repeat_kv function --\n",
        "# xk, x_norm is already calculated during RoPE, RMSNorm testing and is being used for testing here\n",
        "rep= MultiHeadedSelfAttention(512, 8, 4, 10, 256).to(device)\n",
        "n_rep= 8 // 4\n",
        "key= rep.repeat_kv(xk, n_rep)\n",
        "print(f\"xk.shape: {xk.shape}\")\n",
        "print(f\"key.shape: {key.shape}\")\n",
        "\n",
        "# -- Test: Attention function --\n",
        "attn= MultiHeadedSelfAttention(512, 8, 4, 10, 256).to(device)\n",
        "x_out= attn(x_norm, start_pos=0, inference=False)\n",
        "print(f\"x_out.shape: {x_out.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N398w6qFTP5",
        "outputId": "38a3ea82-6e0b-4125-ef56-fdb0abea063b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xk.shape: torch.Size([10, 256, 4, 64])\n",
            "key.shape: torch.Size([10, 256, 8, 64])\n",
            "x_out.shape: torch.Size([10, 256, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.5 - FeedForward Network\n",
        "\n",
        "The Attention output is first normalized during RMSNorm and then fed into the FeedForward network. Inside the feedforward network, the attention output embeddings will be expanded to the higher dimension throughout its hidden layers and learn more complex features of the tokens.\n",
        "\n",
        "The **SwiGLU Activation** function behaves almost like ReLU in the positive axis. However, in the negative axis, SwiGLU outputs some negative values, which might be useful in learning smaller rather than flat 0 in the case of ReLU. Overall, as per the author, the performance with SwiGLU has been better than that with ReLU; hence, it was chosen.\n",
        "\n",
        "- The **Gated Linear Unit (GLU)** architecture is commonly used in modern neural networks, including LLaMA, Gemma, Mistral, Qwen and similar large language models. GLU introduces an element-wise gating mechanism that allows the model to selectively filter and control the flow of information. This architecture consists of paired layers, typically: **gate_proj**, **up_proj**, and **down_proj** (as seen in the FFW network class), that work together to expand and contract data. This mechanism enables the model to process more complex patterns while maintaining efficiency."
      ],
      "metadata": {
        "id": "AVJA6w_J2KLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    The Feedfoward Network with SwiGLU activation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim:int, hidden_dim:int, multiple_of:int,\n",
        "                 ffn_dim_multiplier:Optional[float]=None) -> None:\n",
        "        super(FeedForward, self).__init__()\n",
        "        # using the hidden dimensions calculation shared by Meta which is the ideal one for\n",
        "        # this model. hidden dimension are calculated such that it is a multiple of 256\n",
        "        hidden_dim= int(2 * hidden_dim/3)\n",
        "        if ffn_dim_multiplier is not None:\n",
        "            hidden_dim= int(ffn_dim_multiplier * hidden_dim)\n",
        "        hidden_dim= multiple_of * ((hidden_dim + multiple_of -1) // multiple_of)\n",
        "\n",
        "        self.gate_proj= nn.Linear(dim, hidden_dim, bias=False)\n",
        "        self.up_proj  = nn.Linear(dim, hidden_dim, bias=False)\n",
        "        self.down_proj= nn.Linear(hidden_dim, dim, bias=False)\n",
        "        self.act_fn= nn.SiLU()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.act_fn(self.gate_proj(x)) * self.up_proj(x)\n",
        "        x= self.down_proj(x)\n",
        "\n",
        "        return x # x[bsz,seq_len,dim]\n"
      ],
      "metadata": {
        "id": "j4p9UG5MFTSt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Test: FeedForward module --\n",
        "# x_out is already computed at Attention testing and is being used for testing here\n",
        "feed_forward= FeedForward(512, 4 * 512, 256, None).to(device)\n",
        "x_out= rms_norm(x_out)\n",
        "x_out= feed_forward(x_out)\n",
        "print(f\"feed forward output: x_out.shape: {x_out.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUtmiRDWFTWU",
        "outputId": "55a798a3-7f94-4262-d617-e341823ea91a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feed forward output: x_out.shape: torch.Size([10, 256, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.6 - Building the Decoder Block\n",
        "\n",
        "- The embedding from the input block is fed into the Attention-RMSNorm block. This will be further fed into the Group Query Attention block.\n",
        "- The same embedding from the input block will then be added to the attention output.\n",
        "- After that, the attention output is fed into FeedFoward-RMSNorm and further fed into the FeedFoward network block.\n",
        "- The output of the FeedFoward network is then added again with the attention output.\n",
        "- The resulting output is called Decoder Output. This decoder output is then fed into another decoder block as input. This same operation will be repeated for the next 31 decoder blocks. The final decoder output of the 32nd decoder block is then passed to the Output block."
      ],
      "metadata": {
        "id": "Mp1m8T_Vd86y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    The Decoder Block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim:int, n_heads:int, n_kv_heads:int, max_batch_size:int, max_seq_len:int,\n",
        "                 multiple_of:int, ffn_dim_multiplier:Optional[float]=None, norm_eps:float=1e-5,\n",
        "                 rope_theta=10000.0) -> None:\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        # init RMSNorm for attention\n",
        "        self.attn_norm= RMSNorm(dim, eps=norm_eps)\n",
        "        # init the Attention class\n",
        "        self.attn= MultiHeadedSelfAttention(\n",
        "            dim, n_heads, n_kv_heads, max_batch_size, max_seq_len, rope_theta\n",
        "        )\n",
        "        # init RMSNorm for the FeedForward class\n",
        "        self.ffwd_norm= RMSNorm(dim, eps=norm_eps)\n",
        "        # init the FeedForward class\n",
        "        self.ffwd= FeedForward(dim, (4 * dim), multiple_of, ffn_dim_multiplier)\n",
        "\n",
        "\n",
        "    def forward(self, x, start_pos, inference):\n",
        "        # start_pos = token position for inference\n",
        "        # inference = True for inference and False for training\n",
        "        # i) pass input embedding to attn_norm and then pass to attention block\n",
        "        # ii) the output of attention is then added to embedding(before norm)\n",
        "        x= x + self.attn(self.attn_norm(x), start_pos, inference)\n",
        "\n",
        "        # i) pass attention output to ffwd_norm and then pass to the feedforward network\n",
        "        # ii) the output of feedforward is then added to the attention output(before ffwd_norm)\n",
        "        x= x + self.ffwd(self.ffwd_norm(x))\n",
        "\n",
        "        return x # x[bsz,seq_len,dim]\n"
      ],
      "metadata": {
        "id": "gLn1iFYLndXb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Test: DecoderBlock --\n",
        "x= torch.randn((10, 256, 512), device=device)\n",
        "dec= DecoderBlock(512, 8, 4, 10, 256, 256, None, 1e-5).to(device)\n",
        "dec_out= dec(x, start_pos=0, inference=False)\n",
        "print(f\"transformer_block_out.shape: {dec_out.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt2HMNMMgGFP",
        "outputId": "8ae95afe-c699-43bf-e6a6-4fd25dff031d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer_block_out.shape: torch.Size([10, 256, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - The Output Block\n",
        "\n",
        "The decoder output of the final decoder block will feed into the output block. It is first fed into the RMSNorm. Then, it will feed into the Linear Layer which generates logits. Next, one of the following two operations happens.\n",
        "- If the mode is inference, top_p probability is calculated and the next token is generated. The next tokens generated will stop if the max generation length is reached or the end of sentence token is generated as the next token.\n",
        "- If the mode is Training, loss is computed with the target labels and training is repeated till the max steps length is reached."
      ],
      "metadata": {
        "id": "Z35y02sG0qMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OutputBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim:int, vocab_size:int, norm_eps:float=1e-5) -> None:\n",
        "        super(OutputBlock, self).__init__()\n",
        "        # init RMSNorm for the output block\n",
        "        self.out_norm= RMSNorm(dim, eps=norm_eps)\n",
        "        # init linear layer at the output block\n",
        "        self.output= nn.Linear(dim, vocab_size, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # the output from the final decoder block will feed into the RMSNorm\n",
        "        x= self.out_norm(x)\n",
        "        # after normalized, the embedding x will then feed into the Linear layer\n",
        "        x= self.output(x).float()\n",
        "\n",
        "        return x # x[bsz,seq_len,dim] -> logits[bsz,seq_len,vocab_size]\n"
      ],
      "metadata": {
        "id": "jLF5hrJ8ndaT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Building the Model\n",
        "\n",
        "Finally, let's combine all components of 3 blocks (input block, decoder block and output blocks. This gives our final LLaMA 3 model."
      ],
      "metadata": {
        "id": "bzhEs7SZ0tfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Llama3(nn.Module):\n",
        "    \"\"\"\n",
        "    Define the final LLaMA 3 model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size:int, dim:int, n_layers:int, n_heads:int, n_kv_heads:int,\n",
        "                 max_batch_size:int, max_seq_len:int, multiple_of:int,\n",
        "                 ffn_dim_multiplier:Optional[float]=None, norm_eps:float=1e-5,\n",
        "                 rope_theta=10000.0) -> None:\n",
        "        super(Llama3, self).__init__()\n",
        "        self.vocab_size= vocab_size\n",
        "        self.max_seq_len= max_seq_len\n",
        "\n",
        "        self.embed_tokens= InputBlock(vocab_size, dim)\n",
        "        # init the decoder block and store it inside a ModuleList.\n",
        "        # decoder blocks in our LLaMA 3 model (official LLaMA 3 has 32 blocks)\n",
        "        self.decoder_layers= nn.ModuleList([\n",
        "            DecoderBlock(dim, n_heads, n_kv_heads, max_batch_size, max_seq_len, multiple_of,\n",
        "                         ffn_dim_multiplier, norm_eps, rope_theta)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        # projection from dim numbers of embedding dimensions to vocab_size\n",
        "        self.output= OutputBlock(dim, vocab_size, norm_eps)\n",
        "\n",
        "        # initialize Linear modules with Glorot / fan_avg\n",
        "        # let RMSNorm and Embedding modules use default initializations\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "    def forward(self, x, start_pos=0, targets=None):\n",
        "        # start_pos = token position for inference mode\n",
        "        # inference = True for inference and False for training mode\n",
        "        # x is the batch of token_ids generated from the texts or prompts using tokenizers\n",
        "        bsz, seq_len= x.size()\n",
        "        assert seq_len <= self.max_seq_len, f'Cannot forward sequence of length {seq_len}, block size is only {self.max_seq_len}'\n",
        "        # if the target is none, inference mode is activated and set to 'True'\n",
        "        # and 'False' if training mode is activated\n",
        "        inference=True if targets is None else False\n",
        "\n",
        "        h= self.embed_tokens(x) # x[bsz,seq_len] -> h[bsz,seq_len,dim]\n",
        "\n",
        "        # the embeddings (h) will then pass though all the decoder blocks\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            h= decoder_layer(h, start_pos, inference)\n",
        "\n",
        "        # the output block generates logits that map the embeddings (h) with the vocabulary size\n",
        "        logits= self.output(h)\n",
        "\n",
        "        # inference mode is activated if the targets is not available, training mode is activated\n",
        "        # if the targets are available (and Loss will be calculated for further model training)\n",
        "        if targets is None:\n",
        "            loss= None\n",
        "        else:\n",
        "            loss= F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n"
      ],
      "metadata": {
        "id": "XNDWiKSnndgr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 - Creating a LLaMA 3-8B Model\n",
        "\n",
        "It requires more than 30 GB of memory only to create the 8B version."
      ],
      "metadata": {
        "id": "JjAtyd-c8fPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- LLaMA 3-8B Model defs -----\n",
        "dim:int= 4096                   # embedding dimension\n",
        "n_layers:int= 32                # number of model decoder blocks\n",
        "n_heads:int= 32                 # number of heads for query embedding\n",
        "n_kv_heads:int= 8               # number of heads for key and value embeddings\n",
        "vocab_size:int= 128256          # Length of vocabulary\n",
        "multiple_of:int= 1024           # require to calculate dim of feedfoward network\n",
        "ffn_dim_multiplier:Optional[float]= 1.3  # require to calculate dim of feedfoward network\n",
        "norm_eps:float= 1e-5            # default Epsilon value set for the RMSNorm calculation\n",
        "rope_theta:float= 500000.0      # default theta value for the RePE calculation\n",
        "\n",
        "max_batch_size:int= 32          # max batch size\n",
        "max_seq_len:int= 512            # max sequence length\n",
        "\n",
        "\n",
        "if device== 'cuda': # TF32 computationally more efficient (slightly the same precision of FP32)\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "model= Llama3(vocab_size, dim, n_layers, n_heads, n_kv_heads, max_batch_size, max_seq_len,\n",
        "              multiple_of, ffn_dim_multiplier, norm_eps, rope_theta).to(device)\n",
        "\n",
        "count_parameters(model)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pp5nOn15mnQx",
        "outputId": "c2a3ff0e-1064-41f2-91b6-f2a0efd2edf1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 8030261248\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Llama3(\n",
              "  (embed_tokens): InputBlock(\n",
              "    (embeddings): Embedding(128256, 4096)\n",
              "  )\n",
              "  (decoder_layers): ModuleList(\n",
              "    (0-31): 32 x DecoderBlock(\n",
              "      (attn_norm): RMSNorm()\n",
              "      (attn): MultiHeadedSelfAttention(\n",
              "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        (rotary_emb): QK_RoPE()\n",
              "        (kv_cache): KV_Cache()\n",
              "      )\n",
              "      (ffwd_norm): RMSNorm()\n",
              "      (ffwd): FeedForward(\n",
              "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "        (act_fn): SiLU()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (output): OutputBlock(\n",
              "    (out_norm): RMSNorm()\n",
              "    (output): Linear(in_features=4096, out_features=128256, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(model.parameters()).is_cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGLksA10RKna",
        "outputId": "1c4c3ee9-3165-40ef-8ba4-9d6958591cc9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data\n",
        "\n",
        "We'll use the popular Tiny Shakespeare dataset to build the vocabulary and also train our model."
      ],
      "metadata": {
        "id": "jExyLGs2ucCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text= f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urMws3Btohme",
        "outputId": "6bfaed3e-4ae9-4b79-877e-8f4d9388dfad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-06 14:54:59--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2024-12-06 14:54:59 (157 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer\n",
        "\n",
        "The input to the model should always be in number format as it is unable to process text. Tokenizer helps to convert these texts/prompts into token-ids (which is an index number representation of tokens in vocabulary).\n",
        "\n",
        "The tokenizer used in the LLaMA 3 model is TikToken, a type of subword tokenizer. However, we'll be using a character-level tokenizer for our model building. The main reason is that we should know how to build a vocabulary and tokenizer including encode and decode functions all by ourselves. This way we'll be able to learn how everything works under the hood and we'll have full control over the code."
      ],
      "metadata": {
        "id": "l3B-2DAqiXK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    \"\"\"\n",
        "    Tokenizer class to transform strings into integers and vice versa.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device:str, vocab) -> None:\n",
        "        # training LLaMA 3 model requires some aditional tokens\n",
        "        special_tokens= ['<|begin_of_text|>','<|end_of_text|>','<|pad_id|>']\n",
        "        vocab.extend(special_tokens)\n",
        "        self.vocab_size= len(vocab)\n",
        "\n",
        "        # create a mapping between characters with corresponding integers indexes in vocabulary\n",
        "        # this is required to build encode and decode functions of the tokenizer\n",
        "        self.itos= {i:ch for i, ch in enumerate(vocab)}\n",
        "        self.stoi= {ch:i for i, ch in enumerate(vocab)}\n",
        "\n",
        "        # define tensor with special token variables to be used during model training\n",
        "        self.token_bos= torch.tensor([self.stoi[special_tokens[0]]], dtype=torch.int, device=device)\n",
        "        self.token_eos= torch.tensor([self.stoi[special_tokens[1]]], dtype=torch.int, device=device)\n",
        "        self.token_pad= torch.tensor([self.stoi[special_tokens[2]]], dtype=torch.int, device=device)\n",
        "\n",
        "\n",
        "    def encode(self, s):\n",
        "        \"\"\"\n",
        "        Tokenizer encode function: take a string and output a list of integers.\n",
        "        \"\"\"\n",
        "        return [self.stoi[ch] for ch in s]\n",
        "\n",
        "\n",
        "    def decode(self, x):\n",
        "        \"\"\"\n",
        "        Tokenizer decode function: take a list of integers and output a string.\n",
        "        \"\"\"\n",
        "        return ''.join(self.itos[i] for i in x)\n"
      ],
      "metadata": {
        "id": "TS6akgiXzV0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the vocabulary by taking all the unique characters from the data\n",
        "vocab= sorted(list(set(text)))\n",
        "\n",
        "tk= Tokenizer(device, vocab)\n",
        "\n",
        "prompt= 'Hello World'\n",
        "encoded_tokens= tk.encode(prompt)\n",
        "decoded_text= tk.decode(encoded_tokens)\n",
        "\n",
        "# -- Test: Input Block Code --\n",
        "print(f\"Lenth of Shakespeare data in characters: {len(text)}\")\n",
        "print(f\"The vocabulary looks like this: {''.join(vocab)}\\n\")\n",
        "print(f\"Vocab size: {tk.vocab_size}\")\n",
        "print(f\"encoded_tokens: {encoded_tokens}\")\n",
        "print(f\"decoded_text: {decoded_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b-TgW6-nsQo",
        "outputId": "de5f9161-3925-4c2e-849f-6627af525f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lenth of Shakespeare data in characters: 1115394\n",
            "The vocabulary looks like this: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz<|begin_of_text|><|end_of_text|><|pad_id|>\n",
            "\n",
            "Vocab size: 68\n",
            "encoded_tokens: [20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\n",
            "decoded_text: Hello World\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training our LLaMA 3 Model"
      ],
      "metadata": {
        "id": "54urX_nmxw0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generating batches from the given dataset\n",
        "def get_dataset_batch(data:torch.Tensor, split:str, tk:Tokenizer, max_seq_len:int,\n",
        "                      max_batch_size:int):\n",
        "    train= data[: int(0.8 * len(data))]\n",
        "    val  = data[int(0.8 * len(data)) : int(0.9 * len(data))]\n",
        "    test = data[int(0.9 * len(data)) :]\n",
        "\n",
        "    batch_data= train\n",
        "    if split== 'val':\n",
        "        batch_data= val\n",
        "    if split== 'test':\n",
        "        batch_data= test\n",
        "\n",
        "    # random starting points from the dataset to give random samples for training, val and test\n",
        "    ix= torch.randint(0, (len(batch_data) - max_seq_len - 3), (max_batch_size,)).to(data.device)\n",
        "    x= torch.stack([torch.cat([tk.token_bos, batch_data[i : i+max_seq_len-1]]) for i in ix]).long()\n",
        "    y= torch.stack([torch.cat([batch_data[i+1 : i+max_seq_len], tk.token_eos]) for i in ix]).long()\n",
        "\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "r09-H6tAndjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataset by encoding the entire tiny_shakespeare data token_ids list using the\n",
        "# tokenizer's encode function that we've built at the input block section\n",
        "dataset= torch.tensor(tk.encode(text), dtype=torch.int).to(device)\n",
        "print(f'Dataset-shape: {dataset.shape}')\n",
        "\n",
        "# -- Test: get_dataset function --\n",
        "xs, ys= get_dataset_batch(dataset, 'train', tk, max_seq_len, max_batch_size)\n",
        "print('--- INPUT ---')\n",
        "print(tk.decode(xs[0].tolist()))\n",
        "print('\\n--- TARGET ---')\n",
        "print(tk.decode(ys[0].tolist()))"
      ],
      "metadata": {
        "id": "pMtFRKSmndoj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25224daa-87d9-4ec9-92b6-9f5d7faf4dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset-shape: torch.Size([1115394])\n",
            "--- INPUT ---\n",
            "<|begin_of_text|>piteous corse;\n",
            "Pale, pale as ashes, all bedaub'd in blood,\n",
            "All in gore-blood; I swounded at the sight.\n",
            "\n",
            "JULIET:\n",
            "O, break, my heart! poor bankrupt, break at once!\n",
            "To prison, eyes, ne'er look on liberty!\n",
            "Vile earth, to earth resign; end motion here;\n",
            "And tho\n",
            "\n",
            "--- TARGET ---\n",
            "iteous corse;\n",
            "Pale, pale as ashes, all bedaub'd in blood,\n",
            "All in gore-blood; I swounded at the sight.\n",
            "\n",
            "JULIET:\n",
            "O, break, my heart! poor bankrupt, break at once!\n",
            "To prison, eyes, ne'er look on liberty!\n",
            "Vile earth, to earth resign; end motion here;\n",
            "And thou<|end_of_text|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate loss function to calculate and store training and validation loss for logging\n",
        "@torch.no_grad()\n",
        "def eval_loss(model, data:torch.Tensor, eval_iters:int, tk:Tokenizer, max_seq_len:int,\n",
        "              max_batch_size:int):\n",
        "    out= {}\n",
        "    model.train(False)\n",
        "\n",
        "    for split in ['train', 'val']:\n",
        "        losses= []\n",
        "        for _ in range(eval_iters):\n",
        "            Xb, Yb= get_dataset_batch(data, split, tk, max_seq_len, max_batch_size)\n",
        "            _, loss= model(Xb, targets=Yb)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        out[split]= np.mean(losses)\n",
        "\n",
        "    model.train(True)\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "e1srJfaHndr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# define a training function to perform model training\n",
        "def self_supervised_train(model, data:torch.Tensor, optimizer:torch.optim, tk:Tokenizer,\n",
        "                          max_seq_len:int, max_batch_size:int, steps:int=500, eval_interval:int=10,\n",
        "                          eval_iters:int=10, verbose=True):\n",
        "    losses= []\n",
        "\n",
        "    # --- training loop ---\n",
        "    for step in range(steps):\n",
        "        start_time= time.time()\n",
        "\n",
        "        # --- minibatch construction ---\n",
        "        Xb, Yb= get_dataset_batch(data, 'train', tk, max_seq_len, max_batch_size)\n",
        "\n",
        "        # --- forward pass and get loss ---\n",
        "        model.train(True)\n",
        "        logits, loss= model(Xb, targets=Yb)\n",
        "\n",
        "        # --- backward pass to calculate the gradients ---\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # --- update the parameters using the gradient ---\n",
        "        optimizer.step()\n",
        "\n",
        "        # --- evaluation and track stats ---\n",
        "        if step% eval_interval== 0:\n",
        "            model.train(False)\n",
        "            batch_time= time.time() - start_time\n",
        "            loss_eval= eval_loss(model, data, eval_iters, tk, max_seq_len, max_batch_size)\n",
        "            losses += [loss_eval]\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Step {step} | train loss {loss_eval['train']:.3f}, val loss {loss_eval['val']:.3f} | Time {batch_time:.3f}\")\n",
        "\n",
        "    if verbose: # print the final validation loss\n",
        "        print('Validation loss: ', losses[-1]['val'])\n",
        "\n",
        "    return losses\n"
      ],
      "metadata": {
        "id": "JxeJuJ3P5-I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start training with the following code block and observe the training results in the plot once the training is completed."
      ],
      "metadata": {
        "id": "_qO3d7WLI214"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- reseting a smaller model -----\n",
        "dim:int= 512                    # embedding dimension\n",
        "n_layers:int= 8                 # number of model decoder blocks\n",
        "n_heads:int= 8                  # number of heads for query embedding\n",
        "n_kv_heads:int= 4               # number of heads for key and value embeddings\n",
        "vocab_size:int= tk.vocab_size   # Length of vocabulary\n",
        "multiple_of:int= 256            # require to calculate dim of feedfoward network\n",
        "ffn_dim_multiplier:Optional[float]= None  # require to calculate dim of feedfoward network\n",
        "norm_eps:float= 1e-5            # default Epsilon value set for the RMSNorm calculation\n",
        "rope_theta:float= 10000.0       # default theta value for the RePE calculation\n",
        "\n",
        "max_batch_size:int= 16          # max batch size\n",
        "max_seq_len:int= 256            # max sequence length\n",
        "\n",
        "\n",
        "model= Llama3(vocab_size, dim, n_layers, n_heads, n_kv_heads, max_batch_size, max_seq_len,\n",
        "              multiple_of, ffn_dim_multiplier, norm_eps, rope_theta).to(device)\n",
        "\n",
        "\n",
        "# ----- Training defs -----\n",
        "steps= 2500       # total number of training iterations\n",
        "eval_interval= 10 # number of intervals to print the logs and loss values\n",
        "eval_iters= 100   # total number of evaluation iterations at each eval_interval\n",
        "\n",
        "learning_rate= 1e-3\n",
        "weight_decay= 5e-4\n",
        "# create a PyTorch optimizer\n",
        "optimizer= torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "losses= self_supervised_train(model, dataset, optimizer, tk, max_seq_len, max_batch_size, steps,\n",
        "                              eval_interval, eval_iters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG4S9yOl5-Oy",
        "outputId": "4b4a4211-0aad-4455-f016-3166cc49a3cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | train loss 6.884, val loss 6.885 | Time 0.053\n",
            "Epoch 10 | train loss 3.906, val loss 3.921 | Time 0.156\n",
            "Epoch 20 | train loss 3.362, val loss 3.369 | Time 0.160\n",
            "Epoch 30 | train loss 3.285, val loss 3.289 | Time 0.155\n",
            "Epoch 40 | train loss 3.236, val loss 3.244 | Time 0.156\n",
            "Epoch 50 | train loss 3.217, val loss 3.221 | Time 0.154\n",
            "Epoch 60 | train loss 3.197, val loss 3.200 | Time 0.152\n",
            "Epoch 70 | train loss 3.173, val loss 3.175 | Time 0.154\n",
            "Epoch 80 | train loss 3.129, val loss 3.139 | Time 0.162\n",
            "Epoch 90 | train loss 3.074, val loss 3.073 | Time 0.159\n",
            "Epoch 100 | train loss 3.039, val loss 3.039 | Time 0.159\n",
            "Epoch 110 | train loss 3.026, val loss 3.026 | Time 0.157\n",
            "Epoch 120 | train loss 3.005, val loss 3.004 | Time 0.156\n",
            "Epoch 130 | train loss 2.980, val loss 2.981 | Time 0.155\n",
            "Epoch 140 | train loss 2.965, val loss 2.966 | Time 0.160\n",
            "Epoch 150 | train loss 2.945, val loss 2.958 | Time 0.154\n",
            "Epoch 160 | train loss 2.925, val loss 2.936 | Time 0.158\n",
            "Epoch 170 | train loss 2.908, val loss 2.933 | Time 0.156\n",
            "Epoch 180 | train loss 2.894, val loss 2.914 | Time 0.156\n",
            "Epoch 190 | train loss 2.869, val loss 2.885 | Time 0.157\n",
            "Epoch 200 | train loss 2.865, val loss 2.880 | Time 0.157\n",
            "Epoch 210 | train loss 2.847, val loss 2.875 | Time 0.158\n",
            "Epoch 220 | train loss 2.837, val loss 2.862 | Time 0.157\n",
            "Epoch 230 | train loss 2.806, val loss 2.833 | Time 0.153\n",
            "Epoch 240 | train loss 2.796, val loss 2.829 | Time 0.167\n",
            "Epoch 250 | train loss 2.767, val loss 2.804 | Time 0.156\n",
            "Epoch 260 | train loss 2.753, val loss 2.805 | Time 0.155\n",
            "Epoch 270 | train loss 2.746, val loss 2.790 | Time 0.157\n",
            "Epoch 280 | train loss 2.727, val loss 2.782 | Time 0.160\n",
            "Epoch 290 | train loss 2.715, val loss 2.765 | Time 0.157\n",
            "Epoch 300 | train loss 2.691, val loss 2.753 | Time 0.157\n",
            "Epoch 310 | train loss 2.674, val loss 2.739 | Time 0.143\n",
            "Epoch 320 | train loss 2.661, val loss 2.720 | Time 0.157\n",
            "Epoch 330 | train loss 2.636, val loss 2.709 | Time 0.160\n",
            "Epoch 340 | train loss 2.630, val loss 2.707 | Time 0.156\n",
            "Epoch 350 | train loss 2.630, val loss 2.701 | Time 0.156\n",
            "Epoch 360 | train loss 2.626, val loss 2.710 | Time 0.154\n",
            "Epoch 370 | train loss 2.608, val loss 2.666 | Time 0.156\n",
            "Epoch 380 | train loss 2.592, val loss 2.661 | Time 0.177\n",
            "Epoch 390 | train loss 2.586, val loss 2.637 | Time 0.156\n",
            "Epoch 400 | train loss 2.565, val loss 2.648 | Time 0.145\n",
            "Epoch 410 | train loss 2.554, val loss 2.650 | Time 0.153\n",
            "Epoch 420 | train loss 2.556, val loss 2.622 | Time 0.154\n",
            "Epoch 430 | train loss 2.542, val loss 2.634 | Time 0.153\n",
            "Epoch 440 | train loss 2.538, val loss 2.636 | Time 0.156\n",
            "Epoch 450 | train loss 2.517, val loss 2.605 | Time 0.153\n",
            "Epoch 460 | train loss 2.540, val loss 2.609 | Time 0.155\n",
            "Epoch 470 | train loss 2.516, val loss 2.596 | Time 0.157\n",
            "Epoch 480 | train loss 2.495, val loss 2.582 | Time 0.154\n",
            "Epoch 490 | train loss 2.484, val loss 2.595 | Time 0.155\n",
            "Epoch 500 | train loss 2.496, val loss 2.595 | Time 0.155\n",
            "Epoch 510 | train loss 2.469, val loss 2.572 | Time 0.157\n",
            "Epoch 520 | train loss 2.482, val loss 2.576 | Time 0.148\n",
            "Epoch 530 | train loss 2.459, val loss 2.559 | Time 0.154\n",
            "Epoch 540 | train loss 2.445, val loss 2.540 | Time 0.156\n",
            "Epoch 550 | train loss 2.453, val loss 2.545 | Time 0.156\n",
            "Epoch 560 | train loss 2.435, val loss 2.535 | Time 0.158\n",
            "Epoch 570 | train loss 2.441, val loss 2.532 | Time 0.156\n",
            "Epoch 580 | train loss 2.442, val loss 2.520 | Time 0.158\n",
            "Epoch 590 | train loss 2.419, val loss 2.522 | Time 0.157\n",
            "Epoch 600 | train loss 2.430, val loss 2.533 | Time 0.158\n",
            "Epoch 610 | train loss 2.419, val loss 2.525 | Time 0.156\n",
            "Epoch 620 | train loss 2.407, val loss 2.499 | Time 0.152\n",
            "Epoch 630 | train loss 2.403, val loss 2.498 | Time 0.157\n",
            "Epoch 640 | train loss 2.397, val loss 2.518 | Time 0.156\n",
            "Epoch 650 | train loss 2.388, val loss 2.512 | Time 0.145\n",
            "Epoch 660 | train loss 2.393, val loss 2.513 | Time 0.156\n",
            "Epoch 670 | train loss 2.402, val loss 2.494 | Time 0.166\n",
            "Epoch 680 | train loss 2.386, val loss 2.501 | Time 0.158\n",
            "Epoch 690 | train loss 2.393, val loss 2.499 | Time 0.154\n",
            "Epoch 700 | train loss 2.376, val loss 2.500 | Time 0.156\n",
            "Epoch 710 | train loss 2.376, val loss 2.509 | Time 0.155\n",
            "Epoch 720 | train loss 2.348, val loss 2.493 | Time 0.154\n",
            "Epoch 730 | train loss 2.361, val loss 2.462 | Time 0.143\n",
            "Epoch 740 | train loss 2.357, val loss 2.467 | Time 0.153\n",
            "Epoch 750 | train loss 2.352, val loss 2.449 | Time 0.161\n",
            "Epoch 760 | train loss 2.350, val loss 2.461 | Time 0.160\n",
            "Epoch 770 | train loss 2.362, val loss 2.493 | Time 0.153\n",
            "Epoch 780 | train loss 2.325, val loss 2.472 | Time 0.146\n",
            "Epoch 790 | train loss 2.336, val loss 2.452 | Time 0.163\n",
            "Epoch 800 | train loss 2.345, val loss 2.462 | Time 0.166\n",
            "Epoch 810 | train loss 2.336, val loss 2.445 | Time 0.149\n",
            "Epoch 820 | train loss 2.329, val loss 2.442 | Time 0.166\n",
            "Epoch 830 | train loss 2.332, val loss 2.458 | Time 0.145\n",
            "Epoch 840 | train loss 2.326, val loss 2.448 | Time 0.152\n",
            "Epoch 850 | train loss 2.315, val loss 2.441 | Time 0.152\n",
            "Epoch 860 | train loss 2.310, val loss 2.438 | Time 0.164\n",
            "Epoch 870 | train loss 2.320, val loss 2.457 | Time 0.167\n",
            "Epoch 880 | train loss 2.318, val loss 2.427 | Time 0.158\n",
            "Epoch 890 | train loss 2.309, val loss 2.426 | Time 0.155\n",
            "Epoch 900 | train loss 2.305, val loss 2.432 | Time 0.146\n",
            "Epoch 910 | train loss 2.304, val loss 2.429 | Time 0.156\n",
            "Epoch 920 | train loss 2.324, val loss 2.461 | Time 0.148\n",
            "Epoch 930 | train loss 2.311, val loss 2.459 | Time 0.158\n",
            "Epoch 940 | train loss 2.290, val loss 2.423 | Time 0.156\n",
            "Epoch 950 | train loss 2.301, val loss 2.434 | Time 0.154\n",
            "Epoch 960 | train loss 2.298, val loss 2.426 | Time 0.145\n",
            "Epoch 970 | train loss 2.278, val loss 2.414 | Time 0.146\n",
            "Epoch 980 | train loss 2.291, val loss 2.445 | Time 0.157\n",
            "Epoch 990 | train loss 2.307, val loss 2.440 | Time 0.176\n",
            "Epoch 1000 | train loss 2.292, val loss 2.426 | Time 0.159\n",
            "Epoch 1010 | train loss 2.292, val loss 2.414 | Time 0.169\n",
            "Epoch 1020 | train loss 2.275, val loss 2.414 | Time 0.179\n",
            "Epoch 1030 | train loss 2.265, val loss 2.421 | Time 0.167\n",
            "Epoch 1040 | train loss 2.271, val loss 2.417 | Time 0.155\n",
            "Epoch 1050 | train loss 2.262, val loss 2.415 | Time 0.153\n",
            "Epoch 1060 | train loss 2.276, val loss 2.416 | Time 0.154\n",
            "Epoch 1070 | train loss 2.276, val loss 2.421 | Time 0.149\n",
            "Epoch 1080 | train loss 2.255, val loss 2.409 | Time 0.159\n",
            "Epoch 1090 | train loss 2.268, val loss 2.401 | Time 0.155\n",
            "Epoch 1100 | train loss 2.274, val loss 2.412 | Time 0.156\n",
            "Epoch 1110 | train loss 2.268, val loss 2.423 | Time 0.157\n",
            "Epoch 1120 | train loss 2.266, val loss 2.419 | Time 0.158\n",
            "Epoch 1130 | train loss 2.270, val loss 2.412 | Time 0.156\n",
            "Epoch 1140 | train loss 2.269, val loss 2.414 | Time 0.155\n",
            "Epoch 1150 | train loss 2.266, val loss 2.429 | Time 0.155\n",
            "Epoch 1160 | train loss 2.265, val loss 2.416 | Time 0.159\n",
            "Epoch 1170 | train loss 2.252, val loss 2.405 | Time 0.154\n",
            "Epoch 1180 | train loss 2.234, val loss 2.392 | Time 0.157\n",
            "Epoch 1190 | train loss 2.238, val loss 2.391 | Time 0.156\n",
            "Epoch 1200 | train loss 2.265, val loss 2.393 | Time 0.156\n",
            "Epoch 1210 | train loss 2.252, val loss 2.410 | Time 0.154\n",
            "Epoch 1220 | train loss 2.252, val loss 2.400 | Time 0.155\n",
            "Epoch 1230 | train loss 2.246, val loss 2.389 | Time 0.156\n",
            "Epoch 1240 | train loss 2.234, val loss 2.379 | Time 0.154\n",
            "Epoch 1250 | train loss 2.246, val loss 2.406 | Time 0.157\n",
            "Epoch 1260 | train loss 2.234, val loss 2.378 | Time 0.155\n",
            "Epoch 1270 | train loss 2.250, val loss 2.417 | Time 0.164\n",
            "Epoch 1280 | train loss 2.235, val loss 2.387 | Time 0.157\n",
            "Epoch 1290 | train loss 2.255, val loss 2.409 | Time 0.155\n",
            "Epoch 1300 | train loss 2.232, val loss 2.394 | Time 0.155\n",
            "Epoch 1310 | train loss 2.235, val loss 2.380 | Time 0.156\n",
            "Epoch 1320 | train loss 2.236, val loss 2.380 | Time 0.158\n",
            "Epoch 1330 | train loss 2.223, val loss 2.399 | Time 0.147\n",
            "Epoch 1340 | train loss 2.230, val loss 2.385 | Time 0.158\n",
            "Epoch 1350 | train loss 2.216, val loss 2.360 | Time 0.165\n",
            "Epoch 1360 | train loss 2.219, val loss 2.390 | Time 0.156\n",
            "Epoch 1370 | train loss 2.218, val loss 2.364 | Time 0.156\n",
            "Epoch 1380 | train loss 2.233, val loss 2.360 | Time 0.157\n",
            "Epoch 1390 | train loss 2.239, val loss 2.384 | Time 0.153\n",
            "Epoch 1400 | train loss 2.225, val loss 2.372 | Time 0.156\n",
            "Epoch 1410 | train loss 2.221, val loss 2.376 | Time 0.158\n",
            "Epoch 1420 | train loss 2.219, val loss 2.366 | Time 0.155\n",
            "Epoch 1430 | train loss 2.240, val loss 2.363 | Time 0.156\n",
            "Epoch 1440 | train loss 2.230, val loss 2.367 | Time 0.154\n",
            "Epoch 1450 | train loss 2.224, val loss 2.378 | Time 0.158\n",
            "Epoch 1460 | train loss 2.213, val loss 2.373 | Time 0.166\n",
            "Epoch 1470 | train loss 2.220, val loss 2.370 | Time 0.155\n",
            "Epoch 1480 | train loss 2.215, val loss 2.348 | Time 0.154\n",
            "Epoch 1490 | train loss 2.224, val loss 2.343 | Time 0.155\n",
            "Epoch 1500 | train loss 2.209, val loss 2.361 | Time 0.154\n",
            "Epoch 1510 | train loss 2.222, val loss 2.359 | Time 0.163\n",
            "Epoch 1520 | train loss 2.217, val loss 2.370 | Time 0.159\n",
            "Epoch 1530 | train loss 2.215, val loss 2.374 | Time 0.158\n",
            "Epoch 1540 | train loss 2.207, val loss 2.356 | Time 0.155\n",
            "Epoch 1550 | train loss 2.211, val loss 2.366 | Time 0.156\n",
            "Epoch 1560 | train loss 2.212, val loss 2.355 | Time 0.158\n",
            "Epoch 1570 | train loss 2.199, val loss 2.377 | Time 0.156\n",
            "Epoch 1580 | train loss 2.216, val loss 2.373 | Time 0.145\n",
            "Epoch 1590 | train loss 2.203, val loss 2.354 | Time 0.155\n",
            "Epoch 1600 | train loss 2.196, val loss 2.341 | Time 0.154\n",
            "Epoch 1610 | train loss 2.195, val loss 2.370 | Time 0.167\n",
            "Epoch 1620 | train loss 2.219, val loss 2.364 | Time 0.156\n",
            "Epoch 1630 | train loss 2.198, val loss 2.344 | Time 0.157\n",
            "Epoch 1640 | train loss 2.200, val loss 2.357 | Time 0.158\n",
            "Epoch 1650 | train loss 2.196, val loss 2.344 | Time 0.157\n",
            "Epoch 1660 | train loss 2.199, val loss 2.343 | Time 0.156\n",
            "Epoch 1670 | train loss 2.199, val loss 2.347 | Time 0.155\n",
            "Epoch 1680 | train loss 2.206, val loss 2.357 | Time 0.154\n",
            "Epoch 1690 | train loss 2.193, val loss 2.350 | Time 0.155\n",
            "Epoch 1700 | train loss 2.187, val loss 2.361 | Time 0.151\n",
            "Epoch 1710 | train loss 2.186, val loss 2.364 | Time 0.156\n",
            "Epoch 1720 | train loss 2.175, val loss 2.357 | Time 0.154\n",
            "Epoch 1730 | train loss 2.192, val loss 2.335 | Time 0.157\n",
            "Epoch 1740 | train loss 2.197, val loss 2.342 | Time 0.155\n",
            "Epoch 1750 | train loss 2.186, val loss 2.355 | Time 0.152\n",
            "Epoch 1760 | train loss 2.203, val loss 2.340 | Time 0.158\n",
            "Epoch 1770 | train loss 2.179, val loss 2.319 | Time 0.155\n",
            "Epoch 1780 | train loss 2.185, val loss 2.345 | Time 0.166\n",
            "Epoch 1790 | train loss 2.187, val loss 2.348 | Time 0.154\n",
            "Epoch 1800 | train loss 2.193, val loss 2.349 | Time 0.156\n",
            "Epoch 1810 | train loss 2.169, val loss 2.326 | Time 0.155\n",
            "Epoch 1820 | train loss 2.189, val loss 2.336 | Time 0.152\n",
            "Epoch 1830 | train loss 2.182, val loss 2.319 | Time 0.155\n",
            "Epoch 1840 | train loss 2.179, val loss 2.325 | Time 0.156\n",
            "Epoch 1850 | train loss 2.179, val loss 2.340 | Time 0.156\n",
            "Epoch 1860 | train loss 2.183, val loss 2.341 | Time 0.157\n",
            "Epoch 1870 | train loss 2.182, val loss 2.336 | Time 0.157\n",
            "Epoch 1880 | train loss 2.175, val loss 2.331 | Time 0.155\n",
            "Epoch 1890 | train loss 2.185, val loss 2.334 | Time 0.156\n",
            "Epoch 1900 | train loss 2.183, val loss 2.330 | Time 0.155\n",
            "Epoch 1910 | train loss 2.172, val loss 2.334 | Time 0.154\n",
            "Epoch 1920 | train loss 2.172, val loss 2.344 | Time 0.186\n",
            "Epoch 1930 | train loss 2.172, val loss 2.334 | Time 0.150\n",
            "Epoch 1940 | train loss 2.164, val loss 2.336 | Time 0.156\n",
            "Epoch 1950 | train loss 2.175, val loss 2.319 | Time 0.162\n",
            "Epoch 1960 | train loss 2.176, val loss 2.331 | Time 0.160\n",
            "Epoch 1970 | train loss 2.171, val loss 2.327 | Time 0.155\n",
            "Epoch 1980 | train loss 2.170, val loss 2.315 | Time 0.155\n",
            "Epoch 1990 | train loss 2.171, val loss 2.321 | Time 0.154\n",
            "Epoch 2000 | train loss 2.174, val loss 2.324 | Time 0.159\n",
            "Epoch 2010 | train loss 2.181, val loss 2.315 | Time 0.160\n",
            "Epoch 2020 | train loss 2.150, val loss 2.324 | Time 0.159\n",
            "Epoch 2030 | train loss 2.173, val loss 2.319 | Time 0.155\n",
            "Epoch 2040 | train loss 2.170, val loss 2.309 | Time 0.156\n",
            "Epoch 2050 | train loss 2.158, val loss 2.330 | Time 0.156\n",
            "Epoch 2060 | train loss 2.162, val loss 2.321 | Time 0.156\n",
            "Epoch 2070 | train loss 2.163, val loss 2.321 | Time 0.166\n",
            "Epoch 2080 | train loss 2.161, val loss 2.319 | Time 0.157\n",
            "Epoch 2090 | train loss 2.171, val loss 2.308 | Time 0.159\n",
            "Epoch 2100 | train loss 2.161, val loss 2.326 | Time 0.156\n",
            "Epoch 2110 | train loss 2.156, val loss 2.318 | Time 0.156\n",
            "Epoch 2120 | train loss 2.175, val loss 2.329 | Time 0.159\n",
            "Epoch 2130 | train loss 2.155, val loss 2.307 | Time 0.157\n",
            "Epoch 2140 | train loss 2.173, val loss 2.328 | Time 0.156\n",
            "Epoch 2150 | train loss 2.156, val loss 2.303 | Time 0.155\n",
            "Epoch 2160 | train loss 2.155, val loss 2.322 | Time 0.157\n",
            "Epoch 2170 | train loss 2.168, val loss 2.307 | Time 0.154\n",
            "Epoch 2180 | train loss 2.155, val loss 2.307 | Time 0.157\n",
            "Epoch 2190 | train loss 2.162, val loss 2.322 | Time 0.158\n",
            "Epoch 2200 | train loss 2.160, val loss 2.316 | Time 0.153\n",
            "Epoch 2210 | train loss 2.159, val loss 2.317 | Time 0.158\n",
            "Epoch 2220 | train loss 2.149, val loss 2.308 | Time 0.159\n",
            "Epoch 2230 | train loss 2.154, val loss 2.315 | Time 0.158\n",
            "Epoch 2240 | train loss 2.151, val loss 2.308 | Time 0.155\n",
            "Epoch 2250 | train loss 2.161, val loss 2.298 | Time 0.152\n",
            "Epoch 2260 | train loss 2.150, val loss 2.304 | Time 0.163\n",
            "Epoch 2270 | train loss 2.151, val loss 2.290 | Time 0.154\n",
            "Epoch 2280 | train loss 2.147, val loss 2.306 | Time 0.156\n",
            "Epoch 2290 | train loss 2.146, val loss 2.288 | Time 0.160\n",
            "Epoch 2300 | train loss 2.150, val loss 2.297 | Time 0.156\n",
            "Epoch 2310 | train loss 2.152, val loss 2.311 | Time 0.158\n",
            "Epoch 2320 | train loss 2.143, val loss 2.313 | Time 0.155\n",
            "Epoch 2330 | train loss 2.160, val loss 2.332 | Time 0.155\n",
            "Epoch 2340 | train loss 2.158, val loss 2.316 | Time 0.157\n",
            "Epoch 2350 | train loss 2.138, val loss 2.292 | Time 0.163\n",
            "Epoch 2360 | train loss 2.129, val loss 2.292 | Time 0.156\n",
            "Epoch 2370 | train loss 2.141, val loss 2.309 | Time 0.156\n",
            "Epoch 2380 | train loss 2.133, val loss 2.277 | Time 0.157\n",
            "Epoch 2390 | train loss 2.139, val loss 2.300 | Time 0.155\n",
            "Epoch 2400 | train loss 2.141, val loss 2.299 | Time 0.154\n",
            "Epoch 2410 | train loss 2.140, val loss 2.299 | Time 0.147\n",
            "Epoch 2420 | train loss 2.127, val loss 2.298 | Time 0.155\n",
            "Epoch 2430 | train loss 2.145, val loss 2.312 | Time 0.163\n",
            "Epoch 2440 | train loss 2.143, val loss 2.295 | Time 0.158\n",
            "Epoch 2450 | train loss 2.147, val loss 2.300 | Time 0.158\n",
            "Epoch 2460 | train loss 2.142, val loss 2.305 | Time 0.139\n",
            "Epoch 2470 | train loss 2.140, val loss 2.281 | Time 0.138\n",
            "Epoch 2480 | train loss 2.146, val loss 2.300 | Time 0.139\n",
            "Epoch 2490 | train loss 2.123, val loss 2.287 | Time 0.137\n",
            "Validation loss:  2.2870468974113463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses= pd.DataFrame(losses)\n",
        "plt.plot(losses['train'], label='Train Loss')\n",
        "plt.plot(losses['val'], label='Validation Loss')\n",
        "plt.title('Losses')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "ISUFsx0w5-Lw",
        "outputId": "df932087-80d2-4c25-c717-7489924a20d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** In this case, the x-axis stands for steps, not epochs.\n",
        "\n",
        "The validation loss at the final step is 2.28 which is considered okay given the amount of training data we're using and the number of steps. To reduce the losses significantly, we will have to increase the size of the training data, higher number of steps and higher GPU or processing power."
      ],
      "metadata": {
        "id": "ciR2RbbpLLvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inferencing our LLaMA 3 Model\n",
        "\n",
        "Now that we've completed our training. Let's head into our final step: Inference and see how well the model generates the output texts given new input prompts."
      ],
      "metadata": {
        "id": "hOnUIM2oRG1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_top_p(probs, p):\n",
        "    \"\"\"\n",
        "    Perform top-p (nucleus) samplings on a probability distribution.\n",
        "    - probs (torch.Tensor): Probability distribution tensor derived from the logits.\n",
        "    - p: Probability threshold for top-p sampling.\n",
        "    According to the paper, Top-p sampling selects the smallest set of tokens whose cumulative\n",
        "    probability mass exceeds the threshold p. The distribution is renormalized based on the\n",
        "    selected tokens.\n",
        "    \"\"\"\n",
        "    probs_sort, prob_idx= torch.sort(probs, dim=-1, descending=True)\n",
        "    probs_sum= torch.cumsum(probs_sort, dim=-1)\n",
        "    mask= probs_sum - probs_sort > p\n",
        "    probs_sort[mask]= 0.0\n",
        "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
        "    next_token= torch.multinomial(probs_sort, num_samples=1)\n",
        "    next_token= torch.gather(prob_idx, -1, next_token)\n",
        "\n",
        "    return next_token # sampled token indices from the vocabular\n",
        "\n",
        "\n",
        "def generate(model, prompts, tk:Tokenizer, max_seq_len:int, max_gen_len:int=500,\n",
        "             temperature:float=0.6, top_p:float=0.9):\n",
        "    \"\"\"\n",
        "    This function generates text sequences based on provided prompts using the LLaMA 3 model\n",
        "    we've built and trained.\n",
        "    - prompts: List of user input texts or prompts.\n",
        "    - max_gen_len: Maximum length of the generated text sequence.\n",
        "    - temperature: Temperature value for controlling randomness in sampling. Default 0.6.\n",
        "    - top_p: Top-p probability threshold for sampling prob output from the logits. Default 0.9.\n",
        "    \"\"\"\n",
        "    bsz= 1 # for inferencing, in general user just input one prompt which we'll take it as 1-batch\n",
        "    prompt_tokens= tk.token_bos.tolist() + tk.encode(prompts)\n",
        "    assert len(prompt_tokens) <= max_seq_len, 'Prompt token length should be small than max_seq_len'\n",
        "    total_len= min(len(prompt_tokens) + max_gen_len, max_seq_len)\n",
        "\n",
        "    # this tokens matrix is to store the input prompts and all the output that is generated by the\n",
        "    # model. later we'll use the tokenizer's decode function to decode this tokens to view results\n",
        "    # in text format\n",
        "    tokens= torch.full((bsz, total_len), fill_value=tk.token_pad.item(),\n",
        "                       dtype=torch.long, device=next(model.parameters()).device)\n",
        "    # fill in the prompt tokens into the token matrix\n",
        "    tokens[:,:len(prompt_tokens)]= torch.tensor(\n",
        "        prompt_tokens, dtype=torch.long, device=next(model.parameters()).device\n",
        "    )\n",
        "    #create a prompt_mask_token for later use to identify if the token is a prompt token or a\n",
        "    # padding token\n",
        "    # True if it is a prompt token, False if it is a padding token\n",
        "    input_text_mask= tokens != tk.token_pad.item()\n",
        "\n",
        "    # now we can start inferencing using one token at a time from the prompt_tokens list starting\n",
        "    # with the first position\n",
        "    prev_pos= 0\n",
        "    for cur_pos in range(1, total_len):\n",
        "        with torch.no_grad():\n",
        "            model.train(False)\n",
        "            logits, _= model(x=tokens[:,prev_pos:cur_pos], start_pos=prev_pos)\n",
        "        if temperature> 0:\n",
        "            probs= torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
        "            next_token= sample_top_p(probs, top_p)\n",
        "        else:\n",
        "            next_token= torch.argmax(logits[:, -1], dim=-1)\n",
        "\n",
        "        next_token= next_token.reshape(-1)\n",
        "\n",
        "        # only replace the token if it's a padding token\n",
        "        next_token= torch.where(input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n",
        "        tokens[:, cur_pos]= next_token\n",
        "\n",
        "        prev_pos= cur_pos\n",
        "        if tokens[:,cur_pos]== tk.token_pad.item() and next_token== tk.token_eos.item():\n",
        "            break\n",
        "\n",
        "    output_tokens, output_texts= [], []\n",
        "\n",
        "    for i, toks in enumerate(tokens.tolist()):\n",
        "        # eos_idx= toks.index(tk.token_eos.item())\n",
        "        if tk.token_eos.item() in toks:\n",
        "            eos_idx= toks.index(tk.token_eos.item())\n",
        "            toks= toks[:eos_idx]\n",
        "\n",
        "        output_tokens.append(toks)\n",
        "        output_texts.append(tk.decode(toks))\n",
        "\n",
        "    return output_tokens, output_texts\n"
      ],
      "metadata": {
        "id": "SL2sRyHc5-Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- perform the inferencing on user input prompts --\n",
        "prompts= \"Consider you what services he has done\"\n",
        "output_tokens, output_texts= generate(model, prompts, tk, max_seq_len)\n",
        "output_texts= output_texts[0].replace(\"<|begin_of_text|>\", \"\")\n",
        "print(output_texts)"
      ],
      "metadata": {
        "id": "1KbIuI4ondt3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c114c22f-188d-4b20-c7d8-45ec0665e28f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider you what services he has done\n",
            "eee hmrt u rtlnsal nw i a adiut\n",
            "i o o uoet o o oter o aa i uee eee.\n",
            "Pate o oftn eee h rsa ntsl?I o ad al od\n",
            "hti o rbdts hml,bls hmrl o uoeo ade\n",
            "h hssebe o ryan  o i atydl hv u rslns\n",
            " i a al  o hae ors o eee o o aete\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see that our LLaMA 3 model is able to perform inference and generate texts on new prompts, though the output does not seem great given the amount of training data and steps we've used for training. I am sure with much larger training data, we'll achieve much better accuracy."
      ],
      "metadata": {
        "id": "hwQ_h59RRZ3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/meta-llama/llama3/tree/main\n",
        "# https://pub.towardsai.net/build-your-own-llama-3-architecture-from-scratch-using-pytorch-2ce1ecaa901c\n",
        "\n",
        "# https://levelup.gitconnected.com/building-llama-3-from-scratch-with-python-e0cf4dbbc306#25b6\n",
        "\n",
        "# How to Prune LLaMA 3.2\n",
        "# https://towardsdatascience.com/how-to-prune-llama-3-2-and-similar-large-language-models-cf18e9a2afb6"
      ],
      "metadata": {
        "id": "uMc7WObWRc1u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}