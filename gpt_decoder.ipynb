{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a GPT Decoder"
      ],
      "metadata": {
        "id": "fyboSPz9k-Xq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRgxtdceky2j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "A5yKCwCmsfiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we always start with a dataset to train. let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# read it in to inspect\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text= f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfHHXUM3lNxy",
        "outputId": "bec694d5-06f9-406b-840b-4b6f71419262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-29 11:41:31--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-09-29 11:41:31 (23.3 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Length of dataset in characters: ', len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHwAqQ8tlOG2",
        "outputId": "cc70babf-c0df-4cdb-c582-a7ad051de995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeA-tGDKlOKH",
        "outputId": "9ad02feb-0741-46f4-bff4-97027b69c3d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars= sorted(list(set(text)))\n",
        "vocab_size= len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnWURyeflOQf",
        "outputId": "4e2321d0-801e-44c5-bab2-26a240f8b93e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "\n",
        "Converting the raw text as a string to some sequences of integers according to some vocabulary of possible elements.\n",
        "\n",
        "Tokenizer types:\n",
        "- caracter level - small vocabularies, long sequences of integers\n",
        "- sub-word level - very large vocabularies, short sequences of integers"
      ],
      "metadata": {
        "id": "mBUwagAXMh1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an example of a mapping from characters to integers - caracter level encoding\n",
        "stoi= {ch:i for i, ch in enumerate(chars)}\n",
        "itos= {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode= lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode= lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode('hii there'))\n",
        "print(decode(encode('hii there')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbTIxi1qlOUK",
        "outputId": "ca896d77-6fba-4053-94f1-1f27a2e6094e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "no_eltMbTABy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of encoding using a sub-word leval large vocabulary\n",
        "import tiktoken\n",
        "enc= tiktoken.get_encoding('gpt2')\n",
        "print(enc.n_vocab)\n",
        "\n",
        "print(enc.encode('hii there'))\n",
        "print(enc.decode(enc.encode('hii there')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmyUViwFlOXY",
        "outputId": "15637871-cc90-494d-aa06-78f3ae534b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50257\n",
            "[71, 4178, 612]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a tensor\n",
        "data= torch.tensor(encode(text))\n",
        "print(data.shape, data.dtype)\n",
        "# the 1000 characters we looked at earlier will to the GPT look like this\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TjdnVtYlOai",
        "outputId": "42c68e4e-9fd5-4502-8dc5-24e46b50a35a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now split up the data into train and validation sets\n",
        "n = int(0.9 * len(data)) # first 90% will be train, rest val\n",
        "train_data= data[:n]\n",
        "val_data= data[n:]"
      ],
      "metadata": {
        "id": "gcqDR0u0lOda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating batches. We never feed the entire text into the model at once. It would be computationally very expensive. We work on chunks of the dataset and we train the model basically sampling little random chunks out of the training set and train on just chunks at a time. These chunks have some kind of length and some kind of maximum length we are going to call block_size.\n",
        "\n",
        "The targets of train data are offset by one and that is because. We train this way to make the transformer network be used to seeing contexts all the way from as little as one all the way to block size. Transformer will predict up to block_size characters and have to start truncating because it will never receice more than block_size inputs when predicting the next charactere."
      ],
      "metadata": {
        "id": "_HbujZ0uXYC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size= 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dndCRyxlOgV",
        "outputId": "3913f2bc-e5bf-4b7e-9939-df97f84a8f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x= train_data[:block_size]\n",
        "y= train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "    context= x[:t+1]\n",
        "    target= y[t]\n",
        "    print(f'When input is {context} the target: {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Qm_DEXDlOix",
        "outputId": "bc9c8d72-185a-4a37-f3c8-51f2a58cd4e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input is tensor([18]) the target: 47\n",
            "When input is tensor([18, 47]) the target: 56\n",
            "When input is tensor([18, 47, 56]) the target: 57\n",
            "When input is tensor([18, 47, 56, 57]) the target: 58\n",
            "When input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "When input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We feed transformer with many batches of multiple chunks of text that are all like stacked up in a single tensor for efficiency and keep GPUs busy because they are very good at parallel processing of data. The chunks are processed independently, they don't talk to each other."
      ],
      "metadata": {
        "id": "VXTdRi9P01zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size= 4 # how many independent sequenes will we process in parallel?\n",
        "block_size= 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and target y\n",
        "    data= train_data if split== 'train' else val_data\n",
        "    ix= torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x= torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y= torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y= x.to(device), y.to(device)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "xb, yb= get_batch('train')\n",
        "print('Inputs')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('Target')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "print('------')\n",
        "\n",
        "for b in range(batch_size):     # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context= xb[b, :t+1]\n",
        "        target= yb[b, t]\n",
        "        print(f'When input is {context.tolist()} the target is: {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e31StP4glOlX",
        "outputId": "53067886-7263-4e3b-f24d-271e801d783f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]], device='cuda:0')\n",
            "Target\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]], device='cuda:0')\n",
            "------\n",
            "When input is [24] the target is: 43\n",
            "When input is [24, 43] the target is: 58\n",
            "When input is [24, 43, 58] the target is: 5\n",
            "When input is [24, 43, 58, 5] the target is: 57\n",
            "When input is [24, 43, 58, 5, 57] the target is: 1\n",
            "When input is [24, 43, 58, 5, 57, 1] the target is: 46\n",
            "When input is [24, 43, 58, 5, 57, 1, 46] the target is: 43\n",
            "When input is [24, 43, 58, 5, 57, 1, 46, 43] the target is: 39\n",
            "When input is [44] the target is: 53\n",
            "When input is [44, 53] the target is: 56\n",
            "When input is [44, 53, 56] the target is: 1\n",
            "When input is [44, 53, 56, 1] the target is: 58\n",
            "When input is [44, 53, 56, 1, 58] the target is: 46\n",
            "When input is [44, 53, 56, 1, 58, 46] the target is: 39\n",
            "When input is [44, 53, 56, 1, 58, 46, 39] the target is: 58\n",
            "When input is [44, 53, 56, 1, 58, 46, 39, 58] the target is: 1\n",
            "When input is [52] the target is: 58\n",
            "When input is [52, 58] the target is: 1\n",
            "When input is [52, 58, 1] the target is: 58\n",
            "When input is [52, 58, 1, 58] the target is: 46\n",
            "When input is [52, 58, 1, 58, 46] the target is: 39\n",
            "When input is [52, 58, 1, 58, 46, 39] the target is: 58\n",
            "When input is [52, 58, 1, 58, 46, 39, 58] the target is: 1\n",
            "When input is [52, 58, 1, 58, 46, 39, 58, 1] the target is: 46\n",
            "When input is [25] the target is: 17\n",
            "When input is [25, 17] the target is: 27\n",
            "When input is [25, 17, 27] the target is: 10\n",
            "When input is [25, 17, 27, 10] the target is: 0\n",
            "When input is [25, 17, 27, 10, 0] the target is: 21\n",
            "When input is [25, 17, 27, 10, 0, 21] the target is: 1\n",
            "When input is [25, 17, 27, 10, 0, 21, 1] the target is: 54\n",
            "When input is [25, 17, 27, 10, 0, 21, 1, 54] the target is: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# our input to the transformer\n",
        "xb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTNr2Ro7lOym",
        "outputId": "9006efea-6593-4acd-a0e0-efc3e2ad8e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
              "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
              "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
              "        [25, 17, 27, 10,  0, 21,  1, 54]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bulding Transformer\n",
        "\n",
        "We implemented here a decoder-only transformer. So, there is no an encoder component and there is no cross attention block (the in between piece connecting decoder and encoder). The reason is because we are generating text accordint to a dataset and it is unconditioned on anything.\n",
        "\n",
        "A decoder block uses the triangular mask to mask out the attention and it can be used for language modeling.\n",
        "\n",
        "The transformer paper (Attention is All You Need) presents a encoder-decoder transformer used for text translation. Unlike our model an encoder-decoder transformer wants to condition the generation on some additional information, the original sentence that should be translated. The encoder reads the sentence to translate and creates tokens from it without triangular mask and so, all the tokens are allowed to talk to each other as they much as they want. Once encodded the output is connected to a cross attention block. The queries are still generated from x, but now the keys and values are coming from the encoder."
      ],
      "metadata": {
        "id": "_IzyXfi_6hsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "# ----- hyperparameters setting -----\n",
        "batch_size= 64 # how many independent sequenes will we process in parallel?\n",
        "block_size= 256 # what is the maximum context length for predictions?\n",
        "max_iters= 5000\n",
        "eval_interval= 500\n",
        "learning_rate= 3e-4\n",
        "eval_iters= 200\n",
        "n_embed= 384\n",
        "n_heads= 6\n",
        "n_layers= 6\n",
        "dropout= 0.1"
      ],
      "metadata": {
        "id": "PIzWSnDPVjlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\"\n",
        "    One head of Scaled Self-Attention.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, head_size) -> None:\n",
        "        super(Head, self).__init__()\n",
        "        self.key  = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query= nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value= nn.Linear(n_embed, head_size, bias=False)\n",
        "        # tril is not a parameter of the module; so, we store it in a PyTorch register_buffer\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C= x.shape\n",
        "        k= self.key(x)   # (B, T, C) -- what do I contain?\n",
        "        q= self.query(x) # (B, T, C) -- what am I looking for?\n",
        "        v= self.value(x) # (B, T, C) -- private information to this token, what will communicate\n",
        "        # conpute attention scores (\"affinities\") -- if the key and query are sort of aligned\n",
        "        # they will interact to a very high amount and then we will get to learn more about that\n",
        "        # specific token as opposet ot any other token in the sequence\n",
        "        wei= q @ k.transpose(-2,-1) * C**(-0.5) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei= wei.masked_fill(self.tril[:T, :T]== 0, float('-inf')) # (B, T, T)\n",
        "        wei= F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei= self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        attn= wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "\n",
        "        return attn\n"
      ],
      "metadata": {
        "id": "opYi7LSeC3js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries (i.e., from x). In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module). So, cross-attention is used when there is a separate source of node we'd like to pull information from into our nodes, and in self-attention we just have nodes that would like to look at each other and talk to each other.\n",
        "- \"Scaled\" attention additional divides `wei` by $\\sqrt{head\\_size}$, which is the same to multiply by $head\\_size^{-\\frac{1}{2}}$. This makes it so when input Q, K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much."
      ],
      "metadata": {
        "id": "78aoA5a1LtRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multiple heads of self-attention in parallel.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_heads, head_size) -> None:\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.heads= nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # we run all heads in parallel into a list and simply concatenate all of the\n",
        "        # outputs. we are concatenating over the channel dimension\n",
        "        out= torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out= self.dropout(self.proj(out))\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "Evk_wImXJjuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It helps to have multiple communication channels because, obviously, the tokens have a lot to talk about. They want to find the consonants, the vowels, they want to find the vowels just from certain positions, they want to find any kinds of different things. So, multiple parallel attention heads help to create multiple independent channels of communications, gather lots of different types of data, and then decode the output.\n",
        "\n",
        "The self-attention is the communication and then once they have gathered all the data, now they needd to think on that data individually. That is what feef forward is doing (computation) and that is why we added it just after attention heads.\n",
        "\n",
        "Dropout is something that we can add right before the connection back into the residual pathway. Using dropout is kind of training an ensemble of subnetworks and then, at test time, everything is fully enabled and all the sub networks are merged into a single ensemble."
      ],
      "metadata": {
        "id": "r4x-YV62VIzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple linear layer followed by a non-linearity.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed) -> None:\n",
        "        super(FeedForward, self).__init__()\n",
        "        d_ff= 4 * n_embed\n",
        "        self.ff_net= nn.Sequential(\n",
        "            nn.Linear(n_embed, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, n_embed),\n",
        "            nn.Dropout(p=dropout)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.ff_net(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "-cVy5PIxNxQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Decoder Block\n",
        "\n",
        "Communication and computation block. The communication is done in multi-headed self-attention and then the computation is done using a feed forward network on all the tokens independently."
      ],
      "metadata": {
        "id": "s-_GeJnWGHWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer decoder block: communication followed by computation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_heads) -> None:\n",
        "        # n_embed: embedding dimension, n_heads: the number of heads we'd like\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        head_size= n_embed// n_heads\n",
        "        self.sa_heads= MultiHeadAttention(n_heads, head_size)\n",
        "        self.ln1= nn.LayerNorm(n_embed)\n",
        "        self.ffwd= FeedForward(n_embed)\n",
        "        self.ln2= nn.LayerNorm(n_embed)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # residual connections that dramatically help with the optimization\n",
        "        # we fork off and do some communication and come back\n",
        "        x= x + self.sa_heads(self.ln1(x))\n",
        "        # we fork off and do some computation and come back\n",
        "        x= x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "fS1toxOnRlDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT Decoder Model\n",
        "\n",
        "https://www.youtube.com/watch?v=zduSFxRajkE see 01:43:27"
      ],
      "metadata": {
        "id": "b8e6T3-jGUZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Super simple bigram model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super(BigramLanguageModel, self).__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table= nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table= nn.Embedding(block_size, n_embed)\n",
        "        self.d_blocks= nn.Sequential(*[\n",
        "            DecoderBlock(n_embed, n_heads) for _ in range(n_layers)\n",
        "        ])\n",
        "        self.ln_f= nn.LayerNorm(n_embed) # final layer norm\n",
        "        self.lm_head= nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T= idx.shape\n",
        "        # idx and target are both (B, T) tensor of integers\n",
        "        tok_emb= self.token_embedding_table(idx) # (B, T, C)\n",
        "        pos_emb= self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "        x= tok_emb + pos_emb # (B, T, C)\n",
        "        x= self.d_blocks(x) # apply n_layers blocks of self-attention (B, T, C)\n",
        "        x= self.ln_f(x) # (B, T, C)\n",
        "        logits= self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss= None\n",
        "        else:\n",
        "            B, T, C= logits.shape\n",
        "            logits= logits.view(B*T, C)\n",
        "            targets= targets.view(B*T)\n",
        "            loss= F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond= idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss= self(idx_cond)\n",
        "            # focus only on thee last time step\n",
        "            logits= logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs= F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next= torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx= torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "dvkJghVddWQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m= BigramLanguageModel()\n",
        "model= m.to(device)\n",
        "logits, loss= model(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "context= torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqXZqcs1JT8l",
        "outputId": "70a40c4d-ae36-4309-daca-94170338d3af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.3802, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Z&LUdQ:bIxuKyY3GGltHrgFSFMXfguc-aBcgteDOKVoSPdLDiScOxQkVIE?WBM?r,nQA;IPD bQSfWLqBnQZ-w!xZMQ:\n",
            "$ZSv!MiDPMIDKoRok&rFpp,RKndLtiQscSKrgFc'LvTs'guooNT.Lx'RcQ!v tDVulf-Qsymd?qJj&ZrwKoQGvPVOJ Si;cpj!tVriK'AdlJg:XpqSbpkuswAfS,AJ&biflODzRfuS\n",
            "ik??ucaWU;HDrMs!BsAOenjKDBF$uBvnperULxMon!cVnDFNE&IPTQ&KnHSKixFvPn$VsCJkCrGH,XSpj eXRWgrNcxk $OASQKnSf:B'sdD!hnH?qpPTnXnrn:H!H!ZRRLDB.nH!VTZdLUJ!qF$DrHecqjr,K;iTFvFk3SkmnzezqJVsKxVTDf.k3:HTpLKODOghtD.Dj,EKgU3TTBEQIIYoF?tnqBNDenqBrHp?XCfm!HTmXv,ACjgzd$S3I3yLxTwDpFvnHx-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTdzVg_PIwb_",
        "outputId": "b22408ae-c848-4aa8-8b9b-19bf252dd84d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 10788929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Training and Text Generation"
      ],
      "metadata": {
        "id": "QC3Stu2omDPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out= {}\n",
        "    model.eval()\n",
        "\n",
        "    for split in ['train', 'val']:\n",
        "        losses= torch.zeros(eval_iters)\n",
        "\n",
        "        for k in range(eval_iters):\n",
        "            X, Y= get_batch(split)\n",
        "            logits, loss= model(X, Y)\n",
        "            losses[k]= loss.item()\n",
        "\n",
        "        out[split]= losses.mean()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "DphJriRfuml5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def self_supervised_train(model, learning_rate=1e-3, max_iters=500, eval_interval=50):\n",
        "\n",
        "    # create a PyTorch optimizer\n",
        "    optimizer= torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for iter in range(max_iters):\n",
        "\n",
        "        # every once in a while evaluate the loss on train and val sets\n",
        "        if iter % eval_interval== 0:\n",
        "            losses= estimate_loss(model)\n",
        "            print(f\"Steps {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # sample a batch of data\n",
        "        xb, yb= get_batch('train')\n",
        "\n",
        "        # evaluate the loss\n",
        "        logits, loss= model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "losses= self_supervised_train(model, learning_rate, max_iters, eval_interval)\n",
        "\n",
        "# generate from the model\n",
        "context= torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p17JAfK8mhGQ",
        "outputId": "38f59c00-ba35-4e8d-b278-5d6cb5312848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps 0: train loss 4.2849, val loss 4.2823\n",
            "Steps 500: train loss 1.9696, val loss 2.0683\n",
            "Steps 1000: train loss 1.5614, val loss 1.7466\n",
            "Steps 1500: train loss 1.3975, val loss 1.6065\n",
            "Steps 2000: train loss 1.3029, val loss 1.5459\n",
            "Steps 2500: train loss 1.2343, val loss 1.5188\n",
            "Steps 3000: train loss 1.1791, val loss 1.5043\n",
            "Steps 3500: train loss 1.1255, val loss 1.4962\n",
            "Steps 4000: train loss 1.0793, val loss 1.5091\n",
            "Steps 4500: train loss 1.0253, val loss 1.5137\n",
            "\n",
            "And, prepare into reply a famous flesh\n",
            "With worth your house to his pleasure?\n",
            "\n",
            "THOMAS OF SAUMERS:\n",
            "Could I tear, my lord, then are put t us both.\n",
            "\n",
            "BUCKINGHAM:\n",
            "He was not upon his truth, now boy, the day.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "God forbear! whise glad I stood the prompt, my persecute,\n",
            "Will be clostemal proopous by war.\n",
            "I'll to fall out our headful tend body blood,\n",
            "Which make me winnch and way still make doop:\n",
            "The fond more honour lives grief death,\n",
            "Is ere to deputy his day night's face.\n",
            "\n",
            "FRIAR LAURENCE:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jJh7z3Tc2VE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We did not implemented any of the fine-tune stages that tipically go on top of a pre-trained model. If we are interested in something that is not just language modeling but actually want to perform tasks or be aligned in a specific way, or we want to detect sentiment or anything like that, basically, anytime we don't want something that is just a document completer, we have to implement further stages of fine-tuning, which can be supervised fine-tuning or something more fancy such as a reward model."
      ],
      "metadata": {
        "id": "NJGG4eTpvYBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "bPm_-2f0y70j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "torch.manual_seed(1337)\n",
        "B, T, C= 4,8,2 # batch, time, channels\n",
        "x= torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-yiLA07dWbw",
        "outputId": "8aee5da7-8353-45d5-ced1-a6c06f6cbe4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 1: We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow= torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev= x[b,:t+1] # (t,C)\n",
        "        xbow[b,t]= torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "w4nvfpu8dWgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a= torch.tril(torch.ones(3, 3))\n",
        "a= a / torch.sum(a, 1, keepdim=True)\n",
        "b= torch.randint(0,10,(3,2)).float()\n",
        "c= a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAfRyZLndWYL",
        "outputId": "8b6fc8f0-e378-4144-94ba-5505ebfbf3a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 1: using matrix multiply for a weighted aggregation\n",
        "wei= torch.tril(torch.ones(T, T))\n",
        "wei= wei / wei.sum(1, keepdim=True)\n",
        "xbow2= wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCsx4vlidWkF",
        "outputId": "3662e1e0-f942-4b0f-9faf-ed12addb1cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril= torch.tril(torch.ones(T, T))\n",
        "wei= torch.zeros((T,T))\n",
        "wei= wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei= F.softmax(wei, dim=-1)\n",
        "xbow3= wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAvxD0c9dWoW",
        "outputId": "a52e26f4-7590-421c-cb5a-da2a07ab866d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C= 4,8,32 # batch, time, channels\n",
        "x= torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size= 16\n",
        "key= nn.Linear(C, head_size, bias=False)\n",
        "query= nn.Linear(C, head_size, bias=False)\n",
        "value= nn.Linear(C, head_size, bias=False)\n",
        "k= key(x)   # (B, T, 16)\n",
        "q= query(x) # (B, T, 16)\n",
        "wei=  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril= torch.tril(torch.ones(T, T))\n",
        "wei= wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei= F.softmax(wei, dim=-1)\n",
        "\n",
        "v= value(x)\n",
        "out= wei @ v\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvTuKvqxdWrH",
        "outputId": "9358ae41-fa54-4012-9a7c-4ad9b9bfcbc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EObo7LV6dWtv",
        "outputId": "0e90213c-6669-4ce1-bbca-4fea63e79453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k= torch.randn(B,T,head_size)\n",
        "q= torch.randn(B,T,head_size)\n",
        "wei= q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "_kOHkB_mdWxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ipCiIW10udc",
        "outputId": "468075ea-7421-471c-9305-28de3bfeb605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K_LrDZZ0upz",
        "outputId": "8f1d4321-feb3-4253-95b9-3a3e3dd8b609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJmhLYWplO2G",
        "outputId": "e5eb9da1-134a-4dd1-ad53-ee24dc5c58de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5729s"
      ],
      "metadata": {
        "id": "HNGL9ey5lO8V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}