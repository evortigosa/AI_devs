{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a GPT Decoder"
      ],
      "metadata": {
        "id": "fyboSPz9k-Xq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SRgxtdceky2j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "A5yKCwCmsfiM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we always start with a dataset to train. let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# read it in to inspect\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text= f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfHHXUM3lNxy",
        "outputId": "50bab7b1-530c-48f0-a518-b4f6e7fac713"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-29 03:23:09--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-09-29 03:23:09 (33.4 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Length of dataset in characters: ', len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHwAqQ8tlOG2",
        "outputId": "ddc95b60-1d1a-440c-e122-cf94ffa573d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeA-tGDKlOKH",
        "outputId": "a7821602-d714-4527-afee-8db9d9ace776"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars= sorted(list(set(text)))\n",
        "vocab_size= len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnWURyeflOQf",
        "outputId": "e61ddfef-e1eb-4750-e0c5-00235225ac88"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "\n",
        "Converting the raw text as a string to some sequences of integers according to some vocabulary of possible elements.\n",
        "\n",
        "Tokenizer types:\n",
        "- caracter level - small vocabularies, long sequences of integers\n",
        "- sub-word level - very large vocabularies, short sequences of integers"
      ],
      "metadata": {
        "id": "mBUwagAXMh1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an example of a mapping from characters to integers - caracter level encoding\n",
        "stoi= {ch:i for i, ch in enumerate(chars)}\n",
        "itos= {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode= lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode= lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode('hii there'))\n",
        "print(decode(encode('hii there')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbTIxi1qlOUK",
        "outputId": "5780c87f-5fac-4a67-8e98-118db2a9c329"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "no_eltMbTABy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of encoding using a sub-word leval large vocabulary\n",
        "import tiktoken\n",
        "enc= tiktoken.get_encoding('gpt2')\n",
        "print(enc.n_vocab)\n",
        "\n",
        "print(enc.encode('hii there'))\n",
        "print(enc.decode(enc.encode('hii there')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmyUViwFlOXY",
        "outputId": "1a73d853-7c85-4caf-fe32-5fed5f0aeb75"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50257\n",
            "[71, 4178, 612]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a tensor\n",
        "data= torch.tensor(encode(text))\n",
        "print(data.shape, data.dtype)\n",
        "# the 1000 characters we looked at earlier will to the GPT look like this\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TjdnVtYlOai",
        "outputId": "6e017023-45b4-4114-8767-ebf9f21c9870"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now split up the data into train and validation sets\n",
        "n = int(0.9 * len(data)) # first 90% will be train, rest val\n",
        "train_data= data[:n]\n",
        "val_data= data[n:]"
      ],
      "metadata": {
        "id": "gcqDR0u0lOda"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating batches. We never feed the entire text into the model at once. It would be computationally very expensive. We work on chunks of the dataset and we train the model basically sampling little random chunks out of the training set and train on just chunks at a time."
      ],
      "metadata": {
        "id": "_HbujZ0uXYC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size= 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dndCRyxlOgV",
        "outputId": "0e2d9108-ae7f-4af9-ece2-960c717f52f0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x= train_data[:block_size]\n",
        "y= train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "    context= x[:t+1]\n",
        "    target= y[t]\n",
        "    print(f'When input is {context} the target: {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Qm_DEXDlOix",
        "outputId": "d2dd3d4e-1dc8-4f14-ff90-b1fa8178f829"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input is tensor([18]) the target: 47\n",
            "When input is tensor([18, 47]) the target: 56\n",
            "When input is tensor([18, 47, 56]) the target: 57\n",
            "When input is tensor([18, 47, 56, 57]) the target: 58\n",
            "When input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "When input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size= 4 # how many independent sequenes will we process in parallel?\n",
        "block_size= 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and target y\n",
        "    data= train_data if split== 'train' else val_data\n",
        "    ix= torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x= torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y= torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y= x.to(device), y.to(device)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "xb, yb= get_batch('train')\n",
        "print('Inputs')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('Target')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('------')\n",
        "\n",
        "for b in range(batch_size):     # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context= xb[b, :t+1]\n",
        "        target= yb[b, t]\n",
        "        print(f'When input is {context.tolist()} the target is: {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e31StP4glOlX",
        "outputId": "6819a6bf-15cc-4c2d-9eb5-d9edc11962d1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "Target\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "------\n",
            "When input is [24] the target is: 43\n",
            "When input is [24, 43] the target is: 58\n",
            "When input is [24, 43, 58] the target is: 5\n",
            "When input is [24, 43, 58, 5] the target is: 57\n",
            "When input is [24, 43, 58, 5, 57] the target is: 1\n",
            "When input is [24, 43, 58, 5, 57, 1] the target is: 46\n",
            "When input is [24, 43, 58, 5, 57, 1, 46] the target is: 43\n",
            "When input is [24, 43, 58, 5, 57, 1, 46, 43] the target is: 39\n",
            "When input is [44] the target is: 53\n",
            "When input is [44, 53] the target is: 56\n",
            "When input is [44, 53, 56] the target is: 1\n",
            "When input is [44, 53, 56, 1] the target is: 58\n",
            "When input is [44, 53, 56, 1, 58] the target is: 46\n",
            "When input is [44, 53, 56, 1, 58, 46] the target is: 39\n",
            "When input is [44, 53, 56, 1, 58, 46, 39] the target is: 58\n",
            "When input is [44, 53, 56, 1, 58, 46, 39, 58] the target is: 1\n",
            "When input is [52] the target is: 58\n",
            "When input is [52, 58] the target is: 1\n",
            "When input is [52, 58, 1] the target is: 58\n",
            "When input is [52, 58, 1, 58] the target is: 46\n",
            "When input is [52, 58, 1, 58, 46] the target is: 39\n",
            "When input is [52, 58, 1, 58, 46, 39] the target is: 58\n",
            "When input is [52, 58, 1, 58, 46, 39, 58] the target is: 1\n",
            "When input is [52, 58, 1, 58, 46, 39, 58, 1] the target is: 46\n",
            "When input is [25] the target is: 17\n",
            "When input is [25, 17] the target is: 27\n",
            "When input is [25, 17, 27] the target is: 10\n",
            "When input is [25, 17, 27, 10] the target is: 0\n",
            "When input is [25, 17, 27, 10, 0] the target is: 21\n",
            "When input is [25, 17, 27, 10, 0, 21] the target is: 1\n",
            "When input is [25, 17, 27, 10, 0, 21, 1] the target is: 54\n",
            "When input is [25, 17, 27, 10, 0, 21, 1, 54] the target is: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# our input to the transformer\n",
        "xb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTNr2Ro7lOym",
        "outputId": "8acaaf0e-da9c-4103-a864-8904f09a17f6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
              "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
              "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
              "        [25, 17, 27, 10,  0, 21,  1, 54]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bulding Transformer\n",
        "\n",
        "We implemented here a decoder-only transformer. So, there is no an encoder component and there is no cross attention block (the in between piece connecting decoder and encoder). The reason is because we are generating text accordint to a dataset and it is unconditioned on anything.\n",
        "\n",
        "A decoder block uses the triangular mask to mask out the attention and it can be used for language modeling.\n",
        "\n",
        "The transformer paper (Attention is All You Need) presents a encoder-decoder transformer used for text translation. Unlike our model an encoder-decoder transformer wants to condition the generation on some additional information, the original sentence that should be translated. The encoder reads the sentence to translate and creates tokens from it without triangular mask and so, all the tokens are allowed to talk to each other as they much as they want. Once encodded the output is connected to a cross attention block. The queries are still generated from x, but now the keys and values are coming from the encoder."
      ],
      "metadata": {
        "id": "_IzyXfi_6hsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "# hyperparameters setting\n",
        "batch_size= 64 # how many independent sequenes will we process in parallel?\n",
        "block_size= 256 # what is the maximum context length for predictions?\n",
        "max_iters= 5000\n",
        "eval_interval= 500\n",
        "learning_rate= 3e-4\n",
        "eval_iters= 200\n",
        "n_embed= 384\n",
        "n_heads= 6\n",
        "n_layers= 6\n",
        "dropout= 0.1\n",
        "# --------------------"
      ],
      "metadata": {
        "id": "PIzWSnDPVjlf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\"\n",
        "    One head of self-attention.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "\n",
        "        super(Head, self).__init__()\n",
        "        self.key= nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query= nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value= nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C= x.shape\n",
        "        k= self.key(x)   # (B, T, C)\n",
        "        q= self.query(x) # (B, T, C)\n",
        "        # conpute attention scores (\"affinities\")\n",
        "        wei= q @ k.transpose(-2,-1) * C**(-0.5) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei= wei.masked_fill(self.tril[:T, :T]== 0, float('-inf')) # (B, T, T)\n",
        "        wei= F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei= self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v= self.value(x) # (B, T, C)\n",
        "        out= wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "opYi7LSeC3js"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multiple heads of self-attention in parallel.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_heads, head_size):\n",
        "\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.heads= nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
        "        self.proj= nn.Linear(n_embed, n_embed)\n",
        "        self.dropout= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out= torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out= self.dropout(self.proj(out))\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "Evk_wImXJjuv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple linear layer followed by a non-linearity.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed):\n",
        "        super(FeedForward, self).__init__()\n",
        "        d_ff= 4 * n_embed\n",
        "        self.ff_net= nn.Sequential(\n",
        "            nn.Linear(n_embed, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, n_embed),\n",
        "            nn.Dropout(p=dropout)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.ff_net(x)\n"
      ],
      "metadata": {
        "id": "-cVy5PIxNxQZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Decoder Block"
      ],
      "metadata": {
        "id": "s-_GeJnWGHWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer decoder block: communication followed by computation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_heads):\n",
        "        # n_embed: embedding dimension, n_heads: the number of heads we'd like\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        head_size= n_embed// n_heads\n",
        "        self.sa_heads= MultiHeadAttention(n_heads, head_size)\n",
        "        self.ln1= nn.LayerNorm(n_embed)\n",
        "        self.ffwd= FeedForward(n_embed)\n",
        "        self.ln2= nn.LayerNorm(n_embed)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= x + self.sa_heads(self.ln1(x))\n",
        "        x= x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "fS1toxOnRlDN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT Decoder Model"
      ],
      "metadata": {
        "id": "b8e6T3-jGUZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Super simple bigram model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(BigramLanguageModel, self).__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table= nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table= nn.Embedding(block_size, n_embed)\n",
        "        self.d_blocks= nn.Sequential(*[DecoderBlock(n_embed, n_heads) for _ in range(n_layers)])\n",
        "        self.ln_f= nn.LayerNorm(n_embed) # final layer norm\n",
        "        self.lm_head= nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T= idx.shape\n",
        "\n",
        "        # idx and target are both (B, T) tensor of integers\n",
        "        tok_emb= self.token_embedding_table(idx) # (B, T, C)\n",
        "        pos_emb= self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "        x= tok_emb + pos_emb # (B, T, C)\n",
        "        x= self.d_blocks(x) # apply n_layers blocks of self-attention (B, T, C)\n",
        "        x= self.ln_f(x) # (B, T, C)\n",
        "        logits= self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss= None\n",
        "        else:\n",
        "            B, T, C= logits.shape\n",
        "            logits= logits.view(B*T, C)\n",
        "            targets= targets.view(B*T)\n",
        "            loss= F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond= idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss= self(idx_cond)\n",
        "            # focus only on thee last time step\n",
        "            logits= logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs= F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next= torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx= torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "dvkJghVddWQ9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m= BigramLanguageModel()\n",
        "model= m.to(device)\n",
        "logits, loss= model(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "context= torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqXZqcs1JT8l",
        "outputId": "0eab2288-8e92-4ee4-f206-d18e4efaa7d9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.2069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "!Bp,hmS-SqpU$wPu\n",
            "ZwxOFi-c'v!3pgGVvBDsdVIPfPXUz$IeR$UU&qdAN\n",
            "-:$gnxRGvmhILdAIaHc$$-L'\n",
            "aqXkFhWKcslWWi'O otJpZCOjLiLPgJmJmP\n",
            "g$uC-JgIUH3?p,dSqimi&-rmqRUTxuAfV-bFn:zGxtuvWidy;.fP.Nvj,lZ sKWGioywJTq,pVGsFZCgOi''dk:UuK$$PyyJ-IkfUll,zr-UJ BqfjY\n",
            "kS&DIywiIbJIZjMxb;yrB$jgM'P.G$r-e3uieV?OzEe?EeRd:sAVyd$NyI;vuNbej$B--3uJllLqidD!vAG!jCxoyTSl!IozSKQOzgMUyLy$Z -?J:RK3,.raB$mSKR$qs.NDTQM33Um!E,\n",
            "u;.uwtEQ?xv$GY'TOQQruV,ZmM?ANyjQaBtf$qeSbeRM$Xy.kdnpcuBFzvh.u!$&Z$,JjZ$$evvDQ;kuGpRRSKoRdMMrQ$T!XLd$uNIR$N-xAGXFN!q?vbut\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTdzVg_PIwb_",
        "outputId": "6294171a-b63d-46f3-849c-312f74adb5e3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 10788929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out= {}\n",
        "    model.eval()\n",
        "\n",
        "    for split in ['train', 'val']:\n",
        "        losses= torch.zeros(eval_iters)\n",
        "\n",
        "        for k in range(eval_iters):\n",
        "            X, Y= get_batch(split)\n",
        "            logits, loss= model(X, Y)\n",
        "            losses[k]= loss.item()\n",
        "\n",
        "        out[split]= losses.mean()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "DphJriRfuml5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def self_supervised_train(model, learning_rate=1e-3, max_iters=500, eval_interval=50):\n",
        "\n",
        "    # create a PyTorch optimizer\n",
        "    optimizer= torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for iter in range(max_iters):\n",
        "\n",
        "        # every once in a while evaluate the loss on train and val sets\n",
        "        if iter % eval_interval== 0:\n",
        "            losses= estimate_loss(model)\n",
        "            print(f\"Steps {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        # sample a batch of data\n",
        "        xb, yb= get_batch('train')\n",
        "\n",
        "        # evaluate the loss\n",
        "        logits, loss= model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "losses= self_supervised_train(model, learning_rate, max_iters, eval_interval)\n",
        "\n",
        "# generate from the model\n",
        "context= torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu6MAHN8dWU8",
        "outputId": "4c678258-7e04-495b-a21e-c73433679273"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps 0: train loss 4.3417, val loss 4.3358\n",
            "Steps 500: train loss 1.9957, val loss 2.0825\n",
            "Steps 1000: train loss 1.5663, val loss 1.7371\n",
            "Steps 1500: train loss 1.4018, val loss 1.6106\n",
            "Steps 2000: train loss 1.3069, val loss 1.5447\n",
            "Steps 2500: train loss 1.2349, val loss 1.5091\n",
            "Steps 3000: train loss 1.1766, val loss 1.4946\n",
            "Steps 3500: train loss 1.1230, val loss 1.4995\n",
            "Steps 4000: train loss 1.0709, val loss 1.5015\n",
            "Steps 4500: train loss 1.0187, val loss 1.5214\n",
            "\n",
            "Where's sleeps in blood too exeech with thee,\n",
            "Will to the foretable office to love\n",
            "The reasons with seem as heir hence;\n",
            "And tears the state, eaching surprised,\n",
            "Lest the tremble liss to the fault!\n",
            "Mone's state this fairest I should ravel brrow:\n",
            "Ah, I advantise and reture it, I'll have before\n",
            "Once to take some good word; I'll wrong\n",
            "Things above a strange to best flower for a dued\n",
            "To what's roaring.\n",
            "\n",
            "LEONTES:\n",
            "'Hangst! 'tis, this, mispray! The friends unfall'ne\n",
            "About up; the thore which not seem'st \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jJh7z3Tc2VE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "bPm_-2f0y70j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much."
      ],
      "metadata": {
        "id": "tvJr-ppB09nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "torch.manual_seed(1337)\n",
        "B, T, C= 4,8,2 # batch, time, channels\n",
        "x= torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-yiLA07dWbw",
        "outputId": "1e5f4474-bef4-479b-c632-e9537c8ac1ef"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 1: We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow= torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev= x[b,:t+1] # (t,C)\n",
        "        xbow[b,t]= torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "w4nvfpu8dWgA"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a= torch.tril(torch.ones(3, 3))\n",
        "a= a / torch.sum(a, 1, keepdim=True)\n",
        "b= torch.randint(0,10,(3,2)).float()\n",
        "c= a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAfRyZLndWYL",
        "outputId": "41fad440-f211-4eee-d571-fdb138fba827"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 1: using matrix multiply for a weighted aggregation\n",
        "wei= torch.tril(torch.ones(T, T))\n",
        "wei= wei / wei.sum(1, keepdim=True)\n",
        "xbow2= wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCsx4vlidWkF",
        "outputId": "dc33cef2-db15-4534-8042-d3fab05ffe66"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril= torch.tril(torch.ones(T, T))\n",
        "wei= torch.zeros((T,T))\n",
        "wei= wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei= F.softmax(wei, dim=-1)\n",
        "xbow3= wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAvxD0c9dWoW",
        "outputId": "0120845a-c013-452e-81f4-078b855a9b06"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C= 4,8,32 # batch, time, channels\n",
        "x= torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size= 16\n",
        "key= nn.Linear(C, head_size, bias=False)\n",
        "query= nn.Linear(C, head_size, bias=False)\n",
        "value= nn.Linear(C, head_size, bias=False)\n",
        "k= key(x)   # (B, T, 16)\n",
        "q= query(x) # (B, T, 16)\n",
        "wei=  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril= torch.tril(torch.ones(T, T))\n",
        "wei= wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei= F.softmax(wei, dim=-1)\n",
        "\n",
        "v= value(x)\n",
        "out= wei @ v\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvTuKvqxdWrH",
        "outputId": "925adfa9-96c9-4449-c749-14a3810a42e9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EObo7LV6dWtv",
        "outputId": "86f9c6ee-2315-472f-931a-6221c95bd37f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k= torch.randn(B,T,head_size)\n",
        "q= torch.randn(B,T,head_size)\n",
        "wei= q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "_kOHkB_mdWxW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ipCiIW10udc",
        "outputId": "e96a326f-6099-41ec-b560-b907c3dc421b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K_LrDZZ0upz",
        "outputId": "6bd1cc0c-aa6e-434e-dd9b-51ad13f6e954"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJmhLYWplO2G",
        "outputId": "b17492e9-5031-4908-f439-0bb8d640bc13"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xZwqIdmi01_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5729s"
      ],
      "metadata": {
        "id": "HNGL9ey5lO8V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}