{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attention is All You Need - The Annotated Version"
      ],
      "metadata": {
        "id": "tYhn0xehF1NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://nlp.seas.harvard.edu/annotated-transformer/\n",
        "# https://medium.com/we-talk-data/in-depth-guide-on-pytorchs-nn-transformer-901ad061a195\n",
        "# https://github.com/gordicaleksa/pytorch-original-transformer/tree/main\n",
        "# https://www.youtube.com/watch?v=n9sLZPLOxG8\n",
        "# https://www.youtube.com/watch?v=px4rtkWHFvM"
      ],
      "metadata": {
        "id": "TA_Opz_KAsf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQVo2cigFUrY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "lUheKIEbGFjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Model Architecture (Section 3)\n",
        "\n",
        "Transformers -> GPT series config compatibility\n",
        "- src_vocab -> vocab_size ... total number of tokens for inputs\n",
        "- tgt_vocab -> vocab_size ... total number of tokens for outputs\n",
        "- d_model -> n_embed ........ embedding dimension\n",
        "- d_model-> block_size ....... max sequence length\n",
        "- n_head -> n_head .............. number of heads\n",
        "- n_encoder -> n_layer ........ number of layers\n",
        "- n_decoder -> n_layer ........ number of layers\n",
        "- d_ff -> 4 * n_embed .......... FFN inner-layer dimensionality\n",
        "- dropout -> none"
      ],
      "metadata": {
        "id": "CDel-M08GLmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention (Section 3.2)\n",
        "\n",
        "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
        "\n",
        "We call our particular attention \"Scaled Dot-Product Attention\". The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.\n",
        "\n",
        "The dot product can result in vectors with very positive and very negative numbers inside it, and the problem is that the softmax (applied few lines below) will converge to one-hot-like vectors, so we scale the Attention matrix with the square root of head size to \"normalize\" its variance and keep weights fairly diffuse before softmax.\n",
        "\n",
        "In practice, we compute the Attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as:                      \n",
        "                                                                 \n",
        "$$                                                                         \n",
        "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V               \n",
        "$$\n",
        "\n",
        "- Query -> what we are looking for\n",
        "- Key     -> our content\n",
        "- Value -> token information, what will communicate\n",
        "\n",
        "**Attention scores ($QK^T$, \"affinities\"):** If the query and key are aligned they will interact to each other with a very high amount and then we will learn more about that specific token as opposed to any token in the sequence."
      ],
      "metadata": {
        "id": "BvU1egyvH204"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Attention (Section 3.2.2)\n",
        "\n",
        "Multi-Head Attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single Attention head, averaging inhibits this.\n",
        "\n",
        "$$    \n",
        "\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O    \\\\                                           \n",
        "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)                                \n",
        "$$\n",
        "\n",
        "where the projections are parameter matrices $W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$.\n",
        "\n",
        "In this work we employ $h=8$ parallel Attention layers, or heads. For each of these we use $d_k=d_v=d_{\\text{model}}/h=64$. Due to the reduced dimension of each head, the total computational cost is similar to that of Single-Head Attention with full dimensionality.\n",
        "\n",
        "Dropout is applied after computing the Scaled Dot-Product Attention but before adding it to the residual connection."
      ],
      "metadata": {
        "id": "AJfpQXjfH8uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Defining the Multi-Headed Attention operation into a single class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head, dropout) -> None:\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert n_embed % n_head == 0, \"n_embed must be divisible by n_head\"\n",
        "        self.n_embed= n_embed\n",
        "        self.n_head = n_head\n",
        "        self.d_head = n_embed // n_head\n",
        "        # query, key, value projections\n",
        "        self.q_proj= nn.Linear(n_embed, n_embed)\n",
        "        self.k_proj= nn.Linear(n_embed, n_embed)\n",
        "        self.v_proj= nn.Linear(n_embed, n_embed)\n",
        "        # output projection\n",
        "        self.o_proj= nn.Linear(n_embed, n_embed)\n",
        "        # regularization\n",
        "        self.dropout= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        B, T, C= query.size() # batch_size, sequence length, embedding dim (d_model)\n",
        "        assert C == self.n_embed, \"Input embedding dimension must match model embedding dimension\"\n",
        "        # 1. calculate query, key, values for all heads and move heads forward to be the batch dim\n",
        "        # nh is 'number of heads', dh is 'head dim'\n",
        "        q= self.q_proj(query)\n",
        "        k= self.k_proj(key)   # q,k,v -> (B, T, C)\n",
        "        v= self.v_proj(value)\n",
        "        # 2. reshape for Multi-Head Attention -- q,k,v view -> (B, T, nh, dh)\n",
        "        q= q.view(B, -1, self.n_head, self.d_head).transpose(1, 2)\n",
        "        k= k.view(B, -1, self.n_head, self.d_head).transpose(1, 2) # q,k,v transp -> (B, nh, T, dh)\n",
        "        v= v.view(B, -1, self.n_head, self.d_head).transpose(1, 2)\n",
        "        # 3. this is the original implementation of Attention - the 'scaled dot product'\n",
        "        # Attention (materializes the large (T, T) matrix for all the queries and keys)\n",
        "        attn= (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.d_head))\n",
        "        # apply Attention mask (when the mask is not None)\n",
        "        if mask is not None:\n",
        "            mask= mask.unsqueeze(1)\n",
        "            attn= attn.masked_fill(mask== 0, float('-inf'))\n",
        "        # normalize Attention scores\n",
        "        attn= F.softmax(attn, dim=-1)\n",
        "        attn= self.dropout(attn)\n",
        "        # 4. compute Attention output\n",
        "        y= attn @ v # (B, nh, T, T) x (B, nh, T, dh) -> (B, nh, T, dh)\n",
        "        # 5. concatenate multi-head outputs\n",
        "        y= y.transpose(1, 2).contiguous().view(B, T, C) # re-assembly all head outputs side by side\n",
        "        # 6. output projection\n",
        "        y= self.o_proj(y)\n",
        "\n",
        "        return y, attn\n"
      ],
      "metadata": {
        "id": "jqr48WH7GFri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer uses Multi-Head Attention in three different ways:\n",
        "\n",
        "- The encoder contains Self-Attention layers. In a self-attention layer all of the keys, values and queries come from the same source, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
        "\n",
        "- Similarly, Self-Attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position (in an autoregressive manner). We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to $-\\infty$) all values in the input of the softmax which correspond to illegal connections (using a triangular causal mask).\n",
        "\n",
        "- In \"Encoder-Decoder Attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder (Cross-Attention). This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models.\n",
        "\n",
        "In summary:\n",
        "\n",
        "- Self-Attention: query, key, and value come from the same source (i.e., x from the previous module)\n",
        "\n",
        "- Cross-Attention: query, key, and value come from different sources (query comes from x of the previous module; key and value come from encoder's memory).\n",
        "\n",
        "- Multi-Head Attention is applying multiple Attentions in parallel and concatenating their results. If you're familiar with convolutions Multi-Head Attention is kind of like a group convolution, because instead of having one large convolution we do convolutions in groups."
      ],
      "metadata": {
        "id": "BaP2eKa1LzFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Position-wise Feed-Forward Network (Section 3.3)\n",
        "\n",
        "In addition to Attention modules, each of the layers (blocks) in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
        "\n",
        "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$\n",
        "\n",
        "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is $d_{\\text{model}}=512$, and the inner-layer has dimensionality $d_{ff}=2048$.\n",
        "\n",
        "It's position-wise because this feed forward net will be independently applied to every token's representation. This net will basically be applied independently to every token's representation (you can think of it as if there was a nested for-loop going over the batch size and max token sequence length dimensions and applied this network to token representations. PyTorch does this auto-magically behind the scenes.\n",
        "\n",
        "Dropout is applied after the activation function (ReLU) in the feedforward network and before adding the residual connection."
      ],
      "metadata": {
        "id": "MrwVDE-VL7n-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Defining the FeedForward Network.\n",
        "    It is a two linear projectons sandwiched in between the ReLU nonlinearity.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout) -> None:\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.ffn_in= nn.Linear(d_model, d_ff)\n",
        "        self.relu= nn.ReLU(inplace=True)\n",
        "        self.ffn_proj= nn.Linear(d_ff, d_model)\n",
        "        self.dropout= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.ffn_in(x)\n",
        "        x= self.relu(x)\n",
        "        x= self.dropout(x)\n",
        "        x= self.ffn_proj(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "9kvkhd3LGFuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Attention modules did the communication, allowing tokens to look at each other and find high affinities, but they didn't have time to consider what they found from the other tokens. In other words, Self-Attention is the communication, and once they've gathered all the data, they need to think about that data individually.\n",
        "- Attention do the communication\n",
        "- FeedForward do the computation"
      ],
      "metadata": {
        "id": "NTzSqJkP0DBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder and Decoder Stacks (Section 3.1)\n",
        "\n",
        "A standard Encoder-Decoder architecture. Here, the encoder maps an input sequence of symbol representations $(x_1, ..., x_n)$ to a sequence of continuous representations $z= (z_1, ..., z_n)$. Given $z$, the decoder then generates an output sequence $(y_1, ...,y_m)$ of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.\n",
        "\n",
        "Layer normalization is applied to stabilize the training process. It normalizes the input across the features, ensuring that the outputs have a stable distribution. Dropout is applied after the final output projection of the Multi-Head Attention and the feedforward network outputs."
      ],
      "metadata": {
        "id": "jfrhyGMMN5XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Construct a LayerNorm module (nn.LayerNorm)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, norm_shape, eps=1e-6) -> None:\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.weight= nn.Parameter(torch.ones(norm_shape))\n",
        "        self.bias= nn.Parameter(torch.zeros(norm_shape))\n",
        "        self.eps= eps\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # calculate mean and variance along the last axis\n",
        "        mean= x.mean(-1, keepdim=True)\n",
        "        stdv= x.std(-1, keepdim=True)\n",
        "        # normalize the inputs\n",
        "        norm= self.weight * (x - mean) / (stdv + self.eps) + self.bias\n",
        "\n",
        "        return norm\n"
      ],
      "metadata": {
        "id": "7D3kItkaGFx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder\n",
        "\n",
        "Each encoder block has two sub-layers. The first is a Multi-Head Self-Attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.\n",
        "\n",
        "The paper employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is $\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x))$, where $\\mathrm{Sublayer}(x)$ is the function implemented by the sub-layer itself. Dropout is applied the output of each sub-layer, before it is added to the sub-layer input and normalized.\n",
        "\n",
        "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{\\text{model}}=512$.\n",
        "\n",
        "**NOTE:** Very few details about the Transformer have changed in the last five years, but there is something slightly departs from the original paper. You see in Figure 1 that Add and Norm is applied after (Post-LN) the transformation (Multi-Head Attention). But now it is more common to apply LayerNorm before (Pre-LN) the transformation, so there is a reshuffling of the Layer Norm. This is called pre-norm formulation and that is the one we are going to implement as well."
      ],
      "metadata": {
        "id": "JcREg9hNOdD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    The Decoder block is made of Self-Attention and FeedForward network.\n",
        "    Pre-normalization: LayerNorms are before the application of Attention and FeedForward\n",
        "    in order to keep a clean residual stream all the way down to the input tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_head, d_ff, dropout) -> None:\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.ln_1= LayerNorm(d_model)\n",
        "        self.attn= MultiHeadedAttention(d_model, n_head, dropout)\n",
        "        self.dropout1= nn.Dropout(p=dropout)\n",
        "        self.ln_2= LayerNorm(d_model)\n",
        "        self.ffn= PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.dropout2= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        # pre-norm encoder block connections\n",
        "        x_norm = self.ln_1(x)\n",
        "        attn, _= self.attn(x_norm, x_norm, x_norm, mask=src_mask)\n",
        "        x= x + self.dropout1(attn)\n",
        "\n",
        "        x_norm= self.ln_2(x)\n",
        "        ffn= self.ffn(x_norm)\n",
        "        x= x + self.dropout2(ffn)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "sTOjbKsWGF2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According the paper, the encoder is composed of a stack of N=6 identical encoder blocks (or layers)."
      ],
      "metadata": {
        "id": "4AiITK3gRJfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Core encoder module is a stack of N encoder blocks (layers).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_encoder=6, d_model=512, n_head=8, d_ff=2048, dropout=0.1) -> None:\n",
        "        super(Encoder, self).__init__()\n",
        "        self.e_nx= nn.ModuleList([\n",
        "            EncoderBlock(d_model, n_head, d_ff, dropout) for _ in range(n_encoder)\n",
        "        ])\n",
        "        self.norm= LayerNorm(d_model)\n",
        "\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        # pass the input (and mask) through each encoder block\n",
        "        for block in self.e_nx:\n",
        "            x= block(x, src_mask)\n",
        "        # forward the final layerNorm to the decoder\n",
        "        x= self.norm(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "26VJ5jJTRD0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder\n",
        "\n",
        "The first Attention module of a decoder is a Masked Self-Attention. The purpose of a Masked Self-Attention in the Decoder is to ensure that, during training or inference, a token can only attend to itself and earlier tokens in the sequence. This maintains the autoregressive property required for generation tasks (a triangular causal mask is applied).\n",
        "\n",
        "In addition to the two sub-modules present in each encoder block, each decoder block inserts a third sub-module (placed between the masked Self-Attention and Feed Forward), which performs Multi-Head Attention over the Attention output (queries and keys scaled dot product) from the encoder stack (i.e., a Cross-Attention). Here, the outputs from the first Multi-Headed Attention module of the decoder serve as the value.\n",
        "\n",
        "There is no causal mask in Cross-Attention: The encoder output is fixed for the entire sequence, representing the input context. There's no need to enforce an autoregressive constraint because the encoder output is not dependent on the tokens generated by the decoder.\n",
        "\n",
        "In summary, the decoder's Cross-Attention module takes:\n",
        "- Query ($Q$) from the decoder output (the Self-Attention layer's output).\n",
        "- Key ($K$) and Value ($V$) from the encoder's output (encoder memory).\n",
        "\n",
        "Similar to the encoder, we employ layer normalization (Pre-LN) and residual connections around each of the sub-layers."
      ],
      "metadata": {
        "id": "Um3lJp8NQGv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    The Decoder block is made of masked Self-Attention, Cross-Attention, and FeedForward net.\n",
        "    Pre-normalization: LayerNorms are before the application of Attention and FeedForward\n",
        "    in order to keep a clean residual stream all the way down to the input tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_head, d_ff, dropout) -> None:\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.ln_1= nn.LayerNorm(d_model)\n",
        "        self.mask_attn= MultiHeadedAttention(d_model, n_head, dropout)\n",
        "        self.dropout1= nn.Dropout(p=dropout)\n",
        "        self.ln_2= nn.LayerNorm(d_model)\n",
        "        self.cross_attn= MultiHeadedAttention(d_model, n_head, dropout)\n",
        "        self.dropout2= nn.Dropout(p=dropout)\n",
        "        self.ln_3= nn.LayerNorm(d_model)\n",
        "        self.ffn= PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.dropout3= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, memory_enc, causal_mask, memory_mask=None):\n",
        "        # masked self-attention module of the decoder uses the causal triangular mask\n",
        "        x_norm = self.ln_1(x)\n",
        "        attn, _= self.mask_attn(x_norm, x_norm, x_norm, mask=causal_mask)\n",
        "        x= x + self.dropout1(attn)\n",
        "\n",
        "        x_norm = self.ln_2(x)\n",
        "        attn, _= self.cross_attn(x_norm, memory_enc, memory_enc, mask=memory_mask)\n",
        "        x= x + self.dropout2(attn)\n",
        "\n",
        "        x_norm= self.ln_3(x)\n",
        "        ffn= self.ffn(x_norm)\n",
        "        x= x + self.dropout3(ffn)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "eAp18U15GF4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder is also composed of a stack of N=6 identical blocks (layers).\n",
        "\n",
        "We also modify the Self-Attention module in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$."
      ],
      "metadata": {
        "id": "dUUHwd-JS5sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder module is a stack of N decoder blocks with masking.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_decoder=6, d_model=512, block_size=512, n_head=8, d_ff=2048,\n",
        "                 dropout=0.1) -> None:\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_nx= nn.ModuleList([\n",
        "            DecoderBlock(d_model, n_head, d_ff, dropout) for _ in range(n_decoder)\n",
        "        ])\n",
        "        self.norm= LayerNorm(d_model)\n",
        "        # register_buffer performs Masked Attention on the outputs, so that positions depend on\n",
        "        # the past only -- create a lower triangular matrix (2-D tensor)\n",
        "        self.register_buffer('causal_mask',\n",
        "            torch.tril(torch.ones(block_size, block_size)).view(1, block_size, block_size)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, memory_enc, memory_mask=None):\n",
        "        B, T, C= x.size() # batch_size, sequence length, embedding dim (d_model)\n",
        "        causal_mask= self.causal_mask[:,:T,:T]\n",
        "\n",
        "        # pass the input, memory from the encoder (and mask) through each decoder block\n",
        "        for block in self.d_nx:\n",
        "            x= block(x, memory_enc, causal_mask, memory_mask)\n",
        "        # forward the final layerNorm to the generator\n",
        "        x= self.norm(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "1ZDKc0G5GF6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings and Softmax (Section 3.4)\n",
        "\n",
        "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\\text{model}}$. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation. In the embedding layers, we multiply those weights by $\\sqrt{d_{\\text{model}}}$."
      ],
      "metadata": {
        "id": "5SByKwVWVOsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Convert the input/output tokens to vectors.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model) -> None:\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.d_model= d_model\n",
        "        # token embedding table\n",
        "        self.wte= nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (stated in the paper) multiply the embedding weights by the square root of d_model\n",
        "        x= self.wte(x) * math.sqrt(self.d_model)\n",
        "\n",
        "        return x  # (B, T) -> (B, T, C) where C is the model dimension d_model\n"
      ],
      "metadata": {
        "id": "UfIe3PKBGF-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Define standard linear + softmax generation step.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, vocab_size) -> None:\n",
        "        super(Generator, self).__init__()\n",
        "        self.projection_head= nn.Linear(d_model, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits= self.projection_head(x)\n",
        "        # using log_softmax as PyTorch's nn.KLDivLoss expects log probabilities\n",
        "        probs= F.log_softmax(logits, dim=-1)\n",
        "\n",
        "        return logits, probs\n"
      ],
      "metadata": {
        "id": "JxNI3aEZYDps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding (Section 3.5)\n",
        "\n",
        "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension $d_{\\text{model}}$ as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed.\n",
        "\n",
        "In this work, we use sine and cosine functions of different frequencies:\n",
        "\n",
        "$$\n",
        "    PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}}) \\\\                                                                                      \n",
        "    PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})                       \n",
        "$$\n",
        "\n",
        "where pos is the position and i is the dimension.\n",
        "\n",
        "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. Dropout is typically applied to positional encodings when added to embeddings."
      ],
      "metadata": {
        "id": "ze_xOW-jVV1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement the PE function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, block_size, d_model, dropout, const_val=10000.0) -> None:\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout= nn.Dropout(p=dropout)\n",
        "        # create a long tensor of block_size positions\n",
        "        position= torch.arange(0, block_size).unsqueeze(1)\n",
        "        frequencies= torch.exp(\n",
        "            torch.arange(0, d_model, 2) * -(math.log(const_val) / d_model)\n",
        "        )\n",
        "        # create an empty placeholder\n",
        "        wpe= torch.zeros(block_size, d_model)\n",
        "        # itterating over each element in the sequence using sin and cos\n",
        "        wpe[:, 0::2]= torch.sin(position * frequencies)\n",
        "        wpe[:, 1::2]= torch.cos(position * frequencies)\n",
        "        # register_buffer so it's saved in the model state_dict but not optimized\n",
        "        self.register_buffer('wpe', wpe.unsqueeze(0))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= x + self.wpe[:, : x.size(1)].requires_grad_(False).to(x.device)\n",
        "        # (stated in the paper) dropout to the sum of positional encodings and token embeddings\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "jriVjEU4VQnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building The Full Transformer Model\n",
        "\n",
        "The Nx (n_encoder=6 and n_decoder) here simply represents that this block is chain-repeated N times. So basically you are stacking the block back-to-back and passing the input from the previous block to the next one. This is a way to make the neural network deeper. Let's say N=5. Do we feed the output of each encoder layer to the corresponding decoder layer? No. Basically you run the encoder all the way through once and only once. Then you just take that representation and feed the same thing to every one of the 5 decoder layers."
      ],
      "metadata": {
        "id": "ZuDnhHxcVm0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Construct a Transformer model from hyperparameters.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, src_vocab, tgt_vocab, d_model=512, block_size=512, n_head=8,\n",
        "                 n_encoder=6, n_decoder=6, d_ff=2048, dropout=0.1) -> None:\n",
        "        super(Transformer, self).__init__()\n",
        "        self.d_model= d_model\n",
        "\n",
        "        c= copy.deepcopy\n",
        "        positional_enc= PositionalEncoding(block_size, d_model, dropout)\n",
        "\n",
        "        self.src_embed= nn.Sequential(Embeddings(src_vocab, d_model), c(positional_enc))\n",
        "        self.encoder= Encoder(n_encoder, d_model, n_head, d_ff, dropout)\n",
        "        self.tgt_embed= nn.Sequential(Embeddings(tgt_vocab, d_model), c(positional_enc))\n",
        "        self.decoder= Decoder(n_decoder, d_model, block_size, n_head, d_ff, dropout)\n",
        "        self.generator= Generator(d_model, tgt_vocab)\n",
        "\n",
        "        # initialize parameters with Glorot / fan_avg\n",
        "        # values are scaled by the gain parameter. no gradient is recorded for this operation\n",
        "        for p in self.parameters():\n",
        "            if p.dim()> 1:\n",
        "                nn.init.xavier_normal_(p)\n",
        "\n",
        "\n",
        "    # modularize into encode/decode functions for optimizing the decoding/translation process\n",
        "    def encode(self, src, src_padding_mask=None):\n",
        "        # embedding and positional encoding for source/inputs\n",
        "        src= self.src_embed(src)\n",
        "        # forward pass through the encoder\n",
        "        encoded= self.encoder(src, src_padding_mask)\n",
        "\n",
        "        return encoded\n",
        "\n",
        "\n",
        "    def decode(self, tgt, memory, memory_padding_mask=None):\n",
        "        # embedding and positional encoding for target/outputs\n",
        "        tgt= self.tgt_embed(tgt)\n",
        "        # forward pass through the decoder\n",
        "        decoded= self.decoder(tgt, memory, memory_padding_mask)\n",
        "\n",
        "        return decoded\n",
        "\n",
        "\n",
        "    def forward(self, src, tgt, src_padding_mask=None, tgt_padding_mask=None):\n",
        "        # take in and process masked src and target sequences.\n",
        "        encoded= self.encode(src, src_padding_mask)\n",
        "        decoded= self.decode(tgt, encoded, tgt_padding_mask)\n",
        "        # after this line we'll have a shape (B, T, V), where V is the target vocab size,\n",
        "        # generator does a simple linear projection followed by log_softmax\n",
        "        logits, log_probs= self.generator(decoded)\n",
        "        # reshape probs into (B*T, V) as that's suitable for passing it into KL Div loss\n",
        "        log_probs= log_probs.reshape(-1, log_probs.shape[-1])\n",
        "\n",
        "        return logits, log_probs\n",
        "\n",
        "\n",
        "    def generate(self, src, src_padding_mask, max_new_tokens):\n",
        "        self.eval()\n",
        "        # src is (B, T) tensor of indices in the current context\n",
        "        memory= self.encode(src, src_padding_mask)\n",
        "        tgt= torch.zeros(1, 1).type_as(src)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the decoder's predictions\n",
        "            out= self.decode(tgt, memory, src_padding_mask)\n",
        "            # focus only on thee last time step and apply softmax to get probabilities\n",
        "            logits, log_probs= self.generator(out[:, -1]) # becomes (B, C)\n",
        "            # sample from the distribution\n",
        "            _, next_word= torch.max(log_probs, dim=1) # (B, 1)\n",
        "            next_word= next_word.data[0]\n",
        "            # append sampled index to the running sequence\n",
        "            tgt= torch.cat( # (B, T+1)\n",
        "                [tgt, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
        "            )\n",
        "\n",
        "        return tgt\n"
      ],
      "metadata": {
        "id": "ivRtXug1VQrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- baseline model ---\n",
        "vocab_size= 11\n",
        "d_model= 512\n",
        "block_size= d_model\n",
        "n_head= 8\n",
        "n_encoder= 6\n",
        "n_decoder= n_encoder\n",
        "d_ff= 4 * d_model\n",
        "dropout= 0.1\n",
        "\n",
        "model= Transformer(vocab_size, vocab_size, d_model, block_size, n_head,\n",
        "                   n_encoder, n_decoder, d_ff, dropout).to(device)\n",
        "\n",
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qProy7gcHq6",
        "outputId": "9f6bb60e-a973-4565-97b8-e3b02dd07ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 44157451\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- large model ---\n",
        "vocab_size= 11\n",
        "d_model= 1024\n",
        "block_size= d_model\n",
        "n_head= 16\n",
        "n_encoder= 6\n",
        "n_decoder= n_encoder\n",
        "d_ff= 4 * d_model\n",
        "dropout= 0.3\n",
        "\n",
        "model= Transformer(vocab_size, vocab_size, d_model, block_size, n_head,\n",
        "                   n_encoder, n_decoder, d_ff, dropout).to(device)\n",
        "\n",
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HbANjo1cH0Y",
        "outputId": "de0e4633-7727-4dba-c4e4-6093f0cbd59b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 176395275\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- larger model ---\n",
        "vocab_size= 50257 # GPT-2/3 total number of tokens\n",
        "d_model= 768\n",
        "block_size= 1024\n",
        "n_head= 12\n",
        "n_encoder= 12\n",
        "n_decoder= n_encoder\n",
        "d_ff= 4 * d_model\n",
        "\n",
        "model= Transformer(vocab_size, vocab_size, d_model, block_size, n_head,\n",
        "                   n_encoder, n_decoder, d_ff).to(device)\n",
        "\n",
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}\\n')\n",
        "\n",
        "model"
      ],
      "metadata": {
        "id": "LpcwbMfQVQwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae94fee-c49a-4325-bae7-173c38c6be7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 314321233\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (src_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (wte): Embedding(50257, 768)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (encoder): Encoder(\n",
              "    (e_nx): ModuleList(\n",
              "      (0-11): 12 x EncoderBlock(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): MultiHeadedAttention(\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (ln_2): LayerNorm()\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (ffn_in): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (ffn_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm()\n",
              "  )\n",
              "  (tgt_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (wte): Embedding(50257, 768)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (d_nx): ModuleList(\n",
              "      (0-11): 12 x DecoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mask_attn): MultiHeadedAttention(\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (cross_attn): MultiHeadedAttention(\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (ln_3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (ffn_in): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (ffn_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm()\n",
              "  )\n",
              "  (generator): Generator(\n",
              "    (projection_head): Linear(in_features=768, out_features=50257, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference\n",
        "\n",
        "Here we make a forward step to generate a prediction of the model. We try to use our transformer to memorize the input. As you will see the output is randomly generated due to the fact that the model is not trained yet. In the next part of this tutorial we will build the training function and try to train our model to memorize the numbers from 1 to 10."
      ],
      "metadata": {
        "id": "TLzSP589a4Gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_test(model, device):\n",
        "    src= torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]).to(device)\n",
        "    src_mask= torch.ones(1, 1, 10).to(device)\n",
        "    max_new_tokens= src.shape[1] -1\n",
        "\n",
        "    ys= model.generate(src, src_mask, max_new_tokens)\n",
        "\n",
        "    print('Example Untrained Model Prediction:', ys)\n"
      ],
      "metadata": {
        "id": "33q5FjW3VQzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "    test_model= Transformer(11, 11, n_encoder=2, n_decoder=2).to(device)\n",
        "    inference_test(test_model, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfr1HgLxbgbI",
        "outputId": "0e83e9df-f184-4c07-f2ce-2ec1c7a8e3e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Untrained Model Prediction: tensor([[0, 7, 1, 5, 6, 5, 6, 5, 6, 5]])\n",
            "Example Untrained Model Prediction: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "Example Untrained Model Prediction: tensor([[0, 6, 0, 9, 4, 2, 4, 2, 4, 2]])\n",
            "Example Untrained Model Prediction: tensor([[0, 9, 5, 1, 9, 1, 9, 1, 5, 1]])\n",
            "Example Untrained Model Prediction: tensor([[ 0,  7, 10, 10, 10, 10, 10, 10, 10, 10]])\n",
            "Example Untrained Model Prediction: tensor([[0, 4, 1, 1, 1, 1, 1, 3, 5, 3]])\n",
            "Example Untrained Model Prediction: tensor([[0, 1, 1, 1, 1, 1, 1, 7, 4, 5]])\n",
            "Example Untrained Model Prediction: tensor([[0, 9, 0, 9, 0, 9, 0, 9, 0, 9]])\n",
            "Example Untrained Model Prediction: tensor([[0, 4, 4, 4, 4, 0, 4, 0, 3, 4]])\n",
            "Example Untrained Model Prediction: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Model Training (Section 5)\n",
        "\n",
        "This section describes the training regime for our models. First we define a batch object that holds the src and target sentences for training, as well as constructing the masks.\n",
        "\n",
        "See GPT_2.ipynb for datails on TF32, kernel fusion, Gradient accumulation, and so on."
      ],
      "metadata": {
        "id": "WdGkDZ76a_T3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batches and Masking (Section 5.1)"
      ],
      "metadata": {
        "id": "BdfTZB4UbAJj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EdF_ClbrbfIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer (Section 5.3)\n",
        "\n",
        "We used the Adam optimizer with $\\beta_1= 0.9$, $\\beta_2= 0.98$ and $\\epsilon=10^{-9}$. We varied the learning rate over the course of training, according to the formula:\n",
        "\n",
        "$$\n",
        "lrate= d_{\\text{model}}^{-0.5}\\cdot \\min(step\\_num^{-0.5}, step\\_num\\cdot warmup\\_steps^{-1.5})\n",
        "$$\n",
        "\n",
        "This corresponds to increasing the learning rate linearly for the first $warmup\\_steps$ training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used $warmup\\_steps = 4000$.\n",
        "\n",
        "**Note:** This part is very important. Need to train with this setup of the model."
      ],
      "metadata": {
        "id": "g1lkQzIubfda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WarmupLRScheduler:\n",
        "    \"\"\"\n",
        "    Linear ramp learning rate for the warm-up number of steps and then start decaying\n",
        "    according to the inverse square root law of the current training step number.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, d_model, factor, warmup_steps=4000) -> None:\n",
        "        self.optimizer= optimizer\n",
        "        self.d_model= d_model\n",
        "        self.factor= factor\n",
        "        self.warmup= warmup_steps\n",
        "        self.it= 0\n",
        "        self.step()\n",
        "\n",
        "\n",
        "    def get_last_lr(self):\n",
        "        return self.optimizer.param_groups[0]['lr']\n",
        "\n",
        "\n",
        "    def get_lr(self):\n",
        "        scale= self.d_model ** (-0.5)\n",
        "        step = max(self.it, 1)\n",
        "\n",
        "        return self.factor * (scale * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "\n",
        "\n",
        "    def step(self):\n",
        "        current_lr= self.get_lr()\n",
        "        self.it += 1\n",
        "\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr']= current_lr\n"
      ],
      "metadata": {
        "id": "Ckkyf_1pbopo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "\n",
        "class Configure_Optimizer:\n",
        "    \"\"\"\n",
        "    Splitting up the parameters tha should be weight decayed and those that should not.\n",
        "    Then, create an AdamW PyTorch optimizer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, weight_decay, learning_rate, device) -> None:\n",
        "        self.model= model\n",
        "        self.wdecay= weight_decay\n",
        "        self.lr= learning_rate\n",
        "        self.device= device\n",
        "\n",
        "\n",
        "    def __call__(self, betas, eps=1e-9):\n",
        "        # start with a lot of the candidate parameters (that require grad)\n",
        "        param_dict= {pn: p for pn, p in self.model.named_parameters()}\n",
        "        param_dict= {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any 2D parameters will be weight dacayed, otherwise no\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layerNorms don't\n",
        "        # but most of the parameters will be decayed\n",
        "        decay_params  = [p for n, p in param_dict.items() if p.dim()>= 2]\n",
        "        nodecay_params= [p for n, p in param_dict.items() if p.dim() < 2] # one-dim tensors\n",
        "        optim_groups= [\n",
        "            {'params':   decay_params, 'weight_decay': self.wdecay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params  = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params= sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"Num decayed parameter tensors: {len(decay_params)}, with {num_decay_params} parameters\")\n",
        "        print(f\"Num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params} parameters\")\n",
        "        # create AdamW optimizer and use the fused version of it is available\n",
        "        fused_available= 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        # fused is a lot faster when it is available and when running on cuda\n",
        "        use_fused= fused_available and self.device== \"cuda\"\n",
        "        print(f\"Using fused AdamW: {use_fused}\")\n",
        "        # create the AdamW PyTorch optimizer -- bug fix of Adam\n",
        "        optimizer= torch.optim.AdamW(\n",
        "            optim_groups, lr=self.lr, betas=betas, eps=eps, fused=use_fused\n",
        "        )\n",
        "\n",
        "        return optimizer\n"
      ],
      "metadata": {
        "id": "8W_DCzc1aH4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization (Section 5.4)\n",
        "\n",
        "During training, we employed label smoothing of value $\\epsilon_{ls}= 0.1$. Label smoothing is a regularization technique used to prevent overconfidence in model predictions. Instead of assigning a probability of 1.0 to the correct class and 0.0 to all others, label smoothing assigns a small amount of probability to incorrect classes. This can make the model more robust and improve generalization. In other words, label smoothing hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
        "\n",
        "Label smoothing is implemented using the KL Divergence loss. Instead of using a one-hot target distribution, KL Divergence create a distribution that has confidence of the correct word and the rest of the smoothing mass distributed throughout the vocabulary."
      ],
      "metadata": {
        "id": "-cOhqOB3bpCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement label smoothing. Instead of one-hot target distribution set the target word's\n",
        "    probability to \"confidence_value\" (usually 0.9) and distribute the rest of the \"smoothing_value\"\n",
        "    mass (usually 0.1) over the rest of the vocab.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, smoothing_value, pad_idx, vocab_size) -> None:\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        assert 0.0 <= smoothing_value <= 1.0, \"smoothing_value must be between 0.0 and 1.0\"\n",
        "        self.smoothing_value= smoothing_value\n",
        "        self.pad_idx= pad_idx\n",
        "        self.vocab_size= vocab_size\n",
        "        self.confidence_value= 1.0 - smoothing_value\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B= x.shape[0]\n",
        "        smooth_distributions= torch.zeros((B, self.vocab_size), device=x.device)\n",
        "        # -2 because we are not distributing the smoothing mass over the pad token index and over\n",
        "        # the ground truth index those 2 values will be overwritten by the following 2 lines with\n",
        "        # confidence_value and 0 (for pad token index)\n",
        "        smooth_distributions.fill_(self.smoothing_value / (self.vocab_size - 2))\n",
        "        smooth_distributions.scatter_(1, x, self.confidence_value)\n",
        "        smooth_distributions[:, self.pad_idx]= 0.\n",
        "        # if we had a pad token as a target we set the distribution to all 0s instead of smooth\n",
        "        # labeled distribution\n",
        "        smooth_distributions.masked_fill_(x== self.pad_idx, 0.)\n",
        "\n",
        "        return smooth_distributions\n"
      ],
      "metadata": {
        "id": "H3uy9PrDHYf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since PyTorch 1.10, we can use torch.nn.CrossEntropyLoss with the label_smoothing parameter."
      ],
      "metadata": {
        "id": "X5toQw4JHNGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_TOKEN_ID= 0\n",
        "criterion= nn.CrossEntropyLoss(label_smoothing=0.1, ignore_index=PAD_TOKEN_ID)"
      ],
      "metadata": {
        "id": "B-BEqURVUHgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop\n",
        "\n",
        "Next we create a generic training and scoring function to keep track of loss. We pass in a generic loss compute function that also handles parameter updates."
      ],
      "metadata": {
        "id": "jfJo6t6rbweP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def self_supervised_training(model, train_loader, criterion, optimizer, scheduler, steps,\n",
        "                             grad_accum_steps, eval_interval=200):\n",
        "    tr_loss_hist= []\n",
        "\n",
        "    # --- training loop ---\n",
        "    for step in range(steps):\n",
        "        start= time.time()\n",
        "        last_step= (step== steps-1)\n",
        "\n",
        "        # --- training step ---\n",
        "        model.train(True)\n",
        "        optimizer.zero_grad()\n",
        "        loss_accum= 0.0\n",
        "\n",
        "        # iterating over all batches accumulating gradients\n",
        "        for micro_step in range(grad_accum_steps):\n",
        "            # --- minibatch construction ---\n",
        "            Xmb, Ymb= train_loader.next_batch()    # TODO\n",
        "\n",
        "            # --- forward pass and get loss ---\n",
        "            logits, _= model(  )                   # TODO\n",
        "            # flatten for CrossEntropyLoss\n",
        "            loss= criterion(logits.view(-1, logits.size(-1)), Ymb.view(-1))\n",
        "\n",
        "            # --- gradient pass to calculate the gradients ---\n",
        "            loss= loss / grad_accum_steps\n",
        "            loss_accum += loss.detach()\n",
        "            loss.backward() # this is a plus equals, i.e., accumulates the grads\n",
        "\n",
        "        norm= torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        # determine and set the learning rate for this interaction\n",
        "        scheduler.step(step)\n",
        "\n",
        "        # --- update the parameters ---\n",
        "        optimizer.step()\n",
        "\n",
        "        # --- evaluation and track stats ---\n",
        "        if Xmb.device.type== 'cuda':\n",
        "            torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "        end= time.time()\n",
        "        dt= end - start # time difference in seconds\n",
        "        tokens_processed= train_loader.B * train_loader.T * grad_accum_steps\n",
        "        tokens_per_sec= tokens_processed / dt\n",
        "\n",
        "        tr_loss_hist.append(loss_accum.item())\n",
        "        if ((step % eval_interval== 0) or last_step):\n",
        "            print(f\"Step {step:4d} | loss: {loss_accum.item():.4f} | lr: {scheduler.get_last_lr():.3e} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.1f}\")\n",
        "\n",
        "    return tr_loss_hist\n"
      ],
      "metadata": {
        "id": "k6WcIrLkbxbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_batch_size= 16*1024 * 2  # B*T*n means we will have n grad accum steps\n",
        "B= 16    # micro batch size\n",
        "T= 1024  # sequence length\n",
        "\n",
        "assert total_batch_size % (B * T)== 0, \"Make sure total_batch_size is divisible by B * T\"\n",
        "grad_accum_steps= total_batch_size // (B * T)\n",
        "print(f\"Total desired batch size: {total_batch_size}\")\n",
        "print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "# TODO\n",
        "#train_loader= DataLoaderLite(B=B, T=T, data_file='input.txt', device=device)\n",
        "\n",
        "if device== 'cuda': # TF32 computationally more efficient (slightly the same precision of FP32)\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# create model\n",
        "V= 11\n",
        "model= Transformer(V, V, n_encoder=6, n_decoder=6).to(device)\n",
        "\n",
        "learning_rate= 6e-4\n",
        "steps= 2500\n",
        "warmup_steps= 100\n",
        "PAD_TOKEN_ID= 0\n",
        "\n",
        "criterion= nn.CrossEntropyLoss(label_smoothing=0.1, ignore_index=PAD_TOKEN_ID)\n",
        "\n",
        "optim_conf= Configure_Optimizer(\n",
        "    model, weight_decay=0.1, learning_rate=learning_rate, device=device\n",
        ")\n",
        "optimizer= optim_conf(betas=(0.9, 0.98), eps=1e-9)\n",
        "scheduler= WLR_Scheduler(optimizer, model.d_model, warmup_steps=warmup_steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz2MtlcuX_eo",
        "outputId": "1caf2ed3-6bf2-4e71-bb38-23b654ba3443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total desired batch size: 32768\n",
            "=> calculated gradient accumulation steps: 2\n",
            "Num decayed parameter tensors: 99, with 44057088 parameters\n",
            "Num non-decayed parameter tensors: 161, with 100363 parameters\n",
            "Using fused AdamW: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A First Example\n",
        "\n",
        "We can begin by trying out a simple copy-task. Given a random set of input symbols from a small vocabulary, the goal is to generate back those same symbols."
      ],
      "metadata": {
        "id": "Vt3wntdgb01A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# loss_hist= self_supervised_training(\n",
        "#     model, train_loader, criterion, optimizer, scheduler, steps, grad_accum_steps,\n",
        "#     eval_interval=500\n",
        "# )"
      ],
      "metadata": {
        "id": "l4CxrfGcb1eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: A Real World Example\n",
        "\n",
        "Now we consider a real-world example using the Multi30k German-English Translation task. This task is much smaller than the WMT task considered in the paper, but it illustrates the whole system. We also show how to use multi-gpu processing to make it really fast."
      ],
      "metadata": {
        "id": "22Wdfa5vb4t6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LX7IskS2b5ZA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}