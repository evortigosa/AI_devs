{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Time-Series Forecasting Transformer (TSFT) model"
      ],
      "metadata": {
        "id": "hcCn7pq13L_2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6kr2NHy25eN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "zJoF9Pxm9ZKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Transformer Decoder"
      ],
      "metadata": {
        "id": "5Cw7t-BR4Euh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    The Causal Attention Layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, block_size, n_head, flash_attn=True, dropout=0.1) -> None:\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert n_embed % n_head == 0, \"n_embed must be divisible by n_head\"\n",
        "        self.n_embed= n_embed\n",
        "        self.n_head = n_head\n",
        "        self.d_head = n_embed // n_head\n",
        "        self.flash_attn= flash_attn\n",
        "        # query, key, value projections in a single batch\n",
        "        self.c_attn= nn.Linear(n_embed, 3 * n_embed)\n",
        "        # output projection\n",
        "        self.o_proj= nn.Linear(n_embed, n_embed)\n",
        "        # regularization\n",
        "        self.dropout= nn.Dropout(p=dropout)\n",
        "        # masked attention on the outputs\n",
        "        if not self.flash_attn:\n",
        "            self.register_buffer('causal_mask',\n",
        "                torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C= x.size() # batch_size, sequence length, embedding dim (d_model)\n",
        "        assert C == self.n_embed, \"Input embedding dimension must match model embedding dimension\"\n",
        "        # calculate query, key, values for all heads\n",
        "        qkv= self.c_attn(x)\n",
        "        q, k, v= qkv.split(self.n_embed, dim=2) # q,k,v -> (B, T, C)\n",
        "        # reshape for Multi-Head Attention\n",
        "        q= q.view(B, -1, self.n_head, self.d_head).transpose(1, 2) # q,k,v view   -> (B, T, nh, dh)\n",
        "        k= k.view(B, -1, self.n_head, self.d_head).transpose(1, 2) # q,k,v transp -> (B, nh, T, dh)\n",
        "        v= v.view(B, -1, self.n_head, self.d_head).transpose(1, 2)\n",
        "        # Attention - the 'scaled dot product'\n",
        "        if self.flash_attn:\n",
        "            y= F.scaled_dot_product_attention(  # implements FlashAttention\n",
        "                q, k, v, dropout_p=self.dropout.p, is_causal=True\n",
        "            )\n",
        "        else:  # the original implementation of Attention\n",
        "            attn= (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.d_head))\n",
        "            # apply causal mask and normalize Attention scores\n",
        "            attn= attn.masked_fill(self.causal_mask[:,:,:T,:T]== 0, float('-inf'))\n",
        "            attn= F.softmax(attn, dim=-1)\n",
        "            attn= self.dropout(attn)\n",
        "            # compute Attention output\n",
        "            y= attn @ v # (B, nh, T, dh)\n",
        "        # concatenate multi-head outputs -- re-assembly all head outputs side by side\n",
        "        y= y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        # output projection\n",
        "        return self.o_proj(y)\n"
      ],
      "metadata": {
        "id": "_PrFJGyP3WrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    The Feed Forward Network (FFN) as a Gated Linear Unit (GLU) architecture.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, d_ffn, dropout=0.1) -> None:\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.gate_proj= nn.Linear(n_embed, d_ffn)\n",
        "        self.up_proj  = nn.Linear(n_embed, d_ffn)\n",
        "        self.down_proj= nn.Linear(d_ffn, n_embed)\n",
        "        self.act_fn= nn.SiLU()\n",
        "        self.dropout= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.act_fn(self.gate_proj(x)) * self.up_proj(x)\n",
        "        x= self.dropout(x)\n",
        "        x= self.down_proj(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ymVr_FkN3WuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Root Mean Square normalization (RMSNorm).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, eps=1e-5) -> None:\n",
        "        super(RMSNorm, self).__init__()\n",
        "        # scaling parameter gamma initialized with ones and the amount of parameters equal to dim\n",
        "        self.gamma= nn.Parameter(torch.ones(dim))\n",
        "        self.eps= eps\n",
        "\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm= self._norm(x.float()).type_as(x)\n",
        "\n",
        "        return x_norm * self.gamma\n"
      ],
      "metadata": {
        "id": "s0XpNt7tRjsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    The Decoder Block (pre-normalization version).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, block_size, n_head, d_ff, norm_type='layer', flash_attn=True,\n",
        "                 dropout=0.1) -> None:\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.norm_1= RMSNorm(n_embed) if norm_type=='rms' else nn.LayerNorm(n_embed)\n",
        "        self.attn= MultiHeadSelfAttention(n_embed, block_size, n_head, flash_attn, dropout)\n",
        "        self.dropout1= nn.Dropout(p=dropout)\n",
        "        self.norm_2= RMSNorm(n_embed) if norm_type=='rms' else nn.LayerNorm(n_embed)\n",
        "        self.ffn = FeedForward(n_embed, d_ff, dropout)\n",
        "        self.dropout2= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm= self.norm_1(x)\n",
        "        x= x + self.dropout1(self.attn(x_norm))\n",
        "        x_norm= self.norm_2(x)\n",
        "        x= x + self.dropout2(self.ffn(x_norm))\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "pF_PHu4K3WxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Transformer Decoder is essentially a stack of N Encoder Blocks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed=512, block_size=512, n_layer=6, n_head=8, d_ff=1024,\n",
        "                 norm_type='layer', flash_attn=True, dropout=0.1) -> None:\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.transformer= nn.ModuleList([\n",
        "            DecoderBlock(n_embed, block_size, n_head, d_ff, norm_type, flash_attn, dropout)\n",
        "            for _ in range(n_layer)\n",
        "        ])\n",
        "        self.norm_final= RMSNorm(n_embed) if norm_type=='rms' else nn.LayerNorm(n_embed)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.transformer:\n",
        "            x= layer(x)\n",
        "\n",
        "        return self.norm_final(x)\n"
      ],
      "metadata": {
        "id": "T8VvFa-J3W1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= TransformerDecoder().to(device)\n",
        "\n",
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}\\n')\n",
        "\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWcx8q703W7J",
        "outputId": "a4bea449-4d6a-4a1a-e31a-fcf6a1426718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 15769600\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerDecoder(\n",
              "  (transformer): ModuleList(\n",
              "    (0-5): 6 x DecoderBlock(\n",
              "      (norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiHeadSelfAttention(\n",
              "        (c_attn): Linear(in_features=512, out_features=1536, bias=True)\n",
              "        (o_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (ffn): FeedForward(\n",
              "        (gate_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
              "        (up_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
              "        (down_proj): Linear(in_features=1024, out_features=512, bias=True)\n",
              "        (act_fn): SiLU()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_config= {\n",
        "    'base':   dict(n_embed=512, block_size=512, n_layer=6, n_head=8, d_ff=1024,\n",
        "                   flash_attn=True, dropout=0.1),\n",
        "    'medium': dict(n_embed=1024, block_size=512, n_layer=8, n_head=16, d_ff=2048,\n",
        "                   flash_attn=True, dropout=0.1),\n",
        "    'large':  dict(n_embed=1280, block_size=512, n_layer=16, n_head=20, d_ff=2560,\n",
        "                   flash_attn=True, dropout=0.1)\n",
        "}"
      ],
      "metadata": {
        "id": "hQZFLYJs8BaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Medium config -----\n",
        "model= TransformerDecoder(**model_config['medium']).to(device)\n",
        "\n",
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sao1IJT3XGG",
        "outputId": "06d2b033-637d-47e6-c209-a3e9fdd32b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 83994624\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Large config -----\n",
        "model= TransformerDecoder(**model_config['large']).to(device)\n",
        "\n",
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rb3XC-t3XIN",
        "outputId": "ed11c329-d530-434b-b0fe-52225bc151f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 262412800\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.cuda as cuda\n",
        "\n",
        "data= torch.randn(16, 128, 512).to(device)\n",
        "model= TransformerDecoder(flash_attn=False).to(device)\n",
        "\n",
        "cuda.reset_peak_memory_stats()\n",
        "# Run the model with dense attention\n",
        "model.eval()\n",
        "dec= model(data)\n",
        "# Measure peak memory usage\n",
        "peak_flash_memory= cuda.max_memory_allocated()\n",
        "\n",
        "del dec\n",
        "\n",
        "print(f\"Flash Attention Memory: {np.round(peak_flash_memory / 10**6, decimals=2)} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uINtTkP3W9V",
        "outputId": "280d0991-f0b0-4a55-e67a-36e2030fbb34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flash Attention Memory: 676.35 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model= TransformerDecoder(flash_attn=False).to(device)\n",
        "\n",
        "cuda.reset_peak_memory_stats()\n",
        "# Run the model with dense attention\n",
        "model.eval()\n",
        "dec= model(data)\n",
        "# Measure peak memory usage\n",
        "peak_dense_memory= cuda.max_memory_allocated()\n",
        "\n",
        "del dec\n",
        "\n",
        "print(f\"Dense Attention Memory: {np.round(peak_dense_memory / 10**6, decimals=2)} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-PqJiRb3W_3",
        "outputId": "d5e72f28-c4d5-4188-8698-20d77396abff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense Attention Memory: 682.38 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Time-Series Forecasting Transformer (TSFT)"
      ],
      "metadata": {
        "id": "cvYFlozyl-uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Initializes the Embedding module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ) -> None:\n",
        "        super(Embedding, self).__init__()\n",
        "        # define the patch and positional embeddings... TODO\n",
        "\n",
        "\n",
        "    def forward(self, ts):\n",
        "\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "nOP0jm1G3XKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TSFTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Initializes a Time-Series Forecasting Transformer (TSFT) model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size,\n",
        "                 n_embed=512, block_size=512, n_layer=6, n_head=8, d_ff=1024, norm_type='layer',\n",
        "                 flash_attn=True, dropout=0.1) -> None:\n",
        "        super(TSFTransformer, self).__init__()\n",
        "        # initial considerations ... TODO\n",
        "\n",
        "\n",
        "        # define the patch and positional embeddings\n",
        "        self.embedding= Embedding()\n",
        "        # define the transformer decoder\n",
        "        self.decoder= TransformerDecoder(\n",
        "            n_embed, block_size, n_layer, n_head, d_ff, norm_type, flash_attn, dropout\n",
        "        )\n",
        "        # identity layer (no change to the tensor)\n",
        "        self.latent_space= nn.Identity()\n",
        "        # classification head\n",
        "        self.lm_head= nn.Linear(n_embed, vocab_size, bias=False)\n",
        "\n",
        "        # initialize parameters with Glorot / fan_avg\n",
        "        for p in self.parameters():\n",
        "            if p.dim()> 1:\n",
        "                nn.init.xavier_normal_(p)\n",
        "\n",
        "\n",
        "    def forward(self, ts):\n",
        "\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "jusEJ5y4Waxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5JXsKY68XGU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PcuwjG0xF0Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "T5gGZF3RUCzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if device== 'cuda':\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "    # Enable flash attention\n",
        "    torch.backends.cuda.enable_flash_sdp(True)"
      ],
      "metadata": {
        "id": "KHDnBMuLUKCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8EpDHA9rUWM1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}