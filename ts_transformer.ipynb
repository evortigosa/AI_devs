{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Time-Series Forecasting Transformer (TSFT) model"
      ],
      "metadata": {
        "id": "hcCn7pq13L_2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P6kr2NHy25eN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "zJoF9Pxm9ZKS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count how many trainable weights the model has\n",
        "def count_parameters(model) -> None:\n",
        "    total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'Number of parameters: {total_params}')"
      ],
      "metadata": {
        "id": "Td7wSBDnejaZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Transformer Architecture"
      ],
      "metadata": {
        "id": "5Cw7t-BR4Euh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    The Multi-Headed Self-Attention Layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head, dropout=0.1, flash_attn=True, bias=True) -> None:\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert n_embed % n_head == 0, \"n_embed must be divisible by n_head\"\n",
        "        self.n_embed= n_embed\n",
        "        self.n_head = n_head\n",
        "        self.d_head = n_embed // n_head\n",
        "        self.flash_attn= flash_attn\n",
        "        # query, key, value projections in a single batch\n",
        "        self.c_attn= nn.Linear(n_embed, 3 * n_embed, bias=bias)\n",
        "        # output projection\n",
        "        self.o_proj= nn.Linear(n_embed, n_embed, bias=bias)\n",
        "        # regularization\n",
        "        self.dropout= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, causal_mask=None):\n",
        "        B, T, C= x.size()  # x(batch_size, sequence length, n_embed)\n",
        "        assert C== self.n_embed, \"Input embedding dimension must match model embedding dimension\"\n",
        "        # calculate query, key, values for all heads\n",
        "        qkv= self.c_attn(x)\n",
        "        q, k, v= qkv.split(self.n_embed, dim=2) # q,k,v -> (B, T, C)\n",
        "        # reshape for Multi-Head Attention\n",
        "        q= q.view(B, -1, self.n_head, self.d_head).transpose(1, 2) # q,k,v view   -> (B, T, nh, dh)\n",
        "        k= k.view(B, -1, self.n_head, self.d_head).transpose(1, 2) # q,k,v transp -> (B, nh, T, dh)\n",
        "        v= v.view(B, -1, self.n_head, self.d_head).transpose(1, 2)\n",
        "        # Attention - the 'scaled dot product'\n",
        "        if self.flash_attn:\n",
        "            is_causal= True if causal_mask is not None else False\n",
        "            # implements FlashAttention\n",
        "            y= F.scaled_dot_product_attention(\n",
        "                q, k, v, dropout_p=self.dropout.p, is_causal=is_causal\n",
        "            )\n",
        "        else:  # the original implementation of Attention\n",
        "            attn= (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.d_head))\n",
        "            # apply causal mask (when the mask is not None)\n",
        "            if causal_mask is not None:\n",
        "                attn= attn.masked_fill(causal_mask[:,:,:T,:T]== 0, float('-inf'))\n",
        "            # normalize Attention scores\n",
        "            attn= F.softmax(attn, dim=-1)\n",
        "            attn= self.dropout(attn)\n",
        "            # compute Attention output\n",
        "            y= attn @ v # (B, nh, T, dh)\n",
        "        # concatenate multi-head outputs -- re-assembly all head outputs side by side\n",
        "        y= y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        # output projection\n",
        "        return self.o_proj(y)\n"
      ],
      "metadata": {
        "id": "_PrFJGyP3WrI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    The Feed Forward Network (FFN) as a Gated Linear Unit (GLU) architecture.\n",
        "    The use of a gated mechanism enhances the expressivity of the FFN by introducing gating.\n",
        "    This is more flexible than traditional MLP layers and is proven effective in many Transformer\n",
        "    variants like GPT-NeoX or PaLM.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, d_ff, dropout=0.1, bias=True) -> None:\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.gate_proj= nn.Linear(n_embed, d_ff, bias=bias)\n",
        "        self.up_proj  = nn.Linear(n_embed, d_ff, bias=bias)\n",
        "        self.down_proj= nn.Linear(d_ff, n_embed, bias=bias)\n",
        "        self.act_fn= nn.SiLU()\n",
        "        self.dropout= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.act_fn(self.gate_proj(x)) * self.up_proj(x)\n",
        "        x= self.dropout(x)\n",
        "        x= self.down_proj(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ymVr_FkN3WuC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Root Mean Square normalization (RMSNorm).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, eps=1e-5) -> None:\n",
        "        super(RMSNorm, self).__init__()\n",
        "        # scaling parameter gamma initialized with ones and the amount of parameters equal to dim\n",
        "        self.gamma= nn.Parameter(torch.ones(dim))\n",
        "        self.eps= eps\n",
        "\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm= self._norm(x.float()).type_as(x)\n",
        "\n",
        "        return x_norm * self.gamma\n"
      ],
      "metadata": {
        "id": "s0XpNt7tRjsU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    The Transformer Block (Encoder/Decoder, pre-normalization version).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, n_head, d_ff, dropout=0.1, norm_type='layer', flash_attn=True,\n",
        "                 bias=True) -> None:\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.norm1= RMSNorm(n_embed) if norm_type=='rms' else nn.LayerNorm(n_embed)\n",
        "        self.attn= MultiHeadSelfAttention(n_embed, n_head, dropout, flash_attn, bias)\n",
        "        self.dropout1= nn.Dropout(p=dropout)\n",
        "        self.norm2= RMSNorm(n_embed) if norm_type=='rms' else nn.LayerNorm(n_embed)\n",
        "        self.ffn= FeedForward(n_embed, d_ff, dropout, bias)\n",
        "        self.dropout2= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, causal_mask=None):\n",
        "        x_norm= self.norm1(x)\n",
        "        x= x + self.dropout1(self.attn(x_norm, causal_mask))\n",
        "        x_norm= self.norm2(x)\n",
        "        x= x + self.dropout2(self.ffn(x_norm))\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "pF_PHu4K3WxY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A Transformer model is essentially a stack of N Encoder/Decoder Blocks.\n",
        "    If is_causal=True, we have a Decoder Transformer, otherwise, an Encoder Transformer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, is_causal=True, n_layer=6, n_embed=512, block_size=768, n_head=8, d_ff=1024,\n",
        "                 dropout=0.1, norm_type='layer', flash_attn=True, bias=True) -> None:\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.block_size= block_size\n",
        "        # define the transformer\n",
        "        self.transformer= nn.ModuleList([\n",
        "            TransformerBlock(n_embed, n_head, d_ff, dropout, norm_type, flash_attn, bias)\n",
        "            for _ in range(n_layer)\n",
        "        ])\n",
        "        self.norm_final= RMSNorm(n_embed) if norm_type=='rms' else nn.LayerNorm(n_embed)\n",
        "        # masked attention on the outputs when the TransformerModel is a Decoder\n",
        "        # positions depend on the past only -- create a lower triangular matrix (2-D tensor)\n",
        "        if is_causal and not flash_attn:\n",
        "            self.register_buffer('causal_mask',\n",
        "                torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size)\n",
        "            )\n",
        "        elif is_causal and flash_attn: self.causal_mask= True\n",
        "        else: self.causal_mask= None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C= x.size()  # x(batch_size, sequence length, n_embed)\n",
        "        assert T <= self.block_size, \\\n",
        "            f'Cannot forward sequence of length {T}, block size is only {self.block_size}'\n",
        "        # forward the embedding through the transformer\n",
        "        for block in self.transformer:\n",
        "            x= block(x, self.causal_mask)\n",
        "\n",
        "        return self.norm_final(x)\n"
      ],
      "metadata": {
        "id": "T8VvFa-J3W1H"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= TransformerModel(bias=False).to(device)\n",
        "\n",
        "count_parameters(model)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWcx8q703W7J",
        "outputId": "3e8a81fb-9ac2-4ee8-f386-3bb9ba771049"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 15741952\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (transformer): ModuleList(\n",
              "    (0-5): 6 x TransformerBlock(\n",
              "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiHeadSelfAttention(\n",
              "        (c_attn): Linear(in_features=512, out_features=1536, bias=False)\n",
              "        (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (ffn): FeedForward(\n",
              "        (gate_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
              "        (up_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
              "        (down_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
              "        (act_fn): SiLU()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_config= {\n",
        "    'base':   dict(\n",
        "        n_layer=6, n_embed=512, block_size=768, n_head=8, d_ff=1024, dropout=0.1, bias=False),\n",
        "    'medium': dict(\n",
        "        n_layer=12, n_embed=768, block_size=1024, n_head=12, d_ff=1536, dropout=0.1, bias=False),\n",
        "    'large':  dict(\n",
        "        n_layer=16, n_embed=1024, block_size=1280, n_head=16, d_ff=2048, dropout=0.1, bias=False),\n",
        "    'xlarge':  dict(\n",
        "        n_layer=24, n_embed=1280, block_size=2048, n_head=20, d_ff=2560, dropout=0.1, bias=False)\n",
        "}"
      ],
      "metadata": {
        "id": "hQZFLYJs8BaN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Medium config -----\n",
        "model= TransformerModel(**model_config['medium']).to(device)\n",
        "\n",
        "count_parameters(model)\n",
        "del model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sao1IJT3XGG",
        "outputId": "df9a1ae4-f723-4932-b742-3125eadcfc1c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 70817280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Large config -----\n",
        "model= TransformerModel(**model_config['large']).to(device)\n",
        "\n",
        "count_parameters(model)\n",
        "del model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rb3XC-t3XIN",
        "outputId": "5cfc50fb-6d13-4a38-bf05-2ff138cd0fdb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 167839744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- XLarge config -----\n",
        "model= TransformerModel(**model_config['xlarge']).to(device)\n",
        "\n",
        "count_parameters(model)\n",
        "del model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McXWbM5peUzE",
        "outputId": "4fb97ba9-7467-460d-e20b-1aaa17ee32f9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 393341440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory usage"
      ],
      "metadata": {
        "id": "3xcjEp3khix6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.cuda as cuda\n",
        "\n",
        "data= torch.randn(32, 768, 512).to(device)\n",
        "model= TransformerModel(flash_attn=False, bias=False).to(device)\n",
        "\n",
        "cuda.reset_peak_memory_stats()\n",
        "# Run the model with original Attention\n",
        "model.eval()\n",
        "dec= model(data)\n",
        "# Measure peak memory usage\n",
        "peak_dense_memory= cuda.max_memory_allocated()\n",
        "\n",
        "del dec, model\n",
        "\n",
        "print(f\"Original Attention Memory: {np.round(peak_dense_memory / 10**6, decimals=2)} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-PqJiRb3W_3",
        "outputId": "c48220c0-43ff-4c7c-a650-c27f8e213f57"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Attention Memory: 8751.33 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if device== 'cuda':\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "    # Enable flash attention\n",
        "    torch.backends.cuda.enable_flash_sdp(True)\n",
        "\n",
        "model= TransformerModel(flash_attn=True, bias=False).to(device)\n",
        "\n",
        "cuda.reset_peak_memory_stats()\n",
        "# Run the model with FlashAttention\n",
        "model.eval()\n",
        "dec= model(data)\n",
        "# Measure peak memory usage\n",
        "peak_flash_memory= cuda.max_memory_allocated()\n",
        "\n",
        "del dec, model\n",
        "\n",
        "print(f\"Flash Attention Memory: {np.round(peak_flash_memory / 10**6, decimals=2)} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uINtTkP3W9V",
        "outputId": "7b1ae49a-556c-4c58-d2cd-37b047ac06a0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flash Attention Memory: 5076.33 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Time-Series Forecasting Transformer (TSFT) model"
      ],
      "metadata": {
        "id": "cvYFlozyl-uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Initializes the Embedding module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ) -> None:\n",
        "        super(Embedding, self).__init__()\n",
        "        # define the patch and positional embeddings... TODO\n",
        "\n",
        "\n",
        "    def forward(self, ts):\n",
        "\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "nOP0jm1G3XKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OutputBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    The MLP (classification) head.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed, d_ff, num_classes, dropout=0.1, bias=True,\n",
        "                 fine_tune=False) -> None:\n",
        "        super(OutputBlock, self).__init__()\n",
        "        if fine_tune:\n",
        "            self.c_head= nn.Linear(n_embed, num_classes, bias=bias)\n",
        "        else:\n",
        "            self.c_head= nn.Sequential(\n",
        "                nn.Linear(n_embed, d_ff, bias=bias),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(p=dropout),\n",
        "                nn.Linear(d_ff, num_classes, bias=bias),\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.c_head(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "hjGkatnoHMTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TSFTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Initializes a Time-Series Forecasting Transformer (TSFT) model.\n",
        "    If is_causal=True, we have a Decoder Transformer, otherwise, an Encoder Transformer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, is_causal=True,\n",
        "                 n_layer=6, n_embed=512, block_size=768, n_head=8, d_ff=1024, dropout=0.1,\n",
        "                 norm_type='layer', flash_attn=True, bias=True, fine_tune=False) -> None:\n",
        "        super(TSFTransformer, self).__init__()\n",
        "        # initial considerations ... TODO\n",
        "\n",
        "\n",
        "        # define the patch and positional embeddings\n",
        "        self.embedding= Embedding()\n",
        "        # define the transformer decoder\n",
        "        self.decoder= TransformerModel(\n",
        "            is_causal, n_layer, n_embed, block_size, n_head, d_ff, dropout, norm_type,\n",
        "            flash_attn, bias\n",
        "        )\n",
        "        # identity layer (no change to the tensor)\n",
        "        self.latent_space= nn.Identity()\n",
        "        # classification head\n",
        "        self.lm_head= OutputBlock(n_embed, d_ff, vocab_size, dropout, bias, fine_tune)\n",
        "\n",
        "        # initialize Linear modules with Glorot / fan_avg\n",
        "        # let Normalization and Embedding modules use default initializations\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "    def forward(self, ts):\n",
        "\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "jusEJ5y4Waxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5JXsKY68XGU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PcuwjG0xF0Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "T5gGZF3RUCzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if device== 'cuda':\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "    # Enable flash attention\n",
        "    torch.backends.cuda.enable_flash_sdp(True)"
      ],
      "metadata": {
        "id": "KHDnBMuLUKCm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8EpDHA9rUWM1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}