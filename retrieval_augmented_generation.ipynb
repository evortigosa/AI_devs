{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6768f5fc",
   "metadata": {},
   "source": [
    "# RAG From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f770e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5873bcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.85337  ,  0.011645 , -0.033377 , -0.31981  ,  0.26126  ,\n",
       "        0.16059  ,  0.010724 , -0.15542  ,  0.75044  ,  0.10688  ,\n",
       "        1.9249   , -0.45915  , -3.3887   , -1.2152   , -0.054263 ,\n",
       "       -0.20555  ,  0.54706  ,  0.4371   ,  0.25194  ,  0.0086557,\n",
       "       -0.56612  , -1.1762   ,  0.010479 , -0.55316  , -0.15816  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Downloading a word encoder.\n",
    "We can use word2vect, but glove downloads way faster. For our purposes they're conceptually identical\n",
    "\"\"\"\n",
    "import sys\n",
    "\n",
    "#!{sys.executable} -m pip install gensim\n",
    "\n",
    "import gensim.downloader as gd\n",
    "\n",
    "# downloading encoder\n",
    "word_encoder= gd.load('glove-twitter-25')\n",
    "\n",
    "# getting the embedding for a word\n",
    "word_encoder['apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a20cad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.3483393e-01,  1.3683620e-01,  2.0645106e-01, -2.1831200e-01,\n",
       "       -1.8181981e-01,  2.6023200e-01,  1.3276964e+00,  1.7272198e-01,\n",
       "       -2.7881199e-01, -4.2115799e-01, -4.7215199e-01, -5.3013992e-02,\n",
       "       -4.6326599e+00,  4.3883198e-01,  3.6487383e-01, -3.6672002e-01,\n",
       "       -2.6924044e-03, -3.0394283e-01, -5.5415201e-01, -9.1787003e-02,\n",
       "       -4.4997922e-01, -1.4819117e-01,  1.0654800e-01,  3.7024397e-01,\n",
       "       -4.6688594e-02], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining a function for embedding an entire document to a single mean vector\n",
    "\n",
    "def embed_sequence(sequence):\n",
    "    \n",
    "    vects= word_encoder[sequence.split(' ')]\n",
    "    \n",
    "    return np.mean(vects, axis=0)\n",
    "\n",
    "\n",
    "embed_sequence('its a sunny day today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0255d4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar phrases:\n",
      "8.496297497302294\n",
      "Different phrases:\n",
      "11.832107525318861\n"
     ]
    }
   ],
   "source": [
    "# calculating distance between two embedding vectors uses manhattan distance\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def calc_distance(embedding1, embedding2):\n",
    "    \n",
    "    dist= cdist(np.expand_dims(embedding1, axis=0), \n",
    "                np.expand_dims(embedding2, axis=0), metric='cityblock')[0][0]\n",
    "    \n",
    "    return dist\n",
    "\n",
    "\n",
    "print('Similar phrases:')\n",
    "f1= embed_sequence('sunny day today')\n",
    "f2= embed_sequence('rainy morning presently')\n",
    "print(calc_distance(f1, f2))\n",
    "\n",
    "print('Different phrases:')\n",
    "f3= embed_sequence('perhaps reality is painful')\n",
    "print(calc_distance(f1, f3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8836579",
   "metadata": {},
   "source": [
    "# Retrieval and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "691581cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defining documents\n",
    "for simplicities we only included words the embedder knows. We could just parse out all the words the \n",
    "embedder doesn't know, though. After all, the retreival is done on a mean of all embeddings, so a \n",
    "missing word or two is of little consequence\n",
    "\"\"\"\n",
    "\n",
    "documents= {\"menu\": \"ratatouille is a stew thats twelve dollars and fifty cents also gazpacho is a salad thats thirteen dollars and ninety eight cents also hummus is a dip thats eight dollars and seventy five cents also meat sauce is a pasta dish thats twelve dollars also penne marinera is a pasta dish thats eleven dollars also shrimp and linguini is a pasta dish thats fifteen dollars\",\n",
    "            \"events\": \"on thursday we have karaoke and on tuesdays we have trivia\",\n",
    "            \"allergins\": \"the only item on the menu common allergen is hummus which contain pine nuts\",\n",
    "            \"info\": \"the resteraunt was founded by two brothers in two thousand and three\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edd18130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding relevent doc for: \"what pasta dishes do you have\"\n",
      "('menu', 'ratatouille is a stew thats twelve dollars and fifty cents also gazpacho is a salad thats thirteen dollars and ninety eight cents also hummus is a dip thats eight dollars and seventy five cents also meat sauce is a pasta dish thats twelve dollars also penne marinera is a pasta dish thats eleven dollars also shrimp and linguini is a pasta dish thats fifteen dollars')\n",
      "----\n",
      "Finding relevent doc for: \"what events do you guys do\"\n",
      "('events', 'on thursday we have karaoke and on tuesdays we have trivia')\n",
      "----\n",
      "Finding relevent doc for: \"what pasta dishes do you guys have\"\n",
      "('info', 'the resteraunt was founded by two brothers in two thousand and three')\n"
     ]
    }
   ],
   "source": [
    "# defining a function that retreives the most relevent document\n",
    "\n",
    "def retrieve_relevant(prompt, documents=documents):\n",
    "    \n",
    "    min_dist= 1e10\n",
    "    r_docname= ''\n",
    "    r_doc= ''\n",
    "    \n",
    "    for docname, doc in documents.items():\n",
    "        dist= calc_distance(embed_sequence(prompt), embed_sequence(doc))\n",
    "        \n",
    "        if (dist< min_dist):\n",
    "            min_dist= dist\n",
    "            r_docname= docname\n",
    "            r_doc= doc\n",
    "            \n",
    "    return r_docname, r_doc\n",
    "\n",
    "\n",
    "prompt= 'what pasta dishes do you have'\n",
    "print(f'Finding relevent doc for: \"{prompt}\"')\n",
    "print(retrieve_relevant(prompt))\n",
    "print('----')\n",
    "\n",
    "prompt= 'what events do you guys do'\n",
    "print(f'Finding relevent doc for: \"{prompt}\"')\n",
    "print(retrieve_relevant(prompt))\n",
    "print('----')\n",
    "\n",
    "prompt= 'what pasta dishes do you guys have'\n",
    "print(f'Finding relevent doc for: \"{prompt}\"')\n",
    "print(retrieve_relevant(prompt))\n",
    "# The last case comes with quirks from the reality of the art"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca16f6e3",
   "metadata": {},
   "source": [
    "# Augmenting and Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d2bc5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt for: \"what events do you guys do\":\n",
      "\n",
      "Answer the prompt based on the folowing documents:\n",
      "==== document: events ====\n",
      "on thursday we have karaoke and on tuesdays we have trivia\n",
      "====\n",
      "\n",
      "prompt: what events do you guys do\n",
      "response:\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Defining retreival and augmentation creating a function that does retrieval and augmentation,\n",
    "this can be passed straight to the LLM model\n",
    "\"\"\"\n",
    "\n",
    "def retrieve_and_augment(prompt, documents=documents):\n",
    "    \n",
    "    docname, doc= retrieve_relevant(prompt, documents)\n",
    "    \n",
    "    answer= 'Answer the prompt based on the folowing documents:\\n'\n",
    "    \n",
    "    return f\"{answer}==== document: {docname} ====\\n{doc}\\n====\\n\\nprompt: {prompt}\\nresponse:\"\n",
    "\n",
    "\n",
    "prompt= 'what events do you guys do'\n",
    "print(f'Prompt for: \"{prompt}\":\\n')\n",
    "print(retrieve_and_augment(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7562f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using RAG with OpenAI's gpt model\n",
    "\n",
    "#!{sys.executable} -m pip install openai\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# lets initialize the API client\n",
    "client= OpenAI(\n",
    "    api_key=os.environ.get('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "prompts= ['what pasta dishes do you have',\n",
    "          'what events do you guys do',\n",
    "          'oh cool what is karaoke'\n",
    "         ]\n",
    "\n",
    "for prompt in prompts:\n",
    "    \n",
    "    ra_prompt= retrieve_and_augment(prompt)\n",
    "    response= client.chat.completions.create(model='gpt-3.5-turbo', prompt=ra_prompt,\n",
    "                                       max_tokens=80).choices[0].text\n",
    "    \n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Response: '{response}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f25324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# towardsdatascience.com/retrieval-augmented-generation-intuitively-and-exhaustively-explain-6a39d6fe6fc9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46825d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/openai/openai-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
