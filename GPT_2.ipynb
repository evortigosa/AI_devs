{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na6RJnHmOMLX"
      },
      "source": [
        "# Building the 124M Parameters GPT-2 Model\n",
        "\n",
        "GPT-2 is a decoder only Transformer model, which means that the encoder and cross-attention modules where missing compared to the figure in **Attention is All You Need**. Also, there are two main differences:\n",
        "- There is a reshuffling of the layer Norms (Sec 2.3 from GPT-2 paper). Layer Norms were moved to the input of each sub-block, similar to a pre-activation residual network.\n",
        "- An additional layer norm was added after the final self-attn block (hight before the final classifier)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NhnBi7dgOK7V"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import inspect\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XN6iz6y4Q7xP"
      },
      "outputs": [],
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBs3dK2gr7RG"
      },
      "source": [
        "# GPT-2 Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dyklSBfBQ7z7"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size:int= 1024  # max sequence length\n",
        "    vocab_size:int= 50257 # number of tokens: 50k BPE merges + 256 bytes tokens + 1 <|endoftext|>\n",
        "    n_layer:int= 12       # number of layers\n",
        "    n_head:int= 12        # number of heads\n",
        "    n_embed:int= 768      # embedding dimension\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO8s-KeZctxG"
      },
      "source": [
        "\n",
        "# Input Embedding\n",
        "\n",
        "An embedding is a mapping of discrete objects, such as words, phrases, or documents, into a continuous vector space. In this space, semantically similar items are located close to each other. This is achieved by training machine learning models on large corpora of text data to learn patterns and contexts in which words or phrases appear.\n",
        "\n",
        "- **Token Embedding**: 50257 tokens in the vocabulary size, and for each token we have a 768 dimensional embedding that is the distributed representation that stands in for that tokens. Each token is a little string piece and then the 768 numbers are the vector that represents that token. So, this is our lookup table for tokens.\n",
        "- **Positional Embedding**: 1024 x 768 dimensional lookup table. GPT-2 has a maximum sequence length of 1024; so, we have 1024 positions that each token can be attending to in the past. Everyone of those positions in GPT-2 has a fized vector of 768 weights learned by optimization. Each of the 1024 rows represent the positions of tokens and it is precessed by the Transformer to recover all the relative positions and realizer which token is where and attend to them depending on their position, not just their content. In the **Attention is All You Need** paper the positional embedding is initialized with cosine and sine of different frequencies and it is fixed, but in GPT-2 these are just parameters and they are trained from scratch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfroWJp62v3B"
      },
      "source": [
        "# Decoder Block\n",
        "\n",
        "There is multiple Attention heads inside a Decoder block and they are all functioning in parallel, their outputs are just being concatenated, and that becomes the output of Multi-Headed Attention. The heads are like parallel streams and their outputs get concatenated.\n",
        "\n",
        "**FlashAttention (FA)** is a kernel fusion operation that makes the Attention implementation faster by avoiding loads and stores that torch.compile is not able to optimize. It relies on a \"online softmax\" trick (page 4 of FA paper) that incrementally evaluates softmax.\n",
        "\n",
        "https://arxiv.org/abs/2205.14135\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rhgqdFShddUl"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Defining the Attention operation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config) -> None:\n",
        "        super(CausalSelfAttention, self).__init__()\n",
        "        assert config.n_embed % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn= nn.Linear(config.n_embed, 3 * config.n_embed)\n",
        "        # output projection\n",
        "        self.c_proj= nn.Linear(config.n_embed, config.n_embed)\n",
        "        self.c_proj.GPT_SCALE_INIT= 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embed= config.n_embed\n",
        "        \"\"\"\n",
        "        # register_buffer is only required for the original implementation of Attention\n",
        "        # not really a 'bias', more a mask, but following the OpenAI/HF naming\n",
        "        self.register_buffer('bias',\n",
        "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "            .view(1, 1, config.block_size, config.block_size)\n",
        "        )\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C= x.size() # batch_size, sequence_length, embedding dim (n_embed)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is 'number of heads', hs is 'head size', and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head= 12, hs= 64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv= self.c_attn(x)\n",
        "        q, k, v= qkv.split(self.n_embed, dim=2)\n",
        "        k= k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q= q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v= v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        # Attention (materializes the large (T,T) matrix for all the queries and keys)\n",
        "        \"\"\"\n",
        "        # this is the original implementation of Attention\n",
        "        att= (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att= att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att= F.softmax(att, dim=-1)\n",
        "        y= att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        # and the following implements FlashAttention\n",
        "        \"\"\"\n",
        "        y= F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        y= y.transpose(1, 2).contiguous().view(B, T, C) # re-assembly all head outputs side by side\n",
        "        # output projection\n",
        "        y= self.c_proj(y)\n",
        "\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AShDvhV0DSlq"
      },
      "source": [
        "Tokens are lined up in a sequence and each token at this stage of the attention emits three vectors, the query, key, and value. First, the queries and the keys have to multiply each other to get the Attention amount like how interesting they found each other; so, they have to interact multiplicatively.\n",
        "\n",
        "After calculating and splitting qkv, we make the number of heads nh into a batch dimension, and it is a batch dimension just like B. So, in the operations that follow, PyTorch treats B and nh as batches and it applies all the operations on all of them in parallel in both the batches and the heads. Next, queries and keys interact to give us their attention. The autoregressive mask makes sure that the tokens only attend to tokens before them and never to tokens in the future. Softmax normalizes the attention (so, it sums to one). Then, the attention matrix (att) multiply with the values, which is basically a way to do a weighted sum of the values of the tokens that we found interesting at every single tokens. The last transpose operation performs the concatenation operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i8n3Lz0Mddba"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Defining the MLP (aka FeedForward).\n",
        "    It is a two linear projectons sandwiched in between the GELU nonlinearity approximating\n",
        "    a tanh (basically like a ReLU except there is no exactly flat tail at zero). Today there is\n",
        "    no good reason to use the approximate version of GELU (but GPT-2 used it).\n",
        "    https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config) -> None:\n",
        "        super(MLP, self).__init__()\n",
        "        self.c_fc=   nn.Linear(config.n_embed, 4 * config.n_embed)\n",
        "        self.gelu=   nn.GELU(approximate='tanh')\n",
        "        self.c_proj= nn.Linear(4 * config.n_embed, config.n_embed)\n",
        "        self.c_proj.GPT_SCALE_INIT= 1\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.c_fc(x)\n",
        "        x= self.gelu(x)\n",
        "        x= self.c_proj(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EarH_AInhgCd"
      },
      "source": [
        "Attention is a communication operation. It is where the tokens and the 1024 positional tokens lined up in a sequence and this is where tokens communicate, this is where they exchange information. Attention is an aggregation function, it is like a pooling (reducing operation) function or a weghted sum function. The MLP happens at every single token individually, i.e., there is no information being collected or exchanged between the tokens. Therefore, the Attention is the reduce and the MLP is the map.\n",
        "\n",
        "The Transformer just ends up being a repeated application of map produce. MLP is thinking about the information that Attention gathered. The sequence of decoder blocks iteratively refines the representation is at the residual stream."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "imtYxD-UQ755"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Defining the Decoder. Pre-normalization version.\n",
        "    LayerNorms are before the application of Attention and FeedForward in order to keep\n",
        "    a clean residual stream all the way down to the input tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config) -> None:\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.ln_1= nn.LayerNorm(config.n_embed)\n",
        "        self.attn= CausalSelfAttention(config)\n",
        "        self.ln_2= nn.LayerNorm(config.n_embed)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= x + self.attn(self.ln_1(x))\n",
        "        x= x + self.mlp(self.ln_2(x))\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOKz-Sg935ZQ"
      },
      "source": [
        "# Build the GPT-2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M2QG3UDlQ72p"
      },
      "outputs": [],
      "source": [
        "class GPT2(nn.Module):\n",
        "    \"\"\"\n",
        "    GPT-2 architecture skeleton.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config) -> None:\n",
        "        super(GPT2, self).__init__()\n",
        "        self.config= config\n",
        "        # ModuleDict allows us to index submodules using keys just like a dictionary\n",
        "        # ModuleList indexes itens using integers\n",
        "        self.transformer= nn.ModuleDict(dict(\n",
        "            wte= nn.Embedding(config.vocab_size, config.n_embed), # Token Embedding\n",
        "            wpe= nn.Embedding(config.block_size, config.n_embed), # Positional Embedding\n",
        "            h= nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)]),\n",
        "            ln_f= nn.LayerNorm(config.n_embed)\n",
        "        )) # projection from n_embed numbers of embedding dimensions to vocab_size\n",
        "        self.lm_head= nn.Linear(config.n_embed, config.vocab_size, bias=False)\n",
        "        # weight sharing scheme\n",
        "        self.transformer.wte.weight= self.lm_head.weight\n",
        "        # init params by iterating over them\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std= 0.02\n",
        "            if hasattr(module, 'GPT_SCALE_INIT'):\n",
        "                # std scaling down -- we have two residual connections per decoder block\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias) # zeros init is not default in PyTorch\n",
        "        elif isinstance(module, nn.Embedding):    # 0.02 is close of that used in xavier init\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is our token indices of shape (B, T)\n",
        "        B, T= idx.size()\n",
        "        assert T <= self.config.block_size, f'Cannot forward sequence of length {T}, block size is only {self.config.block_size}'\n",
        "        # forward the token and the position embeddings\n",
        "        pos= torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb= self.transformer.wpe(pos) # position embeddings of shape (T, n_embed)\n",
        "        tok_emb= self.transformer.wte(idx) # token embeddings of shape (B, T, n_embed)\n",
        "        x= tok_emb + pos_emb # pos_emb is broadcasted at every row stacked up in tok_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x= block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x= self.transformer.ln_f(x)\n",
        "        logits= self.lm_head(x) # (B, T, vocab_size)\n",
        "        # at every B by T we will calculate the logits for what tokens come next in the sequence\n",
        "        loss= None\n",
        "        if targets is not None:\n",
        "            loss= F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "            # we are flattening out 3D logits tensor into 2D (B*T, vocab_size)\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"\n",
        "        Loads pretrained GPT-2 model weights from HF.\n",
        "        \"\"\"\n",
        "\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print('Loading weights from pretrained GPT: %s' % model_type)\n",
        "\n",
        "        # n_layer, n_head, and n_embed are determined from model_type\n",
        "        config_args= {\n",
        "            'gpt2':        dict(n_layer=12, n_head=12, n_embed=768),  #  124M params\n",
        "            'gpt2-medium': dict(n_layer=24, n_head=16, n_embed=1024), #  350M params,\n",
        "            'gpt2-large':  dict(n_layer=36, n_head=20, n_embed=1280), #  774M params,\n",
        "            'gpt2-xl':     dict(n_layer=48, n_head=25, n_embed=1600), # 1558M params,\n",
        "        }[model_type]\n",
        "        config_args['vocab_size']= 50257 # always 50257 for GPT models\n",
        "        config_args['block_size']= 1024  # always 1024 for GPT models\n",
        "        # create a from-scratch initializated miniGPT model\n",
        "        config= GPTConfig(**config_args)\n",
        "        model= GPT2(config)\n",
        "        sd= model.state_dict()\n",
        "        sd_keys= sd.keys()\n",
        "        sd_keys= [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask\n",
        "\n",
        "        # init a HF/Transformers model\n",
        "        model_hf= GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf= model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf= sd_hf.keys()\n",
        "        sd_keys_hf= [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # just a buffer\n",
        "        sd_keys_hf= [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same\n",
        "        transposed= ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the OpenAI checkpoints use a 'Conv1D' module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"Mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for thr Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
        "        \"\"\"\n",
        "        Splitting up the parameters tha should be weight decayed and those that should not.\n",
        "        Weight decay acts like a regularization because we're pulling down all the weights,\n",
        "        forcing the optimization to more of the weights and not allowing any one of the weights\n",
        "        individually to be way too large (we're forcing the network to distribute the work across)\n",
        "        more channels because there is a pull of gravity on the weights themselves.\n",
        "        \"\"\"\n",
        "        # start with a lot of the candidate parameters (that require grad)\n",
        "        param_dict= {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict= {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any 2D parameters will be weight dacayed, otherwise no\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layerNorms don't\n",
        "        # but most of the parameters will be decayed\n",
        "        decay_params  = [p for n, p in param_dict.items() if p.dim()>= 2]\n",
        "        nodecay_params= [p for n, p in param_dict.items() if p.dim() < 2] # one-dim tensors\n",
        "        optim_groups= [\n",
        "            {'params':   decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params  = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params= sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"Num decayed parameter tensors: {len(decay_params)}, with {num_decay_params} parameters\")\n",
        "        print(f\"Num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params} parameters\")\n",
        "        # create AdamW optimizer and use the fused version of it is available\n",
        "        fused_available= 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        # fused is a lot faster when it is available and when running on cuda\n",
        "        use_fused= fused_available and device== \"cuda\"\n",
        "        print(f\"Using fused AdamW: {use_fused}\")\n",
        "        # create a AdamW PyTorch optimizer -- bug fix of Adam\n",
        "        optimizer= torch.optim.AdamW(\n",
        "            optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused\n",
        "        )\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eQBj848Q78W",
        "outputId": "4b2e4650-2ab5-46c6-d37e-87f462d0bb43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x DecoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='tanh')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# creating a model with random initialization\n",
        "model= GPT2(GPTConfig())\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isYKBS46Q7-y",
        "outputId": "d4f8b56d-f552-4401-9408-aac6aca47871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 124439808\n"
          ]
        }
      ],
      "source": [
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SUTFSaz4Ctn"
      },
      "source": [
        "# Generating Some Text Sequences\n",
        "\n",
        "Loading the weights, biases, and everithing else from HF to our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIJVs6-JJehq"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken\n",
        "\n",
        "# prefix tokens using tiktoken\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10xUaelBQ8Bs",
        "outputId": "3a38c953-8962-475a-88b7-c92aefcc85a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights from pretrained GPT: gpt2\n"
          ]
        }
      ],
      "source": [
        "num_return_sequences= 5\n",
        "max_length= 30\n",
        "\n",
        "model= GPT2.from_pretrained('gpt2').to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqpanmK-3Zik",
        "outputId": "f602fc49-288f-4840-8616-f087cae5040d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Hello, I'm a language model, not a programming platform! I just make decisions based on other projects. I try to do that.\"\n",
            "\n",
            "\n",
            "> Hello, I'm a language model, a kind of a \"first class citizen\" of the world and a person that comes from a much more egalitarian\n",
            "> Hello, I'm a language model, and I'm starting to talk about the notion of the syntax, and I'm also working on an extension that\n",
            "> Hello, I'm a language model, because I'm writing real-time. I'm writing all languages. And I'm working with languages for me\n",
            "> Hello, I'm a language model, I don't know where to begin but I know there is a big deal going on with our society. What\n"
          ]
        }
      ],
      "source": [
        "# getting a list of integer encoding the following string\n",
        "enc= tiktoken.get_encoding('gpt2')\n",
        "tokens= enc.encode(\"Hello, I'm a language model,\")\n",
        "tokens= torch.tensor(tokens, dtype=torch.long) # (8,)\n",
        "tokens= tokens.unsqueeze(0).repeat(num_return_sequences, 1) # (5,8)\n",
        "x= tokens.to(device)\n",
        "\n",
        "# generate! right now x is (B, T -- batch x time) where B= 5, T= 8\n",
        "# set the seed to 42\n",
        "if device== 'cuda':\n",
        "    torch.cuda.manual_seed(42)\n",
        "else:\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "model.eval()\n",
        "while x.size(1) < max_length:\n",
        "    # forward the model to get the logits -- in every iteration we're going to be adding a\n",
        "    # column of new indices (by resampling) into each one of the rows in x\n",
        "    with torch.no_grad():\n",
        "        logits, loss= model(x) # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        logits= logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs= F.softmax(logits, dim=-1)\n",
        "        # do top-k sampling of 50 (HF pipeline default) -- we only want to keep the top 50\n",
        "        # most likely tokens; so, that way we are never sampling very rare tokens\n",
        "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "        topk_probs, topk_indices= torch.topk(probs, 50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        ix= torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol= torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequence\n",
        "        x= torch.cat((x, xcol), dim=1)\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens= x[i, :max_length].tolist()\n",
        "    decoded= enc.decode(tokens)\n",
        "    print(\">\", decoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2Tr_AgYHWib"
      },
      "source": [
        "# Training the Model from Scratch\n",
        "\n",
        "Using a tine (and nice dataset for debugging) to get us off the ground. We are going to encode the dataset using tiktoken for GPT2 in order to transform our data into sequences of integer tokens, process these token sequences and feed them into a Transformer by rearranging these tokens into the **idx** variable that we're going to be feeding into the Transformer. Specifically, we don't want a single very long onedimensional sequence, but an entire batch where each sequence is up to **T** tokens and **T** cannot be larger than the maximum sequence length (i.e., T-long sequences of tokens and B independent examples of sequences)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHh57H-K3Zlg",
        "outputId": "2642c9fa-9d38-4c07-ea0d-2fab4dbdaef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-17 11:28:12--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-11-17 11:28:13 (17.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# read it in to inspect\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text= f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHiWaClxW56L"
      },
      "source": [
        "We can create the data batches by rearranging our dataset using a view operation. Regarding the labels we need for the target to calculate the loss function, we get that as the next token in a sequence (the right of the current one), but notice we don't have a next token for the last one in a sequence, indeed it is the first one of the next sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_WuTVgr3ZoW",
        "outputId": "e266761a-d5e7-4cce-999d-b90e9d6e151a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
            "        [ 5120,   597,  2252,    11,  3285,   502],\n",
            "        [ 2740,    13,   198,   198,  3237,    25],\n",
            "        [  198,  5248,   461,    11,  2740,    13]])\n",
            "tensor([[22307,    25,   198,  8421,   356,  5120],\n",
            "        [  597,  2252,    11,  3285,   502,  2740],\n",
            "        [   13,   198,   198,  3237,    25,   198],\n",
            "        [ 5248,   461,    11,  2740,    13,   198]])\n"
          ]
        }
      ],
      "source": [
        "tokens= enc.encode(text)\n",
        "buf= torch.tensor(tokens[:24 + 1]) # +1 in order to have the last label\n",
        "x= buf[:-1].view(4, 6)\n",
        "y= buf[1:].view(4, 6)\n",
        "\n",
        "print(x) # input tensors\n",
        "print(y) # labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQdZJ7OjuGSI"
      },
      "outputs": [],
      "source": [
        "# making more clean\n",
        "tokens= enc.encode(text)\n",
        "B, T= 4, 32 # batch_size and seq_len\n",
        "buf= torch.tensor(tokens[:B*T + 1]).to(device) # +1 in order to have the last label\n",
        "x= buf[:-1].view(B, T)\n",
        "y= buf[1:].view(B, T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFcYKYs-3Zr-",
        "outputId": "286b022a-47cd-4c9d-ed02-ebe9e3069056"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 32, 50257])\n",
            "tensor(11.0581, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# get logits\n",
        "model= GPT2(GPTConfig()).to(device)\n",
        "model.eval()\n",
        "logits, loss= model(x, y)\n",
        "\n",
        "print(logits.shape)\n",
        "print(loss) # expected loss -ln(1/50257) ~ 10.8249 (cross entropy is the negative log likelihood)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yvlSPyPFvSGJ"
      },
      "outputs": [],
      "source": [
        "class DataLoaderLite:\n",
        "    \"\"\"\n",
        "    Read the data, tokenize it and build batches of sentences.\n",
        "    next_batch function return two tensors on device for training, x and y\n",
        "    y is the target of x, i.e, containing the next token for prediction.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, B, T, data_file, device, process_rank, num_processes) -> None:\n",
        "        self.B= B\n",
        "        self.T= T\n",
        "        self.device= device\n",
        "        self.process_rank= process_rank\n",
        "        self.num_processes= num_processes\n",
        "\n",
        "        # at init load tokens from disk and store them in the memory\n",
        "        with open(data_file, 'r') as f:\n",
        "            text= f.read()\n",
        "        self.encoder= tiktoken.get_encoding('gpt2')\n",
        "        tokens= self.encoder.encode(text)\n",
        "        self.tokens= torch.tensor(tokens)\n",
        "        print(f'Loaded {len(self.tokens)} tokens')\n",
        "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
        "        # state -- we want to stride out all the processes, i.e., process 0 starts at 0,\n",
        "        # but process rank 1 starts at B*T*1, ...\n",
        "        self.current_position= B * T * process_rank\n",
        "\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T= self.B, self.T\n",
        "        buf= self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        x= (buf[:-1]).view(B, T) # inputs\n",
        "        y= (buf[1:]).view(B, T)  # targets\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B * T * self.num_processes\n",
        "        # if loading the next token would be out of bounds, reset\n",
        "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "            self.current_position= B * T * self.process_rank\n",
        "\n",
        "        return x.to(device), y.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iOC7KIcg8UGY"
      },
      "outputs": [],
      "source": [
        "class Cosine_LR_Decay:\n",
        "    \"\"\"\n",
        "    Modulates learning rate (LR) based on the iteration number which LR there should be.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, min_lr, max_lr=3e-4, warmup_steps=10,\n",
        "                 max_steps=50) -> None:\n",
        "        self.optimizer= optimizer\n",
        "        self.min_lr= min_lr\n",
        "        self.max_lr= max_lr\n",
        "        self.warmup_steps= warmup_steps\n",
        "        self.max_steps= max_steps\n",
        "        self.last_lr= None\n",
        "\n",
        "\n",
        "    def get_last_lr(self):\n",
        "        return self.last_lr\n",
        "\n",
        "\n",
        "    def get_lr(self, it):\n",
        "        # 1) linear warmup for warmup_iters steps\n",
        "        if it< self.warmup_steps:\n",
        "            return self.max_lr * (it+1) / self.warmup_steps\n",
        "        # 2) if it > lr_decay_iters, return min learning rate\n",
        "        if it> self.max_steps:\n",
        "            return self.min_lr\n",
        "        # 3) in between, use cosine decay down to min learning rate\n",
        "        decay_ratio= (it - self.warmup_steps) / (self.max_steps - self.warmup_steps)\n",
        "        assert 0 <= decay_ratio <= 1\n",
        "        # coeff starts at 1 and goes to 0\n",
        "        coeff= 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "\n",
        "        return self.min_lr + coeff * (self.max_lr - self.min_lr)\n",
        "\n",
        "\n",
        "    def step(self, it):\n",
        "        self.last_lr= self.get_lr(it)\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr']= self.last_lr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient accumulation:** simulate in a serial way any arbitrary batch size that we set and so we can do a very large batch size running longer and processing multiple sequences and adding up all the gradients from them to simulate a large batch size.\n",
        "\n",
        "**DDP:** in a forward pass it behaves identically, nothing should be changedd in the forward pass, but in the backward pass, once the backward passes over on each independent GPU, each of them has the gradient for all the parameters and what DDP does is once the backward pass is over it will call the \"all reduce\" and it does an average across all the ranks of their gradients. Then, it will deposit that average on every single, which ends up with with the average on it. So, DDP synchronizes and averages the gradients."
      ],
      "metadata": {
        "id": "pgnFF2pfK96Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "s91bvrCx3Zu9"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def self_supervised_training(model, train_loader, optimizer, scheduler, steps,\n",
        "                             grad_accum_steps, ddp, ddp_rank, master_process,\n",
        "                             use_compile, eval_interval=200):\n",
        "    tr_loss_hist= []\n",
        "\n",
        "    # --- training loop ---\n",
        "    for step in range(steps):\n",
        "        start= time.time()\n",
        "        last_step= (step== steps-1)\n",
        "\n",
        "        # --- once in a while generate from the model (except step 0, which is noise)\n",
        "        if ((step> 0 and step % eval_interval==0) or last_step) and (not use_compile):\n",
        "            model.eval()\n",
        "            enc= train_loader.encoder\n",
        "            num_return_sequences= 4\n",
        "            max_length= 32\n",
        "            tokens= enc.encode(\"Hello, I'm a language model,\")\n",
        "            tokens= torch.tensor(tokens, dtype=torch.long)\n",
        "            tokens= tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "            xgen= tokens.to(device)\n",
        "            sample_rng= torch.Generator(device=device)\n",
        "            sample_rng.manual_seed(42 + ddp_rank)\n",
        "            # same code as seen before in \"Generating Some Text Sequences\"\n",
        "            while xgen.size(1) < max_length:\n",
        "                # forward the model to get the logits -- in every iteration we're going to be adding a\n",
        "                # column of new indices (by resampling) into each one of the rows in x\n",
        "                with torch.no_grad():\n",
        "                    logits, _= model(xgen) # (B, T, vocab_size)\n",
        "                    # take the logits at the last position\n",
        "                    logits= logits[:, -1, :] # (B, vocab_size)\n",
        "                    # get the probabilities\n",
        "                    probs= F.softmax(logits, dim=-1)\n",
        "                    # do top-k sampling of 50 (HF pipeline default) -- we only want to keep the top 50\n",
        "                    # most likely tokens; so, that way we are never sampling very rare tokens\n",
        "                    # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "                    topk_probs, topk_indices= torch.topk(probs, 50, dim=-1)\n",
        "                    # select a token from the top-k probabilities\n",
        "                    ix= torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
        "                    # gather the corresponding indices\n",
        "                    xcol= torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "                    # append to the sequence\n",
        "                    xgen= torch.cat((xgen, xcol), dim=1)\n",
        "            # print the generated text\n",
        "            for i in range(num_return_sequences):\n",
        "                tokens= xgen[i, :max_length].tolist()\n",
        "                decoded= enc.decode(tokens)\n",
        "                print(f\"Rank {ddp_rank} sample {i}: {decoded}\")\n",
        "\n",
        "\n",
        "        # --- training step ---\n",
        "        model.train(True)\n",
        "        optimizer.zero_grad()\n",
        "        loss_accum= 0.0\n",
        "\n",
        "        # iterating over all batches accumulating gradients\n",
        "        for micro_step in range(grad_accum_steps):\n",
        "            # --- minibatch construction ---\n",
        "            Xmb, Ymb= train_loader.next_batch()\n",
        "\n",
        "            # --- forward pass and get loss ---\n",
        "            # DDP does the backward pass and synchronizes the gradient\n",
        "            if ddp: # backward_grad_sync only turns on (True) when the micro_step is the last\n",
        "                model.require_backward_grad_sync= (micro_step== grad_accum_steps-1)\n",
        "            logits, loss= model(Xmb, Ymb)\n",
        "\n",
        "            # --- gradient pass to calculate the gradients ---\n",
        "            \"\"\" the loss reduction is the mean by default. We need to compensate it by\n",
        "            the number of gradient accumulation steps before accumulating on each backward().\n",
        "            addition of grads corresponds to a SUM in the objective, but instead of a SUM\n",
        "            we want MEAN. Scale the loss here so it comes out right \"\"\"\n",
        "            loss= loss / grad_accum_steps\n",
        "            loss_accum += loss.detach()\n",
        "            loss.backward() # this is a plus equals, i.e., accumulates the grads\n",
        "        if ddp:\n",
        "            \"\"\" reduce the accum loss over all the processes to the average over that loss\n",
        "            the loss_accum tensor exists on all the ranks, when we call all_reduce of AVG it\n",
        "            creates the average of those numbers and deposits that average on all the ranks\"\"\"\n",
        "            dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
        "        \"\"\" calculating the global norm of the parameters to preventing the model from getting\n",
        "        too big shocks in terms of gradient magnitude \"\"\"\n",
        "        norm= torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        # determine and set the learning rate for this interaction\n",
        "        scheduler.step(step)\n",
        "\n",
        "        # --- update the parameters ---\n",
        "        optimizer.step()\n",
        "\n",
        "        # --- evaluation and track stats ---\n",
        "        if Xmb.device.type== 'cuda':\n",
        "            torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "        end= time.time()\n",
        "        dt= end - start # time difference in seconds\n",
        "        tokens_processed= train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
        "        tokens_per_sec= tokens_processed / dt\n",
        "        if master_process:\n",
        "            tr_loss_hist.append(loss_accum.item())\n",
        "            print(f\"Step {step:4d} | loss: {loss_accum.item():.4f} | lr: {scheduler.get_last_lr():.3e} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.1f}\")\n",
        "\n",
        "    return tr_loss_hist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1l2rpRdZ120"
      },
      "source": [
        "TF32 uses the same 10-bit mantissa as the half-precision (FP16) math, shown to have more than sufficient margin for the precision requirements of AI workloads. And TF32 adopts the same 8-bit exponent as FP32 so it can support the same numeric range.\n",
        "\n",
        "Torch.compile(model) will analyze the entire model and look at what operations we like to use and with the benefit of knowing exactly what is going to happen. It materializes all the operations as it goes through and the calculations are dispatched and run in order. Specifically, the Python interpreter does not know what kind of operations are going to happen later, but torch.compile sees your code entirely at the same time, and it is able to know what operations are intended to run, and it will optimize that process. As a result, torch.compile significantly optimizes the round trips between GPU cores and memory, what is called **kernel fusion** (a major way in speed up).\n",
        "\n",
        "**Values optimization:** fixing ugly numbers (odds or not power of two) through the model. We increase a value to the next nice number.\n",
        "- vocab_size: from 50257 (ugly) to 50304 (nice). Additional \"fake\" tokens we are introducing will never be used and have to be driven to zero in probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWeQW2yNbEMj"
      },
      "outputs": [],
      "source": [
        "!pip install triton"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "INIT DDP\n",
        "simple launch:\n",
        "python file_name.py\n",
        "DDP launch for 8 GPUs:\n",
        "torchrun --standalone --nproc_per_node=8 file_name.py\n",
        "\"\"\"\n",
        "\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.distributed as dist\n",
        "\n",
        "# set up DDP (distributed data parallel)\n",
        "# torchrun command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n",
        "ddp= int(os.environ.get('RANK', -1))!= -1\n",
        "print(f\"Is this a ddp run? {ddp}\")\n",
        "if ddp:\n",
        "    # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
        "    assert torch.cuda.is_available(), \"For now we need CUDA for DDP\"\n",
        "    init_process_group(backend='nccl')\n",
        "    # create environ where each of these processes can look up\n",
        "    ddp_rank= int(os.environ['RANK'])\n",
        "    ddp_local_rank= int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size= int(os.environ['WORLD_SIZE'])\n",
        "    device= f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process= ddp_rank== 0 # this process will do logging, checkpoiniting\n",
        "else:\n",
        "    # vanilla non-DDP run\n",
        "    ddp_rank= 0\n",
        "    ddp_local_rank= 0\n",
        "    ddp_world_size= 1\n",
        "    master_process= True\n",
        "    # attempt to autodetect device\n",
        "    device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n"
      ],
      "metadata": {
        "id": "swHTOlxrkkyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4317fc4a-7a77-437b-f5ba-60b42f5124b4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is this a ddp run? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx_PKsJn3ZyT",
        "outputId": "9b4e44d8-0f56-4a7f-b705-587d242f33ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total desired batch size: 32768\n",
            "=> calculated gradient accumulation steps: 2\n",
            "Loaded 338025 tokens\n",
            "1 epoch = 20 batches\n",
            "Num decayed parameter tensors: 50, with 124354560 parameters\n",
            "Num non-decayed parameter tensors: 98, with 121344 parameters\n",
            "Using fused AdamW: True\n",
            "Step    0 | loss: 10.9402 | lr: 6.000e-05 | dt: 1783.87ms | tok/sec: 18369.0\n",
            "Step    1 | loss: 9.6733 | lr: 1.200e-04 | dt: 438.09ms | tok/sec: 74797.1\n",
            "Step    2 | loss: 9.2101 | lr: 1.800e-04 | dt: 438.90ms | tok/sec: 74660.0\n",
            "Step    3 | loss: 9.8826 | lr: 2.400e-04 | dt: 438.54ms | tok/sec: 74720.0\n",
            "Step    4 | loss: 9.2471 | lr: 3.000e-04 | dt: 439.30ms | tok/sec: 74590.6\n",
            "Step    5 | loss: 8.7262 | lr: 3.600e-04 | dt: 440.02ms | tok/sec: 74469.3\n",
            "Step    6 | loss: 8.3843 | lr: 4.200e-04 | dt: 439.99ms | tok/sec: 74474.4\n",
            "Step    7 | loss: 8.1466 | lr: 4.800e-04 | dt: 439.40ms | tok/sec: 74574.8\n",
            "Step    8 | loss: 7.8399 | lr: 5.400e-04 | dt: 439.71ms | tok/sec: 74521.7\n",
            "Step    9 | loss: 7.4582 | lr: 6.000e-04 | dt: 440.18ms | tok/sec: 74442.4\n",
            "Step   10 | loss: 7.0778 | lr: 6.000e-04 | dt: 439.26ms | tok/sec: 74598.8\n",
            "Step   11 | loss: 6.8150 | lr: 6.000e-04 | dt: 438.60ms | tok/sec: 74710.0\n",
            "Step   12 | loss: 6.5931 | lr: 6.000e-04 | dt: 439.29ms | tok/sec: 74593.7\n",
            "Step   13 | loss: 6.6005 | lr: 6.000e-04 | dt: 438.84ms | tok/sec: 74669.3\n",
            "Step   14 | loss: 6.3651 | lr: 5.999e-04 | dt: 439.70ms | tok/sec: 74523.2\n",
            "Step   15 | loss: 6.2606 | lr: 5.999e-04 | dt: 440.26ms | tok/sec: 74427.9\n",
            "Step   16 | loss: 6.2988 | lr: 5.998e-04 | dt: 440.58ms | tok/sec: 74374.3\n",
            "Step   17 | loss: 6.3538 | lr: 5.997e-04 | dt: 442.63ms | tok/sec: 74030.1\n",
            "Step   18 | loss: 6.3016 | lr: 5.996e-04 | dt: 442.71ms | tok/sec: 74016.2\n",
            "Step   19 | loss: 6.2159 | lr: 5.996e-04 | dt: 440.87ms | tok/sec: 74325.1\n",
            "Step   20 | loss: 6.2317 | lr: 5.994e-04 | dt: 440.51ms | tok/sec: 74386.4\n",
            "Step   21 | loss: 6.7740 | lr: 5.993e-04 | dt: 440.39ms | tok/sec: 74407.2\n",
            "Step   22 | loss: 6.1540 | lr: 5.992e-04 | dt: 441.86ms | tok/sec: 74159.9\n",
            "Step   23 | loss: 6.3496 | lr: 5.991e-04 | dt: 441.63ms | tok/sec: 74198.3\n",
            "Step   24 | loss: 6.1819 | lr: 5.989e-04 | dt: 441.11ms | tok/sec: 74284.5\n",
            "Step   25 | loss: 6.0987 | lr: 5.988e-04 | dt: 441.02ms | tok/sec: 74301.3\n",
            "Step   26 | loss: 6.1448 | lr: 5.986e-04 | dt: 440.30ms | tok/sec: 74421.6\n",
            "Step   27 | loss: 6.2011 | lr: 5.984e-04 | dt: 441.01ms | tok/sec: 74302.6\n",
            "Step   28 | loss: 6.0799 | lr: 5.982e-04 | dt: 442.18ms | tok/sec: 74104.7\n",
            "Step   29 | loss: 5.9609 | lr: 5.980e-04 | dt: 442.90ms | tok/sec: 73985.3\n",
            "Step   30 | loss: 6.0666 | lr: 5.978e-04 | dt: 441.71ms | tok/sec: 74183.7\n",
            "Step   31 | loss: 6.0683 | lr: 5.976e-04 | dt: 441.26ms | tok/sec: 74259.7\n",
            "Step   32 | loss: 6.0379 | lr: 5.973e-04 | dt: 440.04ms | tok/sec: 74466.5\n",
            "Step   33 | loss: 6.1967 | lr: 5.971e-04 | dt: 441.46ms | tok/sec: 74227.1\n",
            "Step   34 | loss: 6.0232 | lr: 5.968e-04 | dt: 442.48ms | tok/sec: 74055.0\n",
            "Step   35 | loss: 5.9903 | lr: 5.965e-04 | dt: 442.44ms | tok/sec: 74062.7\n",
            "Step   36 | loss: 6.0166 | lr: 5.963e-04 | dt: 443.61ms | tok/sec: 73867.4\n",
            "Step   37 | loss: 6.1070 | lr: 5.960e-04 | dt: 441.70ms | tok/sec: 74186.3\n",
            "Step   38 | loss: 5.9725 | lr: 5.957e-04 | dt: 440.72ms | tok/sec: 74351.4\n",
            "Step   39 | loss: 5.8926 | lr: 5.953e-04 | dt: 440.67ms | tok/sec: 74360.1\n",
            "Step   40 | loss: 5.9496 | lr: 5.950e-04 | dt: 441.19ms | tok/sec: 74272.5\n",
            "Step   41 | loss: 5.9455 | lr: 5.947e-04 | dt: 442.39ms | tok/sec: 74071.2\n",
            "Step   42 | loss: 5.9672 | lr: 5.943e-04 | dt: 442.23ms | tok/sec: 74097.3\n",
            "Step   43 | loss: 6.1817 | lr: 5.940e-04 | dt: 443.60ms | tok/sec: 73869.2\n",
            "Step   44 | loss: 5.9695 | lr: 5.936e-04 | dt: 443.53ms | tok/sec: 73879.2\n",
            "Step   45 | loss: 5.9634 | lr: 5.932e-04 | dt: 442.47ms | tok/sec: 74056.3\n",
            "Step   46 | loss: 5.9651 | lr: 5.928e-04 | dt: 441.45ms | tok/sec: 74228.6\n",
            "Step   47 | loss: 6.0511 | lr: 5.924e-04 | dt: 441.67ms | tok/sec: 74191.5\n",
            "Step   48 | loss: 5.9319 | lr: 5.920e-04 | dt: 442.60ms | tok/sec: 74034.9\n",
            "Step   49 | loss: 5.8576 | lr: 5.916e-04 | dt: 442.20ms | tok/sec: 74101.5\n",
            "Step   50 | loss: 5.9277 | lr: 5.912e-04 | dt: 444.07ms | tok/sec: 73789.7\n",
            "Step   51 | loss: 5.8947 | lr: 5.907e-04 | dt: 443.57ms | tok/sec: 73874.1\n",
            "Step   52 | loss: 5.8986 | lr: 5.903e-04 | dt: 444.59ms | tok/sec: 73703.5\n",
            "Step   53 | loss: 6.1112 | lr: 5.898e-04 | dt: 443.00ms | tok/sec: 73968.3\n",
            "Step   54 | loss: 5.9208 | lr: 5.893e-04 | dt: 442.34ms | tok/sec: 74079.5\n",
            "Step   55 | loss: 5.9191 | lr: 5.888e-04 | dt: 440.65ms | tok/sec: 74363.7\n",
            "Step   56 | loss: 5.9170 | lr: 5.883e-04 | dt: 441.92ms | tok/sec: 74149.4\n",
            "Step   57 | loss: 5.9908 | lr: 5.878e-04 | dt: 442.66ms | tok/sec: 74025.0\n",
            "Step   58 | loss: 5.8651 | lr: 5.873e-04 | dt: 444.06ms | tok/sec: 73792.1\n",
            "Step   59 | loss: 5.7992 | lr: 5.868e-04 | dt: 444.46ms | tok/sec: 73724.8\n",
            "Step   60 | loss: 5.8545 | lr: 5.862e-04 | dt: 443.48ms | tok/sec: 73889.0\n",
            "Step   61 | loss: 5.8316 | lr: 5.857e-04 | dt: 443.99ms | tok/sec: 73804.1\n",
            "Step   62 | loss: 5.8433 | lr: 5.851e-04 | dt: 443.64ms | tok/sec: 73861.1\n",
            "Step   63 | loss: 6.0437 | lr: 5.846e-04 | dt: 443.03ms | tok/sec: 73963.9\n",
            "Step   64 | loss: 5.8500 | lr: 5.840e-04 | dt: 441.89ms | tok/sec: 74153.5\n",
            "Step   65 | loss: 5.8582 | lr: 5.834e-04 | dt: 441.56ms | tok/sec: 74209.6\n",
            "Step   66 | loss: 5.8674 | lr: 5.828e-04 | dt: 442.43ms | tok/sec: 74064.4\n",
            "Step   67 | loss: 5.9390 | lr: 5.822e-04 | dt: 443.72ms | tok/sec: 73848.6\n",
            "Step   68 | loss: 5.8112 | lr: 5.815e-04 | dt: 444.41ms | tok/sec: 73733.2\n",
            "Step   69 | loss: 5.7381 | lr: 5.809e-04 | dt: 444.04ms | tok/sec: 73795.0\n",
            "Step   70 | loss: 5.7943 | lr: 5.803e-04 | dt: 444.08ms | tok/sec: 73789.2\n",
            "Step   71 | loss: 5.7782 | lr: 5.796e-04 | dt: 444.39ms | tok/sec: 73737.3\n",
            "Step   72 | loss: 5.8014 | lr: 5.789e-04 | dt: 444.44ms | tok/sec: 73729.3\n",
            "Step   73 | loss: 5.9958 | lr: 5.783e-04 | dt: 443.68ms | tok/sec: 73855.7\n",
            "Step   74 | loss: 5.7766 | lr: 5.776e-04 | dt: 443.62ms | tok/sec: 73864.8\n",
            "Step   75 | loss: 5.8115 | lr: 5.769e-04 | dt: 442.68ms | tok/sec: 74021.1\n",
            "Step   76 | loss: 5.8064 | lr: 5.762e-04 | dt: 442.44ms | tok/sec: 74061.3\n",
            "Step   77 | loss: 5.8712 | lr: 5.755e-04 | dt: 442.95ms | tok/sec: 73976.8\n",
            "Step   78 | loss: 5.7654 | lr: 5.747e-04 | dt: 442.44ms | tok/sec: 74062.3\n",
            "Step   79 | loss: 5.6639 | lr: 5.740e-04 | dt: 443.49ms | tok/sec: 73887.0\n",
            "Step   80 | loss: 5.7047 | lr: 5.733e-04 | dt: 444.82ms | tok/sec: 73666.1\n",
            "Step   81 | loss: 5.6796 | lr: 5.725e-04 | dt: 444.23ms | tok/sec: 73763.1\n",
            "Step   82 | loss: 5.6997 | lr: 5.717e-04 | dt: 445.34ms | tok/sec: 73579.8\n",
            "Step   83 | loss: 5.9113 | lr: 5.710e-04 | dt: 443.44ms | tok/sec: 73895.2\n",
            "Step   84 | loss: 5.6705 | lr: 5.702e-04 | dt: 442.66ms | tok/sec: 74025.4\n",
            "Step   85 | loss: 5.6907 | lr: 5.694e-04 | dt: 441.76ms | tok/sec: 74176.8\n",
            "Step   86 | loss: 5.6706 | lr: 5.686e-04 | dt: 443.42ms | tok/sec: 73898.4\n",
            "Step   87 | loss: 5.7597 | lr: 5.678e-04 | dt: 443.28ms | tok/sec: 73921.8\n",
            "Step   88 | loss: 5.6227 | lr: 5.669e-04 | dt: 445.24ms | tok/sec: 73597.1\n",
            "Step   89 | loss: 5.5134 | lr: 5.661e-04 | dt: 444.45ms | tok/sec: 73726.6\n",
            "Step   90 | loss: 5.5565 | lr: 5.653e-04 | dt: 443.45ms | tok/sec: 73894.1\n",
            "Step   91 | loss: 5.5234 | lr: 5.644e-04 | dt: 442.07ms | tok/sec: 74124.6\n",
            "Step   92 | loss: 5.5570 | lr: 5.635e-04 | dt: 442.48ms | tok/sec: 74054.7\n",
            "Step   93 | loss: 5.7655 | lr: 5.627e-04 | dt: 443.24ms | tok/sec: 73927.9\n",
            "Step   94 | loss: 5.5297 | lr: 5.618e-04 | dt: 443.75ms | tok/sec: 73843.4\n",
            "Step   95 | loss: 5.5192 | lr: 5.609e-04 | dt: 445.18ms | tok/sec: 73606.5\n",
            "Step   96 | loss: 5.5275 | lr: 5.600e-04 | dt: 444.53ms | tok/sec: 73713.4\n",
            "Step   97 | loss: 5.6455 | lr: 5.591e-04 | dt: 444.74ms | tok/sec: 73678.6\n",
            "Step   98 | loss: 5.4713 | lr: 5.582e-04 | dt: 442.85ms | tok/sec: 73993.1\n",
            "Step   99 | loss: 5.3875 | lr: 5.572e-04 | dt: 442.94ms | tok/sec: 73978.4\n",
            "Rank 0 sample 0: Hello, I'm a language model,\n",
            "\n",
            "For ourEN aUS:\n",
            "F is make myICH we be the lord at be I not theEN\n",
            "Rank 0 sample 1: Hello, I'm a language model, we we good\n",
            "I you\n",
            "Why, I this king.\n",
            "Of you. and we I'll my that we\n",
            "Rank 0 sample 2: Hello, I'm a language model, and\n",
            "Which shall ourUE:\n",
            "\n",
            "Myt the father;\n",
            "\n",
            "\n",
            "MyICH theUC, and so\n",
            "Rank 0 sample 3: Hello, I'm a language model,\n",
            "F aIO:\n",
            "And\n",
            "I his is not what that be a!\n",
            "Both that with I.\n",
            "Step  100 | loss: 5.4210 | lr: 5.563e-04 | dt: 1033.83ms | tok/sec: 31695.6\n",
            "Step  101 | loss: 5.4085 | lr: 5.553e-04 | dt: 441.98ms | tok/sec: 74139.9\n",
            "Step  102 | loss: 5.3951 | lr: 5.544e-04 | dt: 442.89ms | tok/sec: 73986.4\n",
            "Step  103 | loss: 5.6256 | lr: 5.534e-04 | dt: 443.68ms | tok/sec: 73854.7\n",
            "Step  104 | loss: 5.4020 | lr: 5.524e-04 | dt: 445.13ms | tok/sec: 73615.1\n",
            "Step  105 | loss: 5.3958 | lr: 5.514e-04 | dt: 445.00ms | tok/sec: 73635.5\n",
            "Step  106 | loss: 5.4161 | lr: 5.505e-04 | dt: 444.11ms | tok/sec: 73784.2\n",
            "Step  107 | loss: 5.5124 | lr: 5.494e-04 | dt: 445.07ms | tok/sec: 73624.4\n",
            "Step  108 | loss: 5.3449 | lr: 5.484e-04 | dt: 446.40ms | tok/sec: 73404.5\n",
            "Step  109 | loss: 5.2765 | lr: 5.474e-04 | dt: 444.70ms | tok/sec: 73686.3\n",
            "Step  110 | loss: 5.3373 | lr: 5.464e-04 | dt: 445.57ms | tok/sec: 73541.8\n",
            "Step  111 | loss: 5.2933 | lr: 5.453e-04 | dt: 444.24ms | tok/sec: 73761.7\n",
            "Step  112 | loss: 5.3225 | lr: 5.443e-04 | dt: 443.11ms | tok/sec: 73949.9\n",
            "Step  113 | loss: 5.5133 | lr: 5.432e-04 | dt: 443.15ms | tok/sec: 73944.0\n",
            "Step  114 | loss: 5.3404 | lr: 5.422e-04 | dt: 442.25ms | tok/sec: 74093.5\n",
            "Step  115 | loss: 5.3086 | lr: 5.411e-04 | dt: 443.14ms | tok/sec: 73944.9\n",
            "Step  116 | loss: 5.3057 | lr: 5.400e-04 | dt: 443.54ms | tok/sec: 73878.2\n",
            "Step  117 | loss: 5.4274 | lr: 5.389e-04 | dt: 444.81ms | tok/sec: 73667.6\n",
            "Step  118 | loss: 5.2345 | lr: 5.378e-04 | dt: 444.66ms | tok/sec: 73691.7\n",
            "Step  119 | loss: 5.1385 | lr: 5.367e-04 | dt: 444.25ms | tok/sec: 73760.9\n",
            "Step  120 | loss: 5.2804 | lr: 5.356e-04 | dt: 444.47ms | tok/sec: 73723.8\n",
            "Step  121 | loss: 5.2125 | lr: 5.345e-04 | dt: 446.16ms | tok/sec: 73445.3\n",
            "Step  122 | loss: 5.2318 | lr: 5.333e-04 | dt: 445.23ms | tok/sec: 73598.4\n",
            "Step  123 | loss: 5.4547 | lr: 5.322e-04 | dt: 444.98ms | tok/sec: 73638.5\n",
            "Step  124 | loss: 5.2400 | lr: 5.310e-04 | dt: 445.03ms | tok/sec: 73630.8\n",
            "Step  125 | loss: 5.2271 | lr: 5.299e-04 | dt: 445.00ms | tok/sec: 73636.3\n",
            "Step  126 | loss: 5.2031 | lr: 5.287e-04 | dt: 446.56ms | tok/sec: 73379.0\n",
            "Step  127 | loss: 5.3173 | lr: 5.275e-04 | dt: 444.90ms | tok/sec: 73653.1\n",
            "Step  128 | loss: 5.1759 | lr: 5.263e-04 | dt: 444.21ms | tok/sec: 73767.4\n",
            "Step  129 | loss: 5.1034 | lr: 5.252e-04 | dt: 442.86ms | tok/sec: 73992.5\n",
            "Step  130 | loss: 5.1920 | lr: 5.240e-04 | dt: 443.23ms | tok/sec: 73929.8\n",
            "Step  131 | loss: 5.1070 | lr: 5.227e-04 | dt: 443.14ms | tok/sec: 73944.8\n",
            "Step  132 | loss: 5.1022 | lr: 5.215e-04 | dt: 443.49ms | tok/sec: 73887.1\n",
            "Step  133 | loss: 5.3461 | lr: 5.203e-04 | dt: 443.32ms | tok/sec: 73914.8\n",
            "Step  134 | loss: 5.1723 | lr: 5.191e-04 | dt: 443.34ms | tok/sec: 73912.3\n",
            "Step  135 | loss: 5.1612 | lr: 5.178e-04 | dt: 443.84ms | tok/sec: 73828.4\n",
            "Step  136 | loss: 5.1458 | lr: 5.166e-04 | dt: 443.31ms | tok/sec: 73917.0\n",
            "Step  137 | loss: 5.2575 | lr: 5.153e-04 | dt: 444.00ms | tok/sec: 73802.3\n",
            "Step  138 | loss: 5.0777 | lr: 5.141e-04 | dt: 444.61ms | tok/sec: 73700.7\n",
            "Step  139 | loss: 5.0038 | lr: 5.128e-04 | dt: 444.51ms | tok/sec: 73716.4\n",
            "Step  140 | loss: 5.1067 | lr: 5.115e-04 | dt: 444.86ms | tok/sec: 73659.4\n",
            "Step  141 | loss: 5.0475 | lr: 5.102e-04 | dt: 445.30ms | tok/sec: 73586.4\n",
            "Step  142 | loss: 4.9947 | lr: 5.089e-04 | dt: 445.27ms | tok/sec: 73591.5\n",
            "Step  143 | loss: 5.2636 | lr: 5.076e-04 | dt: 444.95ms | tok/sec: 73645.0\n",
            "Step  144 | loss: 5.1134 | lr: 5.063e-04 | dt: 483.50ms | tok/sec: 67773.2\n",
            "Step  145 | loss: 5.0740 | lr: 5.050e-04 | dt: 445.32ms | tok/sec: 73583.1\n",
            "Step  146 | loss: 5.0770 | lr: 5.037e-04 | dt: 445.01ms | tok/sec: 73633.9\n",
            "Step  147 | loss: 5.1758 | lr: 5.024e-04 | dt: 444.79ms | tok/sec: 73670.2\n",
            "Step  148 | loss: 5.0258 | lr: 5.010e-04 | dt: 445.29ms | tok/sec: 73588.4\n",
            "Step  149 | loss: 4.9539 | lr: 4.997e-04 | dt: 445.31ms | tok/sec: 73585.4\n",
            "Step  150 | loss: 5.0166 | lr: 4.983e-04 | dt: 445.46ms | tok/sec: 73560.2\n",
            "Step  151 | loss: 4.9416 | lr: 4.970e-04 | dt: 445.17ms | tok/sec: 73608.0\n",
            "Step  152 | loss: 4.9124 | lr: 4.956e-04 | dt: 446.49ms | tok/sec: 73389.5\n",
            "Step  153 | loss: 5.1846 | lr: 4.943e-04 | dt: 444.48ms | tok/sec: 73722.8\n",
            "Step  154 | loss: 5.0317 | lr: 4.929e-04 | dt: 444.78ms | tok/sec: 73671.8\n",
            "Step  155 | loss: 5.0133 | lr: 4.915e-04 | dt: 447.50ms | tok/sec: 73224.2\n",
            "Step  156 | loss: 5.0233 | lr: 4.901e-04 | dt: 446.36ms | tok/sec: 73410.9\n",
            "Step  157 | loss: 5.1298 | lr: 4.887e-04 | dt: 446.19ms | tok/sec: 73439.8\n",
            "Step  158 | loss: 4.9563 | lr: 4.873e-04 | dt: 444.98ms | tok/sec: 73640.0\n",
            "Step  159 | loss: 4.8564 | lr: 4.859e-04 | dt: 445.92ms | tok/sec: 73484.2\n",
            "Step  160 | loss: 4.9660 | lr: 4.845e-04 | dt: 445.15ms | tok/sec: 73611.0\n",
            "Step  161 | loss: 4.9247 | lr: 4.830e-04 | dt: 446.08ms | tok/sec: 73457.7\n",
            "Step  162 | loss: 4.8606 | lr: 4.816e-04 | dt: 445.39ms | tok/sec: 73571.4\n",
            "Step  163 | loss: 5.0967 | lr: 4.802e-04 | dt: 445.38ms | tok/sec: 73572.6\n",
            "Step  164 | loss: 4.9833 | lr: 4.787e-04 | dt: 444.90ms | tok/sec: 73652.4\n",
            "Step  165 | loss: 4.9643 | lr: 4.773e-04 | dt: 444.77ms | tok/sec: 73674.0\n",
            "Step  166 | loss: 4.9916 | lr: 4.758e-04 | dt: 444.98ms | tok/sec: 73640.0\n",
            "Step  167 | loss: 5.0978 | lr: 4.744e-04 | dt: 444.82ms | tok/sec: 73665.4\n",
            "Step  168 | loss: 4.8797 | lr: 4.729e-04 | dt: 445.37ms | tok/sec: 73575.5\n",
            "Step  169 | loss: 4.7864 | lr: 4.714e-04 | dt: 444.63ms | tok/sec: 73698.0\n",
            "Step  170 | loss: 4.9334 | lr: 4.700e-04 | dt: 445.71ms | tok/sec: 73518.9\n",
            "Step  171 | loss: 4.8766 | lr: 4.685e-04 | dt: 445.06ms | tok/sec: 73626.1\n",
            "Step  172 | loss: 4.8280 | lr: 4.670e-04 | dt: 445.07ms | tok/sec: 73624.0\n",
            "Step  173 | loss: 5.0837 | lr: 4.655e-04 | dt: 445.18ms | tok/sec: 73605.5\n",
            "Step  174 | loss: 4.9479 | lr: 4.640e-04 | dt: 444.98ms | tok/sec: 73639.0\n",
            "Step  175 | loss: 4.9244 | lr: 4.625e-04 | dt: 444.80ms | tok/sec: 73669.4\n",
            "Step  176 | loss: 4.9537 | lr: 4.610e-04 | dt: 444.80ms | tok/sec: 73669.8\n",
            "Step  177 | loss: 5.0592 | lr: 4.595e-04 | dt: 445.02ms | tok/sec: 73632.0\n",
            "Step  178 | loss: 4.8321 | lr: 4.579e-04 | dt: 444.16ms | tok/sec: 73775.9\n",
            "Step  179 | loss: 4.7557 | lr: 4.564e-04 | dt: 445.20ms | tok/sec: 73602.9\n",
            "Step  180 | loss: 4.8858 | lr: 4.549e-04 | dt: 444.46ms | tok/sec: 73725.2\n",
            "Step  181 | loss: 4.8105 | lr: 4.533e-04 | dt: 444.21ms | tok/sec: 73767.1\n",
            "Step  182 | loss: 4.7567 | lr: 4.518e-04 | dt: 444.07ms | tok/sec: 73790.2\n",
            "Step  183 | loss: 5.0549 | lr: 4.503e-04 | dt: 444.77ms | tok/sec: 73673.6\n",
            "Step  184 | loss: 4.9014 | lr: 4.487e-04 | dt: 445.02ms | tok/sec: 73631.9\n",
            "Step  185 | loss: 4.8761 | lr: 4.471e-04 | dt: 445.22ms | tok/sec: 73599.6\n",
            "Step  186 | loss: 4.8938 | lr: 4.456e-04 | dt: 445.33ms | tok/sec: 73581.4\n",
            "Step  187 | loss: 4.9893 | lr: 4.440e-04 | dt: 445.03ms | tok/sec: 73631.3\n",
            "Step  188 | loss: 4.7622 | lr: 4.424e-04 | dt: 445.12ms | tok/sec: 73615.8\n",
            "Step  189 | loss: 4.6785 | lr: 4.409e-04 | dt: 445.08ms | tok/sec: 73622.9\n",
            "Step  190 | loss: 4.8162 | lr: 4.393e-04 | dt: 444.90ms | tok/sec: 73653.2\n",
            "Step  191 | loss: 4.7720 | lr: 4.377e-04 | dt: 445.16ms | tok/sec: 73609.9\n",
            "Step  192 | loss: 4.7178 | lr: 4.361e-04 | dt: 445.11ms | tok/sec: 73617.9\n",
            "Step  193 | loss: 4.9759 | lr: 4.345e-04 | dt: 444.64ms | tok/sec: 73695.7\n",
            "Step  194 | loss: 4.8414 | lr: 4.329e-04 | dt: 445.15ms | tok/sec: 73610.4\n",
            "Step  195 | loss: 4.8329 | lr: 4.313e-04 | dt: 445.51ms | tok/sec: 73551.7\n",
            "Step  196 | loss: 4.8455 | lr: 4.297e-04 | dt: 445.10ms | tok/sec: 73619.7\n",
            "Step  197 | loss: 4.9613 | lr: 4.281e-04 | dt: 444.76ms | tok/sec: 73675.9\n",
            "Step  198 | loss: 4.7344 | lr: 4.265e-04 | dt: 444.32ms | tok/sec: 73748.3\n",
            "Step  199 | loss: 4.6495 | lr: 4.249e-04 | dt: 444.61ms | tok/sec: 73700.7\n",
            "Rank 0 sample 0: Hello, I'm a language model,\n",
            "\n",
            "\n",
            "\n",
            "CUUMET:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "I'll thy true is a sir, the\n",
            "Rank 0 sample 1: Hello, I'm a language model,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CUMGEN' these\n",
            "\n",
            "And in my master;\n",
            "\n",
            "\n",
            "Rank 0 sample 2: Hello, I'm a language model, and I see so by all well all I so yet I a that and\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "As\n",
            "Rank 0 sample 3: Hello, I'm a language model,\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Secondare:\n",
            "\n",
            "\n",
            "Cway:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "O\n",
            "Step  200 | loss: 4.7723 | lr: 4.232e-04 | dt: 568.78ms | tok/sec: 57611.4\n",
            "Step  201 | loss: 4.7115 | lr: 4.216e-04 | dt: 444.81ms | tok/sec: 73668.0\n",
            "Step  202 | loss: 4.6584 | lr: 4.200e-04 | dt: 444.66ms | tok/sec: 73692.8\n",
            "Step  203 | loss: 4.9183 | lr: 4.184e-04 | dt: 443.78ms | tok/sec: 73838.0\n",
            "Step  204 | loss: 4.7949 | lr: 4.167e-04 | dt: 444.50ms | tok/sec: 73719.0\n",
            "Step  205 | loss: 4.7971 | lr: 4.151e-04 | dt: 444.77ms | tok/sec: 73673.9\n",
            "Step  206 | loss: 4.8142 | lr: 4.134e-04 | dt: 444.47ms | tok/sec: 73723.1\n",
            "Step  207 | loss: 4.9211 | lr: 4.118e-04 | dt: 444.79ms | tok/sec: 73670.8\n",
            "Step  208 | loss: 4.6829 | lr: 4.101e-04 | dt: 444.22ms | tok/sec: 73765.7\n",
            "Step  209 | loss: 4.6008 | lr: 4.085e-04 | dt: 444.75ms | tok/sec: 73677.9\n",
            "Step  210 | loss: 4.7141 | lr: 4.068e-04 | dt: 444.61ms | tok/sec: 73700.6\n",
            "Step  211 | loss: 4.6901 | lr: 4.052e-04 | dt: 444.45ms | tok/sec: 73727.8\n",
            "Step  212 | loss: 4.6455 | lr: 4.035e-04 | dt: 444.46ms | tok/sec: 73725.0\n",
            "Step  213 | loss: 4.8961 | lr: 4.018e-04 | dt: 444.07ms | tok/sec: 73790.0\n",
            "Step  214 | loss: 4.7815 | lr: 4.002e-04 | dt: 444.41ms | tok/sec: 73733.2\n",
            "Step  215 | loss: 4.7611 | lr: 3.985e-04 | dt: 444.61ms | tok/sec: 73700.6\n",
            "Step  216 | loss: 4.7547 | lr: 3.968e-04 | dt: 444.51ms | tok/sec: 73717.4\n",
            "Step  217 | loss: 4.8824 | lr: 3.951e-04 | dt: 444.49ms | tok/sec: 73721.0\n",
            "Step  218 | loss: 4.6563 | lr: 3.935e-04 | dt: 444.01ms | tok/sec: 73799.5\n",
            "Step  219 | loss: 4.5691 | lr: 3.918e-04 | dt: 445.06ms | tok/sec: 73626.1\n",
            "Step  220 | loss: 4.7482 | lr: 3.901e-04 | dt: 444.71ms | tok/sec: 73683.5\n",
            "Step  221 | loss: 4.6937 | lr: 3.884e-04 | dt: 443.61ms | tok/sec: 73866.2\n",
            "Step  222 | loss: 4.5861 | lr: 3.867e-04 | dt: 444.67ms | tok/sec: 73690.8\n",
            "Step  223 | loss: 4.8261 | lr: 3.850e-04 | dt: 444.90ms | tok/sec: 73652.1\n",
            "Step  224 | loss: 4.7392 | lr: 3.833e-04 | dt: 444.47ms | tok/sec: 73724.0\n",
            "Step  225 | loss: 4.7179 | lr: 3.816e-04 | dt: 444.23ms | tok/sec: 73763.0\n",
            "Step  226 | loss: 4.7616 | lr: 3.799e-04 | dt: 444.96ms | tok/sec: 73642.2\n",
            "Step  227 | loss: 4.9012 | lr: 3.782e-04 | dt: 445.05ms | tok/sec: 73627.3\n",
            "Step  228 | loss: 4.6454 | lr: 3.765e-04 | dt: 444.78ms | tok/sec: 73673.1\n",
            "Step  229 | loss: 4.5012 | lr: 3.748e-04 | dt: 444.73ms | tok/sec: 73680.5\n",
            "Step  230 | loss: 4.6783 | lr: 3.731e-04 | dt: 444.38ms | tok/sec: 73738.7\n",
            "Step  231 | loss: 4.6323 | lr: 3.714e-04 | dt: 445.24ms | tok/sec: 73596.1\n",
            "Step  232 | loss: 4.5602 | lr: 3.697e-04 | dt: 445.23ms | tok/sec: 73598.5\n",
            "Step  233 | loss: 4.8317 | lr: 3.680e-04 | dt: 444.96ms | tok/sec: 73642.1\n",
            "Step  234 | loss: 4.7146 | lr: 3.662e-04 | dt: 445.39ms | tok/sec: 73571.3\n",
            "Step  235 | loss: 4.6831 | lr: 3.645e-04 | dt: 446.18ms | tok/sec: 73441.0\n",
            "Step  236 | loss: 4.7143 | lr: 3.628e-04 | dt: 446.74ms | tok/sec: 73349.0\n",
            "Step  237 | loss: 4.8369 | lr: 3.611e-04 | dt: 445.58ms | tok/sec: 73539.3\n",
            "Step  238 | loss: 4.5877 | lr: 3.594e-04 | dt: 446.61ms | tok/sec: 73370.3\n",
            "Step  239 | loss: 4.4810 | lr: 3.576e-04 | dt: 445.83ms | tok/sec: 73499.7\n",
            "Step  240 | loss: 4.6615 | lr: 3.559e-04 | dt: 445.76ms | tok/sec: 73511.1\n",
            "Step  241 | loss: 4.5880 | lr: 3.542e-04 | dt: 446.82ms | tok/sec: 73336.2\n",
            "Step  242 | loss: 4.5174 | lr: 3.525e-04 | dt: 446.19ms | tok/sec: 73440.4\n",
            "Step  243 | loss: 4.7936 | lr: 3.508e-04 | dt: 447.84ms | tok/sec: 73168.3\n",
            "Step  244 | loss: 4.6832 | lr: 3.490e-04 | dt: 446.23ms | tok/sec: 73432.4\n",
            "Step  245 | loss: 4.6531 | lr: 3.473e-04 | dt: 444.80ms | tok/sec: 73669.3\n",
            "Step  246 | loss: 4.6749 | lr: 3.456e-04 | dt: 446.17ms | tok/sec: 73443.2\n",
            "Step  247 | loss: 4.7941 | lr: 3.438e-04 | dt: 447.09ms | tok/sec: 73292.5\n",
            "Step  248 | loss: 4.5568 | lr: 3.421e-04 | dt: 446.01ms | tok/sec: 73468.6\n",
            "Step  249 | loss: 4.4539 | lr: 3.404e-04 | dt: 446.12ms | tok/sec: 73451.7\n",
            "Step  250 | loss: 4.6304 | lr: 3.387e-04 | dt: 446.89ms | tok/sec: 73325.3\n",
            "Step  251 | loss: 4.5582 | lr: 3.369e-04 | dt: 445.86ms | tok/sec: 73493.9\n",
            "Step  252 | loss: 4.4813 | lr: 3.352e-04 | dt: 446.81ms | tok/sec: 73337.0\n",
            "Step  253 | loss: 4.7613 | lr: 3.335e-04 | dt: 445.66ms | tok/sec: 73527.6\n",
            "Step  254 | loss: 4.6443 | lr: 3.317e-04 | dt: 446.82ms | tok/sec: 73335.5\n",
            "Step  255 | loss: 4.6144 | lr: 3.300e-04 | dt: 446.06ms | tok/sec: 73461.6\n",
            "Step  256 | loss: 4.6554 | lr: 3.283e-04 | dt: 446.45ms | tok/sec: 73396.6\n",
            "Step  257 | loss: 4.7849 | lr: 3.265e-04 | dt: 447.28ms | tok/sec: 73260.4\n",
            "Step  258 | loss: 4.5398 | lr: 3.248e-04 | dt: 447.03ms | tok/sec: 73301.3\n",
            "Step  259 | loss: 4.4411 | lr: 3.231e-04 | dt: 445.99ms | tok/sec: 73471.7\n",
            "Step  260 | loss: 4.6206 | lr: 3.213e-04 | dt: 445.65ms | tok/sec: 73529.3\n",
            "Step  261 | loss: 4.5393 | lr: 3.196e-04 | dt: 446.43ms | tok/sec: 73399.7\n",
            "Step  262 | loss: 4.4612 | lr: 3.179e-04 | dt: 445.30ms | tok/sec: 73585.9\n",
            "Step  263 | loss: 4.7353 | lr: 3.162e-04 | dt: 445.30ms | tok/sec: 73586.2\n",
            "Step  264 | loss: 4.6399 | lr: 3.144e-04 | dt: 446.29ms | tok/sec: 73422.9\n",
            "Step  265 | loss: 4.6156 | lr: 3.127e-04 | dt: 445.20ms | tok/sec: 73603.4\n",
            "Step  266 | loss: 4.6347 | lr: 3.110e-04 | dt: 445.33ms | tok/sec: 73581.4\n",
            "Step  267 | loss: 4.7402 | lr: 3.092e-04 | dt: 445.29ms | tok/sec: 73587.4\n",
            "Step  268 | loss: 4.4889 | lr: 3.075e-04 | dt: 446.53ms | tok/sec: 73383.6\n",
            "Step  269 | loss: 4.3899 | lr: 3.058e-04 | dt: 445.47ms | tok/sec: 73558.5\n",
            "Step  270 | loss: 4.5805 | lr: 3.041e-04 | dt: 445.18ms | tok/sec: 73605.7\n",
            "Step  271 | loss: 4.5354 | lr: 3.024e-04 | dt: 445.61ms | tok/sec: 73535.9\n",
            "Step  272 | loss: 4.4708 | lr: 3.006e-04 | dt: 445.47ms | tok/sec: 73558.3\n",
            "Step  273 | loss: 4.7093 | lr: 2.989e-04 | dt: 446.06ms | tok/sec: 73460.7\n",
            "Step  274 | loss: 4.5829 | lr: 2.972e-04 | dt: 445.59ms | tok/sec: 73539.3\n",
            "Step  275 | loss: 4.5543 | lr: 2.955e-04 | dt: 445.49ms | tok/sec: 73554.3\n",
            "Step  276 | loss: 4.5903 | lr: 2.938e-04 | dt: 445.84ms | tok/sec: 73497.7\n",
            "Step  277 | loss: 4.7043 | lr: 2.920e-04 | dt: 446.27ms | tok/sec: 73426.6\n",
            "Step  278 | loss: 4.4652 | lr: 2.903e-04 | dt: 446.68ms | tok/sec: 73358.8\n",
            "Step  279 | loss: 4.3728 | lr: 2.886e-04 | dt: 445.60ms | tok/sec: 73536.4\n",
            "Step  280 | loss: 4.5697 | lr: 2.869e-04 | dt: 444.83ms | tok/sec: 73664.0\n",
            "Step  281 | loss: 4.4843 | lr: 2.852e-04 | dt: 445.86ms | tok/sec: 73494.3\n",
            "Step  282 | loss: 4.4103 | lr: 2.835e-04 | dt: 483.72ms | tok/sec: 67741.4\n",
            "Step  283 | loss: 4.6558 | lr: 2.818e-04 | dt: 446.11ms | tok/sec: 73452.9\n",
            "Step  284 | loss: 4.5676 | lr: 2.801e-04 | dt: 445.50ms | tok/sec: 73552.6\n",
            "Step  285 | loss: 4.5474 | lr: 2.784e-04 | dt: 446.02ms | tok/sec: 73468.2\n",
            "Step  286 | loss: 4.5663 | lr: 2.767e-04 | dt: 444.82ms | tok/sec: 73665.6\n",
            "Step  287 | loss: 4.6767 | lr: 2.750e-04 | dt: 444.43ms | tok/sec: 73730.1\n",
            "Step  288 | loss: 4.4538 | lr: 2.733e-04 | dt: 444.47ms | tok/sec: 73724.5\n",
            "Step  289 | loss: 4.3709 | lr: 2.716e-04 | dt: 444.95ms | tok/sec: 73643.9\n",
            "Step  290 | loss: 4.5273 | lr: 2.699e-04 | dt: 445.19ms | tok/sec: 73604.1\n",
            "Step  291 | loss: 4.4547 | lr: 2.682e-04 | dt: 445.16ms | tok/sec: 73610.0\n",
            "Step  292 | loss: 4.3775 | lr: 2.665e-04 | dt: 445.26ms | tok/sec: 73592.6\n",
            "Step  293 | loss: 4.6342 | lr: 2.649e-04 | dt: 445.05ms | tok/sec: 73626.9\n",
            "Step  294 | loss: 4.5495 | lr: 2.632e-04 | dt: 445.54ms | tok/sec: 73547.1\n",
            "Step  295 | loss: 4.5443 | lr: 2.615e-04 | dt: 445.15ms | tok/sec: 73611.4\n",
            "Step  296 | loss: 4.5994 | lr: 2.598e-04 | dt: 445.50ms | tok/sec: 73553.4\n",
            "Step  297 | loss: 4.6928 | lr: 2.582e-04 | dt: 443.90ms | tok/sec: 73818.0\n",
            "Step  298 | loss: 4.4142 | lr: 2.565e-04 | dt: 444.62ms | tok/sec: 73699.1\n",
            "Step  299 | loss: 4.3063 | lr: 2.548e-04 | dt: 445.02ms | tok/sec: 73632.9\n",
            "Rank 0 sample 0: Hello, I'm a language model,\n",
            "\n",
            "But or that for you a which make his soul them then then you be my then so to tell the love\n",
            "Rank 0 sample 1: Hello, I'm a language model, a for have we not that that thou be a all ever been he in your be I never be as to their my\n",
            "Rank 0 sample 2: Hello, I'm a language model, and be well not be we speak you be as well I my my\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "C V\n",
            "Rank 0 sample 3: Hello, I'm a language model,\n",
            "You bring you,\n",
            "And then to meet my lord, he you are true:\n",
            "\n",
            "Qose have as\n",
            "Step  300 | loss: 4.4720 | lr: 2.532e-04 | dt: 568.74ms | tok/sec: 57615.4\n",
            "Step  301 | loss: 4.4564 | lr: 2.515e-04 | dt: 444.81ms | tok/sec: 73666.9\n",
            "Step  302 | loss: 4.4051 | lr: 2.499e-04 | dt: 443.41ms | tok/sec: 73900.5\n",
            "Step  303 | loss: 4.6831 | lr: 2.482e-04 | dt: 444.64ms | tok/sec: 73696.1\n",
            "Step  304 | loss: 4.5758 | lr: 2.466e-04 | dt: 444.97ms | tok/sec: 73641.1\n",
            "Step  305 | loss: 4.5107 | lr: 2.449e-04 | dt: 444.31ms | tok/sec: 73750.8\n",
            "Step  306 | loss: 4.5315 | lr: 2.433e-04 | dt: 445.10ms | tok/sec: 73620.0\n",
            "Step  307 | loss: 4.6490 | lr: 2.416e-04 | dt: 444.48ms | tok/sec: 73722.1\n",
            "Step  308 | loss: 4.3996 | lr: 2.400e-04 | dt: 444.95ms | tok/sec: 73644.8\n",
            "Step  309 | loss: 4.3186 | lr: 2.384e-04 | dt: 444.56ms | tok/sec: 73709.5\n",
            "Step  310 | loss: 4.4766 | lr: 2.368e-04 | dt: 445.22ms | tok/sec: 73599.4\n",
            "Step  311 | loss: 4.4301 | lr: 2.351e-04 | dt: 444.83ms | tok/sec: 73664.3\n",
            "Step  312 | loss: 4.3597 | lr: 2.335e-04 | dt: 444.43ms | tok/sec: 73731.2\n",
            "Step  313 | loss: 4.6183 | lr: 2.319e-04 | dt: 444.65ms | tok/sec: 73693.5\n",
            "Step  314 | loss: 4.5355 | lr: 2.303e-04 | dt: 444.64ms | tok/sec: 73695.2\n",
            "Step  315 | loss: 4.4825 | lr: 2.287e-04 | dt: 444.55ms | tok/sec: 73711.1\n",
            "Step  316 | loss: 4.5155 | lr: 2.271e-04 | dt: 444.99ms | tok/sec: 73637.7\n",
            "Step  317 | loss: 4.6401 | lr: 2.255e-04 | dt: 444.91ms | tok/sec: 73651.3\n",
            "Step  318 | loss: 4.3817 | lr: 2.239e-04 | dt: 445.05ms | tok/sec: 73628.2\n",
            "Step  319 | loss: 4.2974 | lr: 2.223e-04 | dt: 444.77ms | tok/sec: 73674.5\n",
            "Step  320 | loss: 4.4522 | lr: 2.207e-04 | dt: 444.73ms | tok/sec: 73680.8\n",
            "Step  321 | loss: 4.3924 | lr: 2.191e-04 | dt: 446.01ms | tok/sec: 73469.9\n",
            "Step  322 | loss: 4.3262 | lr: 2.176e-04 | dt: 447.74ms | tok/sec: 73184.6\n",
            "Step  323 | loss: 4.6046 | lr: 2.160e-04 | dt: 446.76ms | tok/sec: 73346.5\n",
            "Step  324 | loss: 4.5344 | lr: 2.144e-04 | dt: 446.87ms | tok/sec: 73328.6\n",
            "Step  325 | loss: 4.4909 | lr: 2.129e-04 | dt: 446.86ms | tok/sec: 73328.8\n",
            "Step  326 | loss: 4.5177 | lr: 2.113e-04 | dt: 446.30ms | tok/sec: 73421.4\n",
            "Step  327 | loss: 4.6102 | lr: 2.097e-04 | dt: 446.00ms | tok/sec: 73470.6\n",
            "Step  328 | loss: 4.3431 | lr: 2.082e-04 | dt: 444.52ms | tok/sec: 73715.1\n",
            "Step  329 | loss: 4.2718 | lr: 2.067e-04 | dt: 445.02ms | tok/sec: 73632.7\n",
            "Step  330 | loss: 4.4343 | lr: 2.051e-04 | dt: 444.46ms | tok/sec: 73725.4\n",
            "Step  331 | loss: 4.4046 | lr: 2.036e-04 | dt: 444.97ms | tok/sec: 73641.3\n",
            "Step  332 | loss: 4.3270 | lr: 2.021e-04 | dt: 445.07ms | tok/sec: 73624.0\n",
            "Step  333 | loss: 4.5754 | lr: 2.005e-04 | dt: 444.85ms | tok/sec: 73661.4\n",
            "Step  334 | loss: 4.4913 | lr: 1.990e-04 | dt: 444.83ms | tok/sec: 73664.6\n",
            "Step  335 | loss: 4.4424 | lr: 1.975e-04 | dt: 445.60ms | tok/sec: 73537.2\n",
            "Step  336 | loss: 4.4791 | lr: 1.960e-04 | dt: 445.40ms | tok/sec: 73570.6\n",
            "Step  337 | loss: 4.5897 | lr: 1.945e-04 | dt: 444.96ms | tok/sec: 73643.2\n",
            "Step  338 | loss: 4.3444 | lr: 1.930e-04 | dt: 444.75ms | tok/sec: 73678.0\n",
            "Step  339 | loss: 4.2588 | lr: 1.915e-04 | dt: 444.74ms | tok/sec: 73678.3\n",
            "Step  340 | loss: 4.3984 | lr: 1.900e-04 | dt: 444.31ms | tok/sec: 73750.2\n",
            "Step  341 | loss: 4.3634 | lr: 1.886e-04 | dt: 444.83ms | tok/sec: 73663.5\n",
            "Step  342 | loss: 4.2927 | lr: 1.871e-04 | dt: 444.62ms | tok/sec: 73698.2\n",
            "Step  343 | loss: 4.5298 | lr: 1.856e-04 | dt: 444.73ms | tok/sec: 73680.2\n",
            "Step  344 | loss: 4.4474 | lr: 1.842e-04 | dt: 444.54ms | tok/sec: 73711.6\n",
            "Step  345 | loss: 4.4046 | lr: 1.827e-04 | dt: 445.57ms | tok/sec: 73542.1\n",
            "Step  346 | loss: 4.4591 | lr: 1.813e-04 | dt: 444.20ms | tok/sec: 73768.5\n",
            "Step  347 | loss: 4.5828 | lr: 1.798e-04 | dt: 444.59ms | tok/sec: 73704.4\n",
            "Step  348 | loss: 4.3378 | lr: 1.784e-04 | dt: 444.93ms | tok/sec: 73647.3\n",
            "Step  349 | loss: 4.2464 | lr: 1.770e-04 | dt: 445.03ms | tok/sec: 73631.7\n",
            "Step  350 | loss: 4.3770 | lr: 1.755e-04 | dt: 445.48ms | tok/sec: 73557.4\n",
            "Step  351 | loss: 4.3321 | lr: 1.741e-04 | dt: 444.34ms | tok/sec: 73746.1\n",
            "Step  352 | loss: 4.2804 | lr: 1.727e-04 | dt: 444.04ms | tok/sec: 73794.6\n",
            "Step  353 | loss: 4.5249 | lr: 1.713e-04 | dt: 446.09ms | tok/sec: 73456.5\n",
            "Step  354 | loss: 4.4468 | lr: 1.699e-04 | dt: 444.25ms | tok/sec: 73760.6\n",
            "Step  355 | loss: 4.3922 | lr: 1.685e-04 | dt: 445.08ms | tok/sec: 73623.3\n",
            "Step  356 | loss: 4.4283 | lr: 1.671e-04 | dt: 444.72ms | tok/sec: 73683.1\n",
            "Step  357 | loss: 4.5472 | lr: 1.657e-04 | dt: 444.99ms | tok/sec: 73637.9\n",
            "Step  358 | loss: 4.2960 | lr: 1.644e-04 | dt: 443.97ms | tok/sec: 73807.3\n",
            "Step  359 | loss: 4.2232 | lr: 1.630e-04 | dt: 444.92ms | tok/sec: 73649.5\n",
            "Step  360 | loss: 4.3400 | lr: 1.617e-04 | dt: 444.46ms | tok/sec: 73724.7\n",
            "Step  361 | loss: 4.3075 | lr: 1.603e-04 | dt: 444.75ms | tok/sec: 73678.2\n",
            "Step  362 | loss: 4.2537 | lr: 1.590e-04 | dt: 444.81ms | tok/sec: 73667.1\n",
            "Step  363 | loss: 4.4959 | lr: 1.576e-04 | dt: 444.78ms | tok/sec: 73672.6\n",
            "Step  364 | loss: 4.4083 | lr: 1.563e-04 | dt: 445.55ms | tok/sec: 73544.5\n",
            "Step  365 | loss: 4.3567 | lr: 1.550e-04 | dt: 444.30ms | tok/sec: 73752.1\n",
            "Step  366 | loss: 4.3978 | lr: 1.537e-04 | dt: 443.84ms | tok/sec: 73828.2\n",
            "Step  367 | loss: 4.5360 | lr: 1.524e-04 | dt: 444.96ms | tok/sec: 73642.0\n",
            "Step  368 | loss: 4.2669 | lr: 1.511e-04 | dt: 445.27ms | tok/sec: 73591.9\n",
            "Step  369 | loss: 4.1704 | lr: 1.498e-04 | dt: 444.73ms | tok/sec: 73681.3\n",
            "Step  370 | loss: 4.3075 | lr: 1.485e-04 | dt: 444.88ms | tok/sec: 73655.5\n",
            "Step  371 | loss: 4.2777 | lr: 1.472e-04 | dt: 444.77ms | tok/sec: 73674.8\n",
            "Step  372 | loss: 4.2201 | lr: 1.459e-04 | dt: 445.53ms | tok/sec: 73548.2\n",
            "Step  373 | loss: 4.4605 | lr: 1.447e-04 | dt: 445.24ms | tok/sec: 73595.7\n",
            "Step  374 | loss: 4.3655 | lr: 1.434e-04 | dt: 445.05ms | tok/sec: 73627.4\n",
            "Step  375 | loss: 4.3248 | lr: 1.422e-04 | dt: 445.12ms | tok/sec: 73616.0\n",
            "Step  376 | loss: 4.3599 | lr: 1.409e-04 | dt: 444.62ms | tok/sec: 73699.0\n",
            "Step  377 | loss: 4.4868 | lr: 1.397e-04 | dt: 444.57ms | tok/sec: 73707.0\n",
            "Step  378 | loss: 4.2388 | lr: 1.385e-04 | dt: 444.87ms | tok/sec: 73658.2\n",
            "Step  379 | loss: 4.1460 | lr: 1.373e-04 | dt: 444.83ms | tok/sec: 73663.6\n",
            "Step  380 | loss: 4.2869 | lr: 1.360e-04 | dt: 444.85ms | tok/sec: 73660.2\n",
            "Step  381 | loss: 4.2390 | lr: 1.348e-04 | dt: 445.24ms | tok/sec: 73595.7\n",
            "Step  382 | loss: 4.1838 | lr: 1.337e-04 | dt: 445.09ms | tok/sec: 73621.8\n",
            "Step  383 | loss: 4.4289 | lr: 1.325e-04 | dt: 445.53ms | tok/sec: 73548.5\n",
            "Step  384 | loss: 4.3534 | lr: 1.313e-04 | dt: 444.51ms | tok/sec: 73717.1\n",
            "Step  385 | loss: 4.3046 | lr: 1.301e-04 | dt: 445.08ms | tok/sec: 73623.1\n",
            "Step  386 | loss: 4.3374 | lr: 1.290e-04 | dt: 444.27ms | tok/sec: 73757.6\n",
            "Step  387 | loss: 4.4493 | lr: 1.278e-04 | dt: 444.09ms | tok/sec: 73786.0\n",
            "Step  388 | loss: 4.2082 | lr: 1.267e-04 | dt: 445.61ms | tok/sec: 73534.5\n",
            "Step  389 | loss: 4.1209 | lr: 1.255e-04 | dt: 446.46ms | tok/sec: 73395.7\n",
            "Step  390 | loss: 4.2580 | lr: 1.244e-04 | dt: 446.04ms | tok/sec: 73464.0\n",
            "Step  391 | loss: 4.2227 | lr: 1.233e-04 | dt: 447.20ms | tok/sec: 73273.0\n",
            "Step  392 | loss: 4.1575 | lr: 1.222e-04 | dt: 445.88ms | tok/sec: 73491.0\n",
            "Step  393 | loss: 4.3932 | lr: 1.211e-04 | dt: 446.28ms | tok/sec: 73424.1\n",
            "Step  394 | loss: 4.3320 | lr: 1.200e-04 | dt: 445.66ms | tok/sec: 73527.5\n",
            "Step  395 | loss: 4.2891 | lr: 1.189e-04 | dt: 446.13ms | tok/sec: 73449.1\n",
            "Step  396 | loss: 4.3242 | lr: 1.178e-04 | dt: 447.19ms | tok/sec: 73275.2\n",
            "Step  397 | loss: 4.4392 | lr: 1.168e-04 | dt: 446.44ms | tok/sec: 73398.9\n",
            "Step  398 | loss: 4.1900 | lr: 1.157e-04 | dt: 447.32ms | tok/sec: 73254.3\n",
            "Step  399 | loss: 4.0931 | lr: 1.147e-04 | dt: 446.06ms | tok/sec: 73461.3\n",
            "Rank 0 sample 0: Hello, I'm a language model,\n",
            "\n",
            "O::\n",
            "But let and whom to help my will take this\n",
            "That do all, if I,\n",
            "Rank 0 sample 1: Hello, I'm a language model,\n",
            "In let theost\n",
            "The lie\n",
            "To break\n",
            "And if then I do, with all as that what we\n",
            "Rank 0 sample 2: Hello, I'm a language model,\n",
            "\n",
            "Which this news as but do your daughter me a brother\n",
            "\n",
            "\n",
            "\n",
            "And,\n",
            "\n",
            "\n",
            "And will\n",
            "Rank 0 sample 3: Hello, I'm a language model,\n",
            "In now he shall be the noble and every good\n",
            "\n",
            "My goodine dert iner thy fortune this no\n",
            "Step  400 | loss: 4.2323 | lr: 1.136e-04 | dt: 570.53ms | tok/sec: 57434.3\n",
            "Step  401 | loss: 4.2168 | lr: 1.126e-04 | dt: 445.07ms | tok/sec: 73623.9\n",
            "Step  402 | loss: 4.1619 | lr: 1.116e-04 | dt: 445.28ms | tok/sec: 73589.1\n",
            "Step  403 | loss: 4.3995 | lr: 1.106e-04 | dt: 445.61ms | tok/sec: 73534.8\n",
            "Step  404 | loss: 4.3202 | lr: 1.095e-04 | dt: 446.25ms | tok/sec: 73429.0\n",
            "Step  405 | loss: 4.2684 | lr: 1.086e-04 | dt: 446.28ms | tok/sec: 73425.1\n",
            "Step  406 | loss: 4.3010 | lr: 1.076e-04 | dt: 445.67ms | tok/sec: 73525.2\n",
            "Step  407 | loss: 4.4189 | lr: 1.066e-04 | dt: 444.77ms | tok/sec: 73673.2\n",
            "Step  408 | loss: 4.1806 | lr: 1.056e-04 | dt: 445.28ms | tok/sec: 73589.5\n",
            "Step  409 | loss: 4.0896 | lr: 1.047e-04 | dt: 446.05ms | tok/sec: 73462.4\n",
            "Step  410 | loss: 4.2355 | lr: 1.037e-04 | dt: 446.05ms | tok/sec: 73462.2\n",
            "Step  411 | loss: 4.2023 | lr: 1.028e-04 | dt: 445.95ms | tok/sec: 73478.9\n",
            "Step  412 | loss: 4.1354 | lr: 1.018e-04 | dt: 445.07ms | tok/sec: 73624.5\n",
            "Step  413 | loss: 4.3684 | lr: 1.009e-04 | dt: 445.66ms | tok/sec: 73527.6\n",
            "Step  414 | loss: 4.2950 | lr: 1.000e-04 | dt: 445.93ms | tok/sec: 73482.4\n",
            "Step  415 | loss: 4.2547 | lr: 9.911e-05 | dt: 446.13ms | tok/sec: 73450.3\n",
            "Step  416 | loss: 4.2959 | lr: 9.822e-05 | dt: 445.48ms | tok/sec: 73556.9\n",
            "Step  417 | loss: 4.4095 | lr: 9.734e-05 | dt: 445.82ms | tok/sec: 73500.9\n",
            "Step  418 | loss: 4.1717 | lr: 9.646e-05 | dt: 444.93ms | tok/sec: 73647.3\n",
            "Step  419 | loss: 4.0718 | lr: 9.560e-05 | dt: 445.53ms | tok/sec: 73548.1\n",
            "Step  420 | loss: 4.2015 | lr: 9.474e-05 | dt: 445.95ms | tok/sec: 73479.7\n",
            "Step  421 | loss: 4.1657 | lr: 9.390e-05 | dt: 445.59ms | tok/sec: 73539.1\n",
            "Step  422 | loss: 4.1092 | lr: 9.306e-05 | dt: 445.24ms | tok/sec: 73596.5\n",
            "Step  423 | loss: 4.3587 | lr: 9.224e-05 | dt: 445.33ms | tok/sec: 73581.4\n",
            "Step  424 | loss: 4.2961 | lr: 9.142e-05 | dt: 445.23ms | tok/sec: 73597.7\n",
            "Step  425 | loss: 4.2494 | lr: 9.062e-05 | dt: 446.72ms | tok/sec: 73353.1\n",
            "Step  426 | loss: 4.2763 | lr: 8.982e-05 | dt: 445.98ms | tok/sec: 73474.8\n",
            "Step  427 | loss: 4.3738 | lr: 8.904e-05 | dt: 444.77ms | tok/sec: 73673.5\n",
            "Step  428 | loss: 4.1345 | lr: 8.826e-05 | dt: 444.84ms | tok/sec: 73662.9\n",
            "Step  429 | loss: 4.0532 | lr: 8.749e-05 | dt: 446.15ms | tok/sec: 73445.9\n",
            "Step  430 | loss: 4.1902 | lr: 8.674e-05 | dt: 445.15ms | tok/sec: 73610.6\n",
            "Step  431 | loss: 4.1705 | lr: 8.599e-05 | dt: 445.71ms | tok/sec: 73518.9\n",
            "Step  432 | loss: 4.1046 | lr: 8.526e-05 | dt: 445.05ms | tok/sec: 73627.1\n",
            "Step  433 | loss: 4.3316 | lr: 8.453e-05 | dt: 445.78ms | tok/sec: 73507.9\n",
            "Step  434 | loss: 4.2661 | lr: 8.381e-05 | dt: 445.50ms | tok/sec: 73553.0\n",
            "Step  435 | loss: 4.2238 | lr: 8.311e-05 | dt: 445.02ms | tok/sec: 73632.4\n",
            "Step  436 | loss: 4.2696 | lr: 8.241e-05 | dt: 444.70ms | tok/sec: 73685.8\n",
            "Step  437 | loss: 4.3827 | lr: 8.173e-05 | dt: 445.98ms | tok/sec: 73474.9\n",
            "Step  438 | loss: 4.1290 | lr: 8.105e-05 | dt: 445.20ms | tok/sec: 73602.2\n",
            "Step  439 | loss: 4.0249 | lr: 8.039e-05 | dt: 444.84ms | tok/sec: 73662.8\n",
            "Step  440 | loss: 4.1665 | lr: 7.973e-05 | dt: 444.98ms | tok/sec: 73640.1\n",
            "Step  441 | loss: 4.1433 | lr: 7.909e-05 | dt: 444.42ms | tok/sec: 73732.4\n",
            "Step  442 | loss: 4.0919 | lr: 7.845e-05 | dt: 445.47ms | tok/sec: 73558.1\n",
            "Step  443 | loss: 4.3300 | lr: 7.783e-05 | dt: 444.78ms | tok/sec: 73671.6\n",
            "Step  444 | loss: 4.2573 | lr: 7.722e-05 | dt: 445.03ms | tok/sec: 73631.4\n",
            "Step  445 | loss: 4.2048 | lr: 7.661e-05 | dt: 444.31ms | tok/sec: 73750.0\n",
            "Step  446 | loss: 4.2338 | lr: 7.602e-05 | dt: 444.55ms | tok/sec: 73710.9\n",
            "Step  447 | loss: 4.3387 | lr: 7.544e-05 | dt: 445.50ms | tok/sec: 73553.8\n",
            "Step  448 | loss: 4.1048 | lr: 7.487e-05 | dt: 444.70ms | tok/sec: 73686.0\n",
            "Step  449 | loss: 4.0222 | lr: 7.431e-05 | dt: 445.16ms | tok/sec: 73609.2\n",
            "Step  450 | loss: 4.1512 | lr: 7.375e-05 | dt: 445.02ms | tok/sec: 73632.4\n",
            "Step  451 | loss: 4.1287 | lr: 7.321e-05 | dt: 444.77ms | tok/sec: 73674.3\n",
            "Step  452 | loss: 4.0634 | lr: 7.269e-05 | dt: 445.29ms | tok/sec: 73588.0\n",
            "Step  453 | loss: 4.2962 | lr: 7.217e-05 | dt: 445.78ms | tok/sec: 73506.9\n",
            "Step  454 | loss: 4.2290 | lr: 7.166e-05 | dt: 445.70ms | tok/sec: 73519.5\n",
            "Step  455 | loss: 4.1885 | lr: 7.116e-05 | dt: 445.00ms | tok/sec: 73635.6\n",
            "Step  456 | loss: 4.2309 | lr: 7.067e-05 | dt: 445.04ms | tok/sec: 73628.7\n",
            "Step  457 | loss: 4.3399 | lr: 7.020e-05 | dt: 444.83ms | tok/sec: 73664.5\n",
            "Step  458 | loss: 4.0975 | lr: 6.973e-05 | dt: 445.27ms | tok/sec: 73590.6\n",
            "Step  459 | loss: 3.9939 | lr: 6.927e-05 | dt: 446.82ms | tok/sec: 73336.4\n",
            "Step  460 | loss: 4.1387 | lr: 6.883e-05 | dt: 447.00ms | tok/sec: 73306.5\n",
            "Step  461 | loss: 4.1138 | lr: 6.840e-05 | dt: 446.95ms | tok/sec: 73314.5\n",
            "Step  462 | loss: 4.0607 | lr: 6.797e-05 | dt: 446.36ms | tok/sec: 73411.3\n",
            "Step  463 | loss: 4.2941 | lr: 6.756e-05 | dt: 445.91ms | tok/sec: 73485.3\n",
            "Step  464 | loss: 4.2210 | lr: 6.716e-05 | dt: 446.31ms | tok/sec: 73420.3\n",
            "Step  465 | loss: 4.1772 | lr: 6.677e-05 | dt: 445.28ms | tok/sec: 73590.3\n",
            "Step  466 | loss: 4.2115 | lr: 6.639e-05 | dt: 445.87ms | tok/sec: 73492.1\n",
            "Step  467 | loss: 4.3183 | lr: 6.602e-05 | dt: 444.78ms | tok/sec: 73672.2\n",
            "Step  468 | loss: 4.0931 | lr: 6.566e-05 | dt: 445.39ms | tok/sec: 73571.2\n",
            "Step  469 | loss: 3.9988 | lr: 6.532e-05 | dt: 445.59ms | tok/sec: 73537.9\n",
            "Step  470 | loss: 4.1332 | lr: 6.498e-05 | dt: 444.75ms | tok/sec: 73677.7\n",
            "Step  471 | loss: 4.0957 | lr: 6.465e-05 | dt: 445.37ms | tok/sec: 73574.2\n",
            "Step  472 | loss: 4.0305 | lr: 6.434e-05 | dt: 444.53ms | tok/sec: 73713.0\n",
            "Step  473 | loss: 4.2695 | lr: 6.404e-05 | dt: 445.90ms | tok/sec: 73487.1\n",
            "Step  474 | loss: 4.2214 | lr: 6.374e-05 | dt: 444.52ms | tok/sec: 73715.4\n",
            "Step  475 | loss: 4.1774 | lr: 6.346e-05 | dt: 445.56ms | tok/sec: 73543.5\n",
            "Step  476 | loss: 4.2113 | lr: 6.319e-05 | dt: 444.15ms | tok/sec: 73777.0\n",
            "Step  477 | loss: 4.3042 | lr: 6.293e-05 | dt: 445.13ms | tok/sec: 73614.1\n",
            "Step  478 | loss: 4.0650 | lr: 6.268e-05 | dt: 444.59ms | tok/sec: 73704.2\n",
            "Step  479 | loss: 3.9684 | lr: 6.244e-05 | dt: 444.59ms | tok/sec: 73704.3\n",
            "Step  480 | loss: 4.1113 | lr: 6.222e-05 | dt: 445.04ms | tok/sec: 73629.7\n",
            "Step  481 | loss: 4.0987 | lr: 6.200e-05 | dt: 445.31ms | tok/sec: 73584.5\n",
            "Step  482 | loss: 4.0442 | lr: 6.180e-05 | dt: 444.84ms | tok/sec: 73662.6\n",
            "Step  483 | loss: 4.2706 | lr: 6.160e-05 | dt: 444.93ms | tok/sec: 73647.5\n",
            "Step  484 | loss: 4.1968 | lr: 6.142e-05 | dt: 444.57ms | tok/sec: 73707.5\n",
            "Step  485 | loss: 4.1524 | lr: 6.125e-05 | dt: 443.69ms | tok/sec: 73853.2\n",
            "Step  486 | loss: 4.1877 | lr: 6.109e-05 | dt: 445.17ms | tok/sec: 73608.4\n",
            "Step  487 | loss: 4.2934 | lr: 6.094e-05 | dt: 444.32ms | tok/sec: 73748.7\n",
            "Step  488 | loss: 4.0667 | lr: 6.080e-05 | dt: 444.73ms | tok/sec: 73681.3\n",
            "Step  489 | loss: 3.9702 | lr: 6.067e-05 | dt: 444.74ms | tok/sec: 73678.4\n",
            "Step  490 | loss: 4.1080 | lr: 6.055e-05 | dt: 444.34ms | tok/sec: 73744.6\n",
            "Step  491 | loss: 4.0760 | lr: 6.045e-05 | dt: 444.36ms | tok/sec: 73741.9\n",
            "Step  492 | loss: 4.0116 | lr: 6.036e-05 | dt: 445.24ms | tok/sec: 73595.6\n",
            "Step  493 | loss: 4.2459 | lr: 6.027e-05 | dt: 444.59ms | tok/sec: 73703.8\n",
            "Step  494 | loss: 4.1895 | lr: 6.020e-05 | dt: 444.72ms | tok/sec: 73682.2\n",
            "Step  495 | loss: 4.1482 | lr: 6.014e-05 | dt: 445.06ms | tok/sec: 73625.8\n",
            "Step  496 | loss: 4.1822 | lr: 6.009e-05 | dt: 445.36ms | tok/sec: 73577.2\n",
            "Step  497 | loss: 4.2701 | lr: 6.005e-05 | dt: 445.04ms | tok/sec: 73628.5\n",
            "Step  498 | loss: 4.0423 | lr: 6.002e-05 | dt: 445.25ms | tok/sec: 73594.6\n",
            "Rank 0 sample 0: Hello, I'm a language model,\n",
            "\n",
            "'er he were they say 'fore RICHIZAB that lord.\n",
            "R nobleot in my that\n",
            "Rank 0 sample 1: Hello, I'm a language model,\n",
            "With will be so will not;\n",
            "To know't your sovereign,\n",
            "And to-\n",
            "In this made it\n",
            "Rank 0 sample 2: Hello, I'm a language model,\n",
            "\n",
            "You'\n",
            "IUS:\n",
            "Than you to me this\n",
            "Of her\n",
            "\n",
            "\n",
            "\n",
            "And will\n",
            "Rank 0 sample 3: Hello, I'm a language model,\n",
            "With thy name and be all me so doth be nothing\n",
            "That I warrant but then else! nobleot nor\n",
            "Step  499 | loss: 3.9488 | lr: 6.001e-05 | dt: 568.97ms | tok/sec: 57591.8\n"
          ]
        }
      ],
      "source": [
        "total_batch_size= 16*1024 * 2\n",
        "B= 16    # micro batch size\n",
        "T= 1024  # sequence length\n",
        "\n",
        "assert total_batch_size % (B * T * ddp_world_size)== 0, \"Make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
        "grad_accum_steps= total_batch_size // (B * T * ddp_world_size)\n",
        "if master_process: # only the master_process will print it, otherwise all process will print\n",
        "    print(f\"Total desired batch size: {total_batch_size}\")\n",
        "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\"\"\"\n",
        "We have B x T sequences to forward and backward the Transformer but we are not going to do an\n",
        "update. We're goiing to do many forward and backwards and those gradients are all going to be\n",
        "accumulated on the parameter gradients for a single update once all that is accumulated.\n",
        "So, we have to do grad_accum_steps forward backward and then a single update\n",
        "\"\"\"\n",
        "\n",
        "train_loader= DataLoaderLite(B=B, T=T, data_file='input.txt', device=device,\n",
        "                             process_rank=ddp_rank, num_processes=ddp_world_size)\n",
        "\n",
        "if device== 'cuda': # TF32 computationally more efficient (slightly the same precision of FP32)\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# create model\n",
        "model= GPT2(GPTConfig(vocab_size=50304)).to(device)\n",
        "use_compile= False # torch.compile interferes with Generation. TODO fix\n",
        "if device== 'cuda' and use_compile:\n",
        "    model= torch.compile(model)\n",
        "if ddp:\n",
        "    # wrap the model into the DDP container\n",
        "    model= DDP(model, device_ids=[ddp_local_rank])\n",
        "raw_model= model.module if ddp else model # always contains the raw unwrapped model\n",
        "\n",
        "learning_rate=6e-4\n",
        "steps=500\n",
        "\n",
        "max_lr= learning_rate\n",
        "min_lr= max_lr * 0.1\n",
        "warmup_steps= 10\n",
        "max_steps= steps\n",
        "\n",
        "optimizer= raw_model.configure_optimizers(\n",
        "    weight_decay=0.1, learning_rate=learning_rate, device=device\n",
        ")\n",
        "\n",
        "scheduler= Cosine_LR_Decay(optimizer, min_lr, max_lr, warmup_steps, max_steps)\n",
        "\n",
        "loss_hist= self_supervised_training(model, train_loader, optimizer, scheduler, steps,\n",
        "                         grad_accum_steps, ddp, ddp_rank, master_process,\n",
        "                         use_compile, eval_interval=100)\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "plt.plot(loss_hist, label='Train Loss')\n",
        "plt.title('Losse')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "uLYaVO5-cDIb",
        "outputId": "d82efb88-0858-4297-869c-96f17a492b0f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAHHCAYAAABKudlQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABu7ElEQVR4nO3dd3hUVcIG8HdaJnXSKyShE7oU6SJKExE7Nj5XcdXdFVZY24orCqKCuCprw7pYUbGArIIQUHrvPXQSIL1Nkkmm3u+Pydy5k5kJkEwN7+95eJzcOXPnzEkwL6fKBEEQQERERBSk5P6uABEREVFzMMwQERFRUGOYISIioqDGMENERERBjWGGiIiIghrDDBEREQU1hhkiIiIKagwzREREFNQYZoiIiCioMcwQERFRUGOYISKf+OyzzyCTybBz505/V4WIWhiGGSIiIgpqDDNEREQU1BhmiChg7NmzB2PHjoVGo0FkZCRGjBiBrVu3OpQxGo2YNWsWOnbsiNDQUMTHx2Po0KHIzs4WyxQUFGDSpElo3bo11Go1UlNTccstt+DMmTMO91qxYgWuueYaREREICoqCuPGjcOhQ4d88VGJyIOU/q4AEREAHDp0CNdccw00Gg2eeeYZqFQqfPjhhxg+fDjWrVuHAQMGAABmzpyJOXPm4OGHH0b//v2h1Wqxc+dO7N69G6NGjQIA3HHHHTh06BD+/ve/o02bNigqKkJ2djZyc3PRpk0bAMCXX36JBx54AGPGjMFrr70GnU6HBQsWYOjQodizZ49YjogCn0wQBMHflSCilu+zzz7DpEmTsGPHDvTr18/p+dtuuw3Lly/HkSNH0K5dOwBAfn4+OnfujN69e2PdunUAgKuuugqtW7fGL7/84vJ9KioqEBsbi9dffx1PPfWUyzLV1dVIT0/HhAkT8NFHH4nXCwsL0blzZ9x1110O14kosHGYiYj8zmw2Y9WqVbj11lvFIAMAqampuO+++7Bx40ZotVoAQExMDA4dOoTjx4+7vFdYWBhCQkKwdu1alJeXuyyTnZ2NiooK3HvvvSgpKRH/KBQKDBgwAH/88YfnPyQReQ3DDBH5XXFxMXQ6HTp37uz0XJcuXWCxWJCXlwcAeOmll1BRUYFOnTqhR48eePrpp7F//36xvFqtxmuvvYYVK1YgOTkZw4YNw7x581BQUCCWsQWh66+/HomJiQ5/Vq1ahaKiIi9/YiLyJM6ZIaKgMmzYMJw8eRI///wzVq1ahU8++QRvvfUWPvjgAzz88MMAgGnTpmH8+PFYunQpVq5ciRkzZmDOnDn4/fff0bt3b1gsFgDWeTMpKSlO76FU8n+NRMGEf2OJyO8SExMRHh6OnJwcp+eOHj0KuVyO9PR08VpcXBwmTZqESZMmobq6GsOGDcPMmTPFMAMA7du3x5NPPoknn3wSx48fx1VXXYU33ngDX331Fdq3bw8ASEpKwsiRI73/AYnIqzjMRER+p1AoMHr0aPz8888Oy6cLCwuxaNEiDB06FBqNBgBQWlrq8NrIyEh06NABer0eAKDT6VBXV+dQpn379oiKihLLjBkzBhqNBq+++iqMRqNTfYqLiz358YjIy9gzQ0Q+9d///he//fab0/WZM2ciOzsbQ4cOxWOPPQalUokPP/wQer0e8+bNE8t17doVw4cPR9++fREXF4edO3fihx9+wJQpUwAAx44dw4gRI3DXXXeha9euUCqVWLJkCQoLC3HPPfcAADQaDRYsWID7778fffr0wT333IPExETk5ubi119/xZAhQ/Duu+/6pkGIqNm4NJuIfMK2NNudvLw8FBcXY/r06di0aRMsFgsGDBiAV155BYMGDRLLvfLKK1i2bBmOHTsGvV6PzMxM3H///Xj66aehUqlQWlqKF198EWvWrEFeXh6USiWysrLw5JNPYsKECQ7vuXbtWsydOxdbt26FXq9Hq1atcM0112DKlCno27ev19qCiDyLYYaIiIiCGufMEBERUVBjmCEiIqKgxjBDREREQY1hhoiIiIIawwwREREFNYYZIiIiCmotftM8i8WCCxcuICoqCjKZzN/VISIioksgCAKqqqqQlpYGubzxvpcWH2YuXLjgcKYLERERBY+8vDy0bt260TItPsxERUUBsDaG7WwXTzEajVi1ahVGjx4NlUrl0XuTHdvZN9jOvsO29g22s294q521Wi3S09PF3+ONafFhxja0pNFovBJmwsPDodFo+BfFi9jOvsF29h22tW+wnX3D2+18KVNEOAGYiIiIghrDDBEREQU1hhkiIiIKai1+zgwREbUsZrMZRqPxouWMRiOUSiXq6upgNpt9ULMrU1PbWaVSQaFQeKQODDNERBQUBEFAQUEBKioqLrl8SkoK8vLyuM+YFzWnnWNiYpCSktLs7w/DDBERBQVbkElKSkJ4ePhFfwFaLBZUV1cjMjLyopuuUdM1pZ0FQYBOp0NRUREAIDU1tVl1YJghIqKAZzabxSATHx9/Sa+xWCwwGAwIDQ1lmPGiprZzWFgYAKCoqAhJSUnNGnLy63d3/fr1GD9+PNLS0iCTybB06VKH53/66SeMHj0a8fHxkMlk2Lt3r1/qSURE/mWbIxMeHu7nmpAn2b6flzIHqjF+DTM1NTXo1asX3nvvPbfPDx06FK+99pqPa0ZERIGIc19aFk99P/06zDR27FiMHTvW7fP3338/AODMmTM+qhEREREFmxY3Z0av10Ov14tfa7VaANYurOZ2YzVku5+n70uO2M6+wXb2Hbb15TMajRAEARaLBRaL5ZJeIwiC+N9LfU0waNeuHaZOnYqpU6f6uyoAmtfOFosFgiDAaDQ6zZm5nL8fLS7MzJkzB7NmzXK6vmrVKq+NtWZnZ3vlvuSI7ewbbGffYVtfOqVSiZSUFFRXV8NgMFzWa6uqqrxUq8bFxsY2+vw///lPPPvss5d939WrVyM8PFz8x3pT3HTTTejRowfmzJnT5Hs01JR2NhgMqK2txfr162EymRye0+l0l3yfFhdmpk+fjieeeEL82nbq5ujRoz160GRVnQmlVbXYvnkDbrtxFA8x8yKj0Yjs7GyMGsV29ia2s++wrS9fXV0d8vLyEBkZidDQ0Et6jSAIqKqqQlRUlF/m2pw/f158vHjxYrz44os4cuSIeC0yMhKRkZFiXc1mM5TKi/9a9sTvMqVSiZCQEI/cqzntXFdXh7CwMAwbNszp+3o5Ya3FhRm1Wg21Wu10XaVSefR/Gt9sPIvXV+ZgUJIcd3n43uSap7+H5Brb2XfY1pfObDZDJpNBLpdf8vJf25CH7XW+lpaWJj6OiYmBTCYTr61duxbXXXcdli9fjueffx4HDhzAqlWrkJ6ejieeeAJbt25FTU0NunTpgjlz5mDkyJHivdq0aYNp06Zh2rRpAKyf7+OPP8avv/6KlStXolWrVnjjjTdw8803N1q/xtrlxx9/xAsvvIATJ04gNTUVf//73/Hkk0+Kz7///vt46623kJeXh+joaAwcOBBLliyBXC7HDz/8gFmzZuHEiRMIDw9H79698fPPPyMiIsLpfeRyOWQymcu/C5fzd6PFhRlfkdenT7Pg54oQEV2hBEFArdH99vkWiwW1BjOUBpNHw0yYSuGxnp5nn30W//73v9GuXTvExsYiLy8PN954I1555RWo1Wp88cUXGD9+PHJycpCRkeH2PrNmzcK8efPw+uuv45133sHEiRNx9uxZxMXFXXaddu3ahbvuugszZ87E3Xffjc2bN+Oxxx5DfHw8HnzwQezcuROPP/44vvzySwwePBglJSVYvXo1ACA/Px/33nsv5s2bh9tuuw1VVVXYsGGDOK/GW/waZqqrq3HixAnx69OnT2Pv3r2Ii4tDRkYGysrKkJubiwsXLgAAcnJyAAApKSlISUnxS51tlHLrD7KFYYaIyC9qjWZ0fWGlz9/38EtjEB7imV+fL730EkaNGiV+HRcXh169eolfz549G0uWLMGyZcswZcoUt/d58MEHce+99wIAXn31Vbz99tvYvn07brjhhsuu05tvvokRI0ZgxowZAIBOnTrh8OHDeP311/Hggw8iNzcXERERuOmmmxAVFYX09HS0b98egDXMmEwm3H777cjMzAQA9OjR47LrcLn8us/Mzp070bt3b/Tu3RsA8MQTT6B379544YUXAADLli1D7969MW7cOADAPffcg969e+ODDz7wW51tFAwzRETUTP369XP4urq6Gk899RS6dOmCmJgYREZG4siRI8jNzW30Pj179hQfR0REQKPRiEcFXK4jR45gyJAhDteGDBmC48ePw2w2Y9SoUcjMzES7du1w//334+uvvxYn6/bq1QsjRoxAjx49MGHCBHz88ccoLy9vUj0uh197ZoYPH95o19ODDz6IBx980HcVugxKRX2Y8XM9iIiuVGEqBQ6/NMbt8xaLBVXaKkRpojw+zOQpDeeRPPXUU8jOzsa///1vdOjQAWFhYbjzzjsvuoKr4fwSmUzmteXoUVFR2L17N9auXYtVq1Zh5syZmDlzJnbs2IG4uDhkZ2dj8+bNWLVqFd555x3861//wrZt29C2bVuv1Afwc89MMGPPDBGRf8lkMoSHKBv9ExaiuGiZy/3jzZVRmzZtwoMPPojbbrsNPXr0QEpKis83ju3SpQs2bdrkVK9OnTqJe8EolUqMHDkS8+bNw969e5Gbm4vff/8dgPX7MmTIEMyaNQt79uxBSEgIlixZ4tU6cwJwEylkDDNERORZHTt2xE8//YTx48dDJpNhxowZXuthKS4udjrzMDU1FU8++SSuvvpqzJ49G3fffTe2bNmCd999F++//z4A4JdffsGpU6cwbNgwxMbG4pdffoHFYkHnzp2xbds2rFmzBqNHj0ZSUhK2bduG4uJidOnSxSufwYZhpolsPTNczURERJ7y5ptv4qGHHsLgwYORkJCAf/7zn83aHK8xixYtwqJFixyuzZ49G88//zwWL16MF154AbNnz0ZqaipeeuklcdpHTEwMfvrpJ8ycORN1dXXo2LEjPvnkE3Tr1g05OTlYv3495s+fD61Wi8zMTLzxxhuNHl3kCQwzTSTOmWGYISKii2g4B9TdnNE2bdqIwzU2kydPdvi64bCTq/tUVFQ0Wp+1a9c2+vwdd9yBO+64w+VzQ4cOdXi9xWIRA1eXLl3w22+/NXpvb+CcmSZS1E8mY5ghIiLyL4aZJrLvM8Pj6ImIiPyJYaaJbDsAc2k2ERGRfzHMNBF3ACYiIgoMDDNNpFBwNRMRka95+4wf8i1PfT8ZZprI1jPDv1dERN5n2+HWtm0+tQy272dzT4/n0uwm4j4zRES+o1AoEBMTI543FB4eftGdeC0WCwwGA+rq6jx6nAE5ako7C4IAnU6HoqIixMTEiDsLNxXDTBNxB2AiIt9KSUkBgEs+QFEQBNTW1iIsLMyrRxBc6ZrTzjExMeL3tTkYZpqIm+YREfmWTCZDamoqkpKSYDQaL1reaDRi/fr1GDZsWLOHMci9prazSqVqdo+MDcNME4mb5vm5HkREVxqFQnFJvwQVCgVMJhNCQ0MZZrwoENqZg4hNxKXZREREgYFhpokUDDNEREQBgWGmibiaiYiIKDAwzDQRe2aIiIgCA8NME3HODBERUWBgmGki9swQEREFBoaZJlJyaTYREVFAYJhpItuOzeyZISIi8i+GmSay9cwIkMHCRENEROQ3DDNNZJszAwAmhhkiIiK/YZhpIqUkzJgZZoiIiPyGYaaJpD0zZoFhhoiIyF8YZppIwZ4ZIiKigMAw00QKGefMEBERBQKGmSaSy2Wwdc6wZ4aIiMh/GGaaQTxskmGGiIjIbxhmmoFhhoiIyP8YZpqBYYaIiMj/GGaawbbXDCcAExER+Q/DTDPYe2Z43CQREZG/MMw0g+18JvbMEBER+Q/DTDPYlmazY4aIiMh/GGaawT5nhmmGiIjIX/waZtavX4/x48cjLS0NMpkMS5cudXheEAS88MILSE1NRVhYGEaOHInjx4/7p7IuKOqHmbiaiYiIyH/8GmZqamrQq1cvvPfeey6fnzdvHt5++2188MEH2LZtGyIiIjBmzBjU1dX5uKauKbiaiYiIyO+U/nzzsWPHYuzYsS6fEwQB8+fPx/PPP49bbrkFAPDFF18gOTkZS5cuxT333OPLqrqk5D4zREREfufXMNOY06dPo6CgACNHjhSvRUdHY8CAAdiyZYvbMKPX66HX68WvtVotAMBoNMJoNHq0jrazJvUGz9+b7Gxtyzb2Lraz77CtfYPt7BveaufLuV/AhpmCggIAQHJyssP15ORk8TlX5syZg1mzZjldX7VqFcLDwz1aR121AoAMO3fvQe1p9s54W3Z2tr+rcEVgO/sO29o32M6+4el21ul0l1w2YMNMU02fPh1PPPGE+LVWq0V6ejpGjx4NjUbj0ff6NHcr8mq06NnrKozpnurRe5Od0WhEdnY2Ro0aBZVK5e/qtFhsZ99hW/sG29k3vNXOtpGVSxGwYSYlJQUAUFhYiNRUe1AoLCzEVVdd5fZ1arUaarXa6bpKpfL4D7NKUT9/WibnXxQf8Mb3kJyxnX2Hbe0bbGff8HQ7X869AnafmbZt2yIlJQVr1qwRr2m1Wmzbtg2DBg3yY83seNAkERGR//m1Z6a6uhonTpwQvz59+jT27t2LuLg4ZGRkYNq0aXj55ZfRsWNHtG3bFjNmzEBaWhpuvfVW/1VawhZmqvUmHC3QonNyFGS2WcFERETkE34NMzt37sR1110nfm2b6/LAAw/gs88+wzPPPIOamho8+uijqKiowNChQ/Hbb78hNDTUX1V2YAsz//r5MABg4aSrcV3nJH9WiYiI6Irj1zAzfPhwCIL7IRqZTIaXXnoJL730kg9rdelsYcZmye7zDDNEREQ+FrBzZoKBskGY4cwZIiIi32OYaYaGPTNERETkewwzzaDgZF8iIiK/Y5hphoY9M43N/yEiIiLvYJhphoZzZoiIiMj3GGaaQaHgBGAiIiJ/Y5hpBvbMEBER+R/DTDPIG04AZtcMERGRzzHMNAN7ZoiIiPyPYaYZQpRsPiIiIn/jb+NmCA9ROHzN07OJiIh8j2GmGUJVjmHGYLb4qSZERERXLoaZZghrGGZMDDNERES+xjDTDGENhpn0JrOfakJERHTlYphphoY9M3r2zBAREfkcw0wzNOyZ4TATERGR7zHMNEOYyrH52DNDRETkewwzzcAJwERERP7HMNMMznNmOAGYiIjI1xhmmiG04WomI3tmiIiIfI1hphnCG/bMcNM8IiIin2OYaQanHYBNFggCjzQgIiLyJYaZZmi4mgngkQZERES+xjDTDEqFc/NxeTYREZFvMcx4GCcBExER+RbDjIdxmImIiMi3GGY8TG/kXjNERES+xDDjYeyZISIi8i2GGQ/jnBkiIiLfYpjxMK5mIiIi8i2GGQ+r45wZIiIin2KYaaZx6Y7hpZZhhoiIyKcYZpppdGsBB18YgUHt4gGwZ4aIiMjXGGY8QK1SIKz+BG1OACYiIvIthhkPCa0/p6nOxJ4ZIiIiX2KY8ZBQpbVnptbAMENERORLDDMeolZZw0wdh5mIiIh8KuDDTFVVFaZNm4bMzEyEhYVh8ODB2LFjh7+r5STMFmY4zERERORTAR9mHn74YWRnZ+PLL7/EgQMHMHr0aIwcORLnz5/3d9UciHNmuJqJiIjIpwI6zNTW1uLHH3/EvHnzMGzYMHTo0AEzZ85Ehw4dsGDBAn9Xz0Eoh5mIiIj8QunvCjTGZDLBbDYjNDTU4XpYWBg2btzo8jV6vR56vV78WqvVAgCMRiOMRqNH62e7n9FoRH3HDHR6z7/PlU7azuQ9bGffYVv7BtvZN7zVzpdzP5kgCIJH393DBg8ejJCQECxatAjJycn45ptv8MADD6BDhw7IyclxKj9z5kzMmjXL6fqiRYsQHh7utXpuKJDhh9MK9Iqz4KHO7J0hIiJqDp1Oh/vuuw+VlZXQaDSNlg34MHPy5Ek89NBDWL9+PRQKBfr06YNOnTph165dOHLkiFN5Vz0z6enpKCkpuWhjXC6j0Yjs7GyMGjUKPx8owvQlh3BtpwR8cn8fj77PlU7aziqVyt/VabHYzr7DtvYNtrNveKudtVotEhISLinMBPQwEwC0b98e69atQ01NDbRaLVJTU3H33XejXbt2Lsur1Wqo1Wqn6yqVyms/zCqVChGhIQAAg0ngXxov8eb3kOzYzr7DtvYNtrNveLqdL+deAT0BWCoiIgKpqakoLy/HypUrccstt/i7Sg5Cldam5EGTREREvhXwPTMrV66EIAjo3LkzTpw4gaeffhpZWVmYNGmSv6vmwL6aiWGGiIjIlwK+Z6ayshKTJ09GVlYW/vSnP2Ho0KFYuXJlwHUZigdNmjj5l4iIyJcCvmfmrrvuwl133eXvalyU7Wwm9swQERH5VsD3zAQL2w7AnDNDRETkWwwzHsI5M0RERP7BMOMhavFsJgsCfOseIiKiFoVhxkNsp2YDnARMRETkSwwzHhIqDTM8bJKIiMhnGGY8RKWQQyGXAeAkYCIiIl9imPEg21CTzmDyc02IiIiuHAwzHhSptm7bU6NnzwwREZGvMMx4UGSoNcxU6Y1+rgkREdGVg2HGg6Lqw0x1HYeZiIiIfIVhxoNsw0zVeoYZIiIiX2GY8SCxZ4ZhhoiIyGcYZjzI1jNTxWEmIiIin2GY8aBItQoAe2aIiIh8iWHGgyI5AZiIiMjnGGY8KIoTgImIiHyOYcaDxH1m2DNDRETkMwwzHmSfAMxN84iIiHyFYcaDIrk0m4iIyOcYZjxIwzBDRETkcwwzHiQuzeacGSIiIp9hmPEg+0GTDDNERES+wjDjQbYJwAaTBXqT2c+1ISIiujIwzHhQlFoJucz6uFLHFU1ERES+wDDjQXK5DLHhIQCAMp3Bz7UhIiK6MjDMeFhsRH2YqWaYISIi8gWGGQ+Li2DPDBERkS8xzHhYXP0wU3kNwwwREZEvMMx4mDjMVMMJwERERL7AMONhcRHWjfPKOcxERETkEwwzHhYXoQYAlHKYiYiIyCcYZjxM7JlhmCEiIvIJhhkPE/eZYZghIiLyCYYZD7MtzeacGSIiIt9gmPEw9swQERH5FsOMh2lCrXNm9CYLDCaLn2tDRETU8jHMeFhkqFJ8XFXHvWaIiIi8LaDDjNlsxowZM9C2bVuEhYWhffv2mD17NgRB8HfV3FLIZYhUWwONts7k59oQERG1fMqLF/Gf1157DQsWLMDnn3+Obt26YefOnZg0aRKio6Px+OOP+7t6bmlClajWm6CtZc8MERGRtwV0mNm8eTNuueUWjBs3DgDQpk0bfPPNN9i+fbufa9Y4TZgKFyrrUMWeGSIiIq8L6DAzePBgfPTRRzh27Bg6deqEffv2YePGjXjzzTfdvkav10Ov14tfa7VaAIDRaITR6NmeEtv9Gt43Uq0AAJRV13r8Pa9E7tqZPIvt7Dtsa99gO/uGt9r5cu4nEwJ4AorFYsFzzz2HefPmQaFQwGw245VXXsH06dPdvmbmzJmYNWuW0/VFixYhPDzcm9UVfXRUjkPlctzTzoxByQHbvERERAFLp9PhvvvuQ2VlJTQaTaNlA7pnZvHixfj666+xaNEidOvWDXv37sW0adOQlpaGBx54wOVrpk+fjieeeEL8WqvVIj09HaNHj75oY1wuo9GI7OxsjBo1CiqVSrz+u+4ADpXnI7NjF9w4tI1H3/NK5K6dybPYzr7DtvYNtrNveKudbSMrlyKgw8zTTz+NZ599Fvfccw8AoEePHjh79izmzJnjNsyo1Wqo1Wqn6yqVyms/zA3vHV2/cZ7OaOFfIA/y5veQ7NjOvsO29g22s294up0v514BvTRbp9NBLnesokKhgMUS2JvR2TbO42omIiIi7wvonpnx48fjlVdeQUZGBrp164Y9e/bgzTffxEMPPeTvqjVKE8Z9ZoiIiHwloMPMO++8gxkzZuCxxx5DUVER0tLS8Je//AUvvPCCv6vWqCj2zBAREflMQIeZqKgozJ8/H/Pnz/d3VS6LbZiJ+8wQERF5X0DPmQlWtmGmSvbMEBEReR3DjBfYzmaq1rNnhoiIyNsYZrwgoj7M1BgYZoiIiLyNYcYLbGFGpzf7uSZEREQtH8OMF0SGWMOMwWyBwRTYe+IQEREFO4YZLwivP2gSAHQcaiIiIvIqhhkvUCnkCFFam5aTgImIiLyLYcZLbCuaajhvhoiIyKuaFGby8vJw7tw58evt27dj2rRp+OijjzxWsWAXHmIdauKKJiIiIu9qUpi577778McffwAACgoKMGrUKGzfvh3/+te/8NJLL3m0gsHK3jPDMENERORNTQozBw8eRP/+/QEAixcvRvfu3bF582Z8/fXX+OyzzzxZv6AVwWEmIiIin2hSmDEajVCr1QCA1atX4+abbwYAZGVlIT8/33O1C2LiMBN7ZoiIiLyqSWGmW7du+OCDD7BhwwZkZ2fjhhtuAABcuHAB8fHxHq1gsIrkLsBEREQ+0aQw89prr+HDDz/E8OHDce+996JXr14AgGXLlonDT1e68BAOMxEREfmCsikvGj58OEpKSqDVahEbGytef/TRRxEeHu6xygWzSDWHmYiIiHyhST0ztbW10Ov1YpA5e/Ys5s+fj5ycHCQlJXm0gsGKh00SERH5RpPCzC233IIvvvgCAFBRUYEBAwbgjTfewK233ooFCxZ4tILBKoJLs4mIiHyiSWFm9+7duOaaawAAP/zwA5KTk3H27Fl88cUXePvttz1awWAVIa5m4pwZIiIib2pSmNHpdIiKigIArFq1CrfffjvkcjkGDhyIs2fPerSCwYrDTERERL7RpDDToUMHLF26FHl5eVi5ciVGjx4NACgqKoJGo/FoBYMVh5mIiIh8o0lh5oUXXsBTTz2FNm3aoH///hg0aBAAay9N7969PVrBYGULM9UcZiIiIvKqJi3NvvPOOzF06FDk5+eLe8wAwIgRI3Dbbbd5rHLBzLY0W8dhJiIiIq9qUpgBgJSUFKSkpIinZ7du3Zob5knYN81jmCEiIvKmJg0zWSwWvPTSS4iOjkZmZiYyMzMRExOD2bNnw2KxeLqOQSmSB00SERH5RJN6Zv71r3/h008/xdy5czFkyBAAwMaNGzFz5kzU1dXhlVde8Wglg5HtoMlaoxlmiwCFXObnGhEREbVMTQozn3/+OT755BPxtGwA6NmzJ1q1aoXHHnuMYQb2CcCAdXm2JlTlx9oQERG1XE0aZiorK0NWVpbT9aysLJSVlTW7Ui2BWimHsr43RsehJiIiIq9pUpjp1asX3n33Xafr7777Lnr27NnsSrUEMplMHGqq5iRgIiIir2nSMNO8efMwbtw4rF69WtxjZsuWLcjLy8Py5cs9WsFgFqlWQltn4vJsIiIiL2pSz8y1116LY8eO4bbbbkNFRQUqKipw++2349ChQ/jyyy89XcegZd84j2GGiIjIW5q8z0xaWprTRN99+/bh008/xUcffdTsirUE4VyeTURE5HVN6pmhS8NdgImIiLyPYcaLIkI4zERERORtDDNexJOziYiIvO+y5szcfvvtjT5fUVHRnLq0OBH1w0ycM0NEROQ9lxVmoqOjL/r8n/70p2ZVqCVhzwwREZH3XVaYWbhwobfq0SLZ5szUGNgzQ0RE5C0BP2emTZs2kMlkTn8mT57s76pdFPeZISIi8r4m7zPjKzt27IDZbO/ZOHjwIEaNGoUJEyb4sVaXJibMerhkZa3RzzUhIiJquQI+zCQmJjp8PXfuXLRv3x7XXnutn2p06WLCrWGmQmfwc02IiIharoAPM1IGgwFfffUVnnjiCchkMpdl9Ho99Hq9+LVWqwUAGI1GGI2e7SGx3c/dfSNDrKN45TUGj7/3leRi7UyewXb2Hba1b7CdfcNb7Xw595MJgiB49N29aPHixbjvvvuQm5uLtLQ0l2VmzpyJWbNmOV1ftGgRwsPDvV1FB4W1wKt7lQhTCJjbn5OAiYiILpVOp8N9992HyspKaDSaRssGVZgZM2YMQkJC8L///c9tGVc9M+np6SgpKbloY1wuo9GI7OxsjBo1CiqVyun50hoDBs5dCwA4MnMklIqAn28dkC7WzuQZbGffYVv7BtvZN7zVzlqtFgkJCZcUZoJmmOns2bNYvXo1fvrpp0bLqdVqqNVqp+sqlcprP8zu7p0QpRAf15pliAvlX6bm8Ob3kOzYzr7DtvYNtrNveLqdL+deQdNVsHDhQiQlJWHcuHH+rsolUyrkiKpfns1JwERERN4RFGHGYrFg4cKFeOCBB6BUBk1nEgAgun5FU7mOE9CIiIi8ISjCzOrVq5Gbm4uHHnrI31W5bLHhIQCAylr2zBAREXlDUHRzjB49GkE0T9mBfa8Z9swQERF5Q1D0zASz6DCGGSIiIm9imPEy2zATJwATERF5B8OMl8XWDzOVMcwQERF5BcOMl8VFWHtmymrch5mD5ysx/p2N2Hi8xFfVIiIiajEYZrwsPtK6gV9JtfswM+mzHThwvhL/9+k2X1WLiIioxWCY8bL4SGvPTGm13m2Z4ir3zxEREVHjGGa8LKG+Z6axYSYiIiJqOoYZL7PNmSnXGWEyW/xcGyIiopaHYcbLYsNDIJNZH3NFExERkecxzHiZQi5DXPjFVzQRERFR0zDM+IB9EjDDDBERkacxzPiAbd7Mm9nHYLYE5xlTREREgYphxgdse83sOluOVYcK/FwbIiKiloVhxgcm9s8QH5+vqPVjTYiIiFoehhkfGNwhAQ8ObgMAKOUkYCIiIo9imPERcb8ZhhkiIiKPYpjxkUs5cJKIiIguH8OMj1xqmBEErnYiIiK6HAwzPiKGmQa7ADcMLwYeeUBERHRZGGZ8xF3PTMPwYjAxzBAREV0OhhkfsYWZylrHAyfrjI7hRc8wQ0REdFkYZnwkJkwFABAEoKLWKF7XG80O5RhmiIiILg/DjI8oFXLEhFsDjXR5tlPPTINwQ0RERI1jmPEh2+nZxdV6vPfHCXyzPRd1Jsfw8vnmM9CbGGiIiIguldLfFbiSZMSH41RJDX7YeQ4/7TkPAPjo/r4OZT7fchaxESGYNrKTP6pIREQUdNgz40NZKRoAEIMMACzemedU7reDPIySiIjoUjHM+FCX1Cina6uPFDldM1m4cR4REdGlYpjxIVvPzMWYGWaIiIguGcOMD7VLjBAfPz6iI9LjwlyWM3IXYCIiokvGCcA+pFLI8dToTjicr8Xfrm2PMyU1yCurdSpXUq2HIAiQyWR+qCUREVFwYZjxsSnXdxQf986IwbJ9F5zK1BktqKw1orTGAKVchsx4e4+OIAjQ1pkQXb8JHxER0ZWOw0x+NLpbCpKi1BjfK83puf3nKjHijXW46Z2NDsNO7/1xAle9tArrjxW7vW+lzohf9l9AHTfgIyKiKwDDjB+1ignDtudG4J17ezs99/GGUwCAqjoTcst04vV/rzoGQQCeW3LA7X0nL9qNKYv24M3sY56vNBERUYBhmPEzd/NiNhwvER+fLKp2el7eyHyajSesr120LbeZtSMiIgp8DDMB4sP7+yI1OhR39m3t9NzJ4hoA1vkyNgr5xScHS8u7YuEScCIiagEYZgLEmG4p2DJ9BP56bXun504WW3tmSiUHVALWsDJn+RG8uvzIZb/f//ZdQM9Zq/BHjvOmfURERMGEYSbAtEuwr1wa3jkRgD3MnC+3L+MuqdbjXHktPlx/Ch+tP4VCbR2Ai/fG2Pz9mz2o1pvw8Oc7PVV1IiIivwj4MHP+/Hn83//9H+Lj4xEWFoYePXpg586W+wtYLpfhs0lXY/rYLDwzJgsAsCe3AqPfWodb3tsklquqM2HLqVLx67Ol1knCNQbnFUwGkwUGk+uN+LjbMBERBbuADjPl5eUYMmQIVCoVVqxYgcOHD+ONN95AbGysv6vmVcM7J+Ev17ZHu8QI2KbGHCt0ngS86pD9QMozpdZ5NaXVevFardEMk9mC297fhJFvrrvspdqHL2jx0Gc7cPiCtgmfgoiIyDcCetO81157Denp6Vi4cKF4rW3btn6skW+FqhRIjwsXe10akh5SebY+zJRIwoxFAHadLceh+jCSU1CFXukxl/z+Dy7cjqIqPfafq8TO50c24RMQERF5X0CHmWXLlmHMmDGYMGEC1q1bh1atWuGxxx7DI4884vY1er0eer39F7pWa/1FbjQaYTQaPVo/2/08fV+ptvH2MDNrfBdU601Yf7wE206XO5Q7XVwNo9GIwgrH4xFWHswXH58o1KJrSoTTvBqDwYDKWhNMFgsSItXi9aIqazuWVOvdfsaqOiN+OVCA0V2TER8R0vQP2ghftDOxnX2Jbe0bbGff8FY7X879ZMKlzhj1g9DQUADAE088gQkTJmDHjh2YOnUqPvjgAzzwwAMuXzNz5kzMmjXL6fqiRYsQHh7u1fp6w09n5FiXbx0NfP4qExLDgHM1wOv7HXNo6wgBT/c0Y3OhDN+dUojX49UCSvXWsaoxrSy4McOCGiPw3E776+debcK8/QrUmYEZvc0Ir39q6hZ7mf8MMrms3xfH5dhVIkenaAsmd+UBmURE5Bk6nQ733XcfKisrodFoGi0b0GEmJCQE/fr1w+bNm8Vrjz/+OHbs2IEtW7a4fI2rnpn09HSUlJRctDEul9FoRHZ2NkaNGgWVyjtnJc39LQefbjoLAMiZNQry+kk0H204je2ny/HINW3wf//diYgQBb57pD9ezz6OdcdKXN7rhm7JeOeeXjhRVI2x79jb9O27e+Lx7/YDAL6Y1BeD2sUDADrOWCWWOT57tMt7XkqZ5vJFOxPb2ZfY1r7BdvYNb7WzVqtFQkLCJYWZgB5mSk1NRdeuXR2udenSBT/++KPb16jVaqjVaqfrKpXKaz/M3rz3hKsz8Omms+iXGQu12j6MM/n6TpgMwGS2IDxEgRqDGTe9Zw94N/ZIwYbjJaiqs/eonC7RoVRnRnGNYy/LLwcKxcf5WoPLz6JUKvHN9jycKa3B9LFZLncu9vb/LLzZzmTHdvYdtrVvsJ19w9PtfDn3CugwM2TIEOTk5DhcO3bsGDIzM/1UI9/LStHg9yevRbIm1OXzSoUcfTJixSMMAGDqiI74x6hOMJgsKKnWo85oxvVvrENOYRUGzlnjdI/fj9onEp+q321YZ3AMPEVVevE8qNFdk9GvTdwlf4Y6oxmrjxRiaIcExIR7Z14NERFduQJ6afY//vEPbN26Fa+++ipOnDiBRYsW4aOPPsLkyZP9XTWfapcYiQi1+9x5lWSF0nePDsQ/RnUCAIQo5UiLCUPbhAgkRjn3VtmYJHvNnCqpXxVV5bjb8P/2XRAfn28wyfhiXl+ZgymL9uCxr3df1uuIiIguRUCHmauvvhpLlizBN998g+7du2P27NmYP38+Jk6c6O+qBZQB7ey9JK56TGQyGQbWz4ORmjqiI375+1B8eH9fPD6iIwDgdH2YKa6ucyi7dO958bGtTMMN90xmCw5dqMRKyf43gP3Ay80nS0FERORpAT3MBAA33XQTbrrpJn9XI6AN7ZCAf0/ohU7JkW4PoOzZKlrsXXl6TGd0TdNgWMdEKOQydG8VjW5pGry95jhOFFXjuSUHUKlzXBJ38Lx94zxbmCnXOfbelFQbMO7tjQCA/00Zih6towEA5sCdY05ERC1AwIcZujiZTObytG2pcT1TMX/1MXRN02DydR2cnk+LDkNCpBol1XqxJ8WdM/Vhpkird7guPbRy//kKMczwdG4iIvImhpkrRFpMGNY9cx3CVAqXz8vlMlzbKRE/7j4nXlPIZZg+NgtlNQaM7Z6KEKUcY+avx6mSGgiCgKIqx6Go6T8dEB8flxy/YGHPDBEReRHDzBVEuruvK9d0TBDDzJTrOuDmq9LQKTlKfL7OaIZCLkNVnQm3vrcJBxs5s+logf05aceM2SK4HQojIiJqioCeAEy+NaJLEjonR+HGHil4akxnhyADWM+K6tHKOnS071ylyxO3bWc/HcmvgiAITmUqa40wmS3QmxwPvTSYLFh+IB/lNY7zcIiIiC6GYYZEUaEqrPzHMLw/sa/bMoPbO66KuufqdLx6Ww8o5TK8fW9vLP7LQCjkMlTWGlGgrUNZg3BSVmPA1G/3ot/s1SjS2oep3l97Ao99vRt//nyHZz8UERG1eBxmossysF083l97EgCw6h/DxN6b2/u0Qmj9fJz2iRE4VliNI/lapMWEOby+oLIOvx6wHn654mABHhjcBgDw3Y48AMDu3AoffAoiImpJGGbosgxoF4fB7eORrAl1GIYKlUws7pKqwbHCary+8hhu7J7i8HrpTsW1RvtQEycJExFRUzHM0GVRKxVY9MjARstkpWjwMy7gSL4WR/IdJwl/sO6k+PhsqU58bOaB20RE1EScM0Mel5UadfFCAHLLasTH0sPbA/ggdyIiCkAMM+Rx3dLsR7X3bxuHubf3wIvjuzqVc+iZkQSYWqMZ648VY86KIzCxy4aIiC6Cw0zkcUlRofjkT/0QqlJgaMcEANbellFdk5GiCcXBC1rc+t4mXKiohcFkgUohg85gnz9TVmPAn/67HQDQITESt/ayzrsp1xnw1pojuLNvOvpmxvr+gxERUUBizwx5xciuyWKQAaxHLrSODYdSIUev1tEID1HAIgC5ZTpo60wwmOw9MIWSJdsXKuyP5608jm+25+GOBZt98yGIiCgoMMyQz8lkMnRIigQAHC+sQkm14xlPu86Wi4+lw09HC6p8U0EiIgoqDDPkF53rl3V/v+scdp0pd3hu+2n714WVjuc/ERERNcQ5M+QXnVOsYeb3o0X4/WiRw3M7zpSJjwskQ04yHulEREQusGeG/KLhuU8AxAMoK2uN4rUCNz0zOoMJOoMJp4qrHa4LgoAD5ypRozd5sLZERBTIGGbIL2w9M+LXyVFYMLGP04na0p4ZvdE+Sbi02oBJC3fg+jfWYf+5CvH6ykMFGP/uRjz8+U7vVJyIiAIOh5nIL5I1oXhiVCeYLQLG90pDYqQa0eEqrPrHMOw8U4YerWJw49sbUFlrRG39su1ynf3QypJqPbadtg5H/bT7PHq2jgEAfL0tFwCw5VSpbz8QERH5DcMM+c3jIzo6XWufGIn2idaVTpFqJar1JpyvqIUgAOU6+/DTHsmBlBFq+7lQ8kuYWGO2CPhx9zn0bxOHNgkRzfgEREQUCDjMRAGrU7I11By8oEWtGTBZ7Mu01x0rFh8XV9mXdktHqeqMZlgsAkobLP1evDMPz/ywH9e/sdY7FSciIp9imKGA1SfDusvv0z8exBfHHX9UpWEmXzJJuE4yr6agsg7//HE/+r68GgfOVYrXN9Wf3G3hEVBERC0CwwwFLOmRBUcq3P+oXqioFR8XVdmDTYG2Dt/vOgcA+HC9/bRuGdd4ExG1KAwzFLD6SMKMRuXcjZIWHQrA2jNjO2m7SDLklCs5yDIq1D49TBpl9CYziIgouHECMAWsZE0oHhveHuU1evSVn4EsvTeubpsAg9mMQxe0GNYxEb1nZ0NnMENbZ4JaKUdVnX1/ma1uVjSZLPahqLIaA1Kjw1yWO1VcjZToUISH8K8JEVEg4/+lKaA9c0MWjEYjli8/gxt7pUKlUgEAOiRZ96mJCVehQmfEn/67HfERIQ6vXSuZV1OktffYlNVIlnhXuQ4ze/MqcOt7m9C9lQa//P0aj34mIiLyLA4zUVDrlqYBAOzLq3A6FkEaWqTDT6XVkjBTo8eSPedwy7sbca7cPiz1897zAICD57VeqTcREXkOwwwFtWEdE52u3d0vHVe3iXW4VlRlnVdjtggolYSc0moD/vHdPuw7V4l/r8wRr6uV9r1rzFz2REQU0DjMREHtmo6JmLPiqPh1QmQI/jk2C0azBfNXH8fxwirsPFuOoio9xv5nA8wWwaHHRrpHTYmkx0Yp2bCmtFqPJE0oBEFwWgl1tECLVE0YosNV3vh4RER0CRhmKKh1SY1C91YalFYbMP/uq5AeF464+rkzc27vAZPZgo7Pr4AgAEcLqpxef+iCff8ZuSTAaOvsuw3nV9Zh5aECzF99HJ8/1B/dW0WLrx339kakRYdi8/QR3vqIRER0EQwzFNRkMhl++tsQWAQBoSqF0/NKhRzxESEOvS5Sv+zPFx8XVNr3q5H23uRX1mHGz4cAAHNWHMHXDw8EAKzNsU4wvuDmZG8iIvINzpmhoBeilLsMMjatYsOdriVr1A57zwBAbplO3K9GeqilNORI589IX19Za+/JISIi32LPDLV43dI02JdXIX6dEKnG1w8PhFopxzXz/hCv1xkteHLxPoQo5Q4rnvZJjkKQ7jljMNn3qzlfXovoMBUMJgtClI7/Rvhp9zl0TIpCj9bRnvxYRERUj2GGWrzuafYQ8eiwdphyfQdoQq0Tdn99fCgWbcvF4p15MJoF/LTnvNPrl0iuSYefKiSneJ8r12HrqVLMXXEUCyddjSEdEgAA206V4onF+wAAZ+aO8+wHIyIiABxmoitA91Ya8XGfjBgxyABAt7RovHJbD1zbyXmJtytFWvv8mIpae7A5V16Ll345DIPZgpf+d1i8frqkRnxsMtt7coiIyHMYZqjF65QcJT5OjAp1WWZQ+wSX1xc+eLXD3JiiKj0s9fNmpD0zuWX2Dfekq7elQ04F2jpsOF6MW97b5LCKioiImodhhlq8UJUCz92Yhbv7paN3eozLMoPbxztdUylkGN45Eeufvg5v3tULAGCyCHgz+xjmrDjiMEl4veToBLUkwEjPijpfXov7P92OfXkVePbHAw7vdbqkBuX1Q1h1RjO2niqFkT05RESXhHNm6Irw6LD2jT7fWdJ7Y9M6NhwymQyxESG4vU9rzF1xFEVVerz7xwkAQIjCHlpOSYaT8iVLtR3n1dhXRRVKhqvyK2tx3b/XQhOqxP6ZYzDrf4fxzfZcTBvZEdNGdhLLvffHCaTHhePmXmmX8pGJiK4YAd8zM3PmTMhkMoc/WVlZ/q4WtTByuQy/TbsGn026Gtn/GIbnx3XB/LuvciiTGu04RGWo7zmJUjv+m6CoSo86oxmA47ya40XV4uPoMPu8nX151iEnbZ0JNXoTvtmeCwB45/cTYplDFyrx+socPP7NHlgsArR1RnyzPRcVOsf9c85X1KKkWg8ioitJUPTMdOvWDatXrxa/ViqDotoUZLJSNMhKsU4W7uiip6Zbq2iHZdo2c+7ogXd/P4HY8BBsOVUKwBoq2idGOuw/szbHfhCmSbJfjW1vGwA4IQk8CZH2U8Clw1WFVXV48edDWHW4EJtOlODd+/oAsO5aPGTu7wCA03NudDp6gYiopQr4nhnAGl5SUlLEPwkJridrEnnToHbO82ps13+bNgzfPDpQHK7642gRtp0qRaVkmEl6nEJBZZ0YYsokvSvbTpeKj+Mi1OLjakmYOVuqw6rDhQAcdzDOk0xClg5vERG1dEHRxXH8+HGkpaUhNDQUgwYNwpw5c5CRkeGyrF6vh15v72bXarUAAKPRCKPRs/+Dt93P0/clR4HSzv0y7Eu85TLA1rkSrrTXrU18GHIKq/Dyr0cAADFhrg+grDWaUVpl3WivWDJ/5n/7LoiP6wwm8b4lVfb5NqeK7KEoQq0Qy9TU2UPR2ZIqRIZooDOYHDb6a0ygtPOVgG3tG2xn3/BWO1/O/WSCtI87AK1YsQLV1dXo3Lkz8vPzMWvWLJw/fx4HDx5EVJTzUMDMmTMxa9Ysp+uLFi1CeLjztvZEl2P+QQXOVQN3tLXg21MKhCsFzLnaLD6/Pl+GH884H61wWxszjlbI0DZKwO8X5Kgzy/DPniakRQA/nZZjXYFzJ2mYQsDc/tZ7/35Bhp/PWu87ONmCzYXW8gmhAmb0tpY5Ui7DB0etZR7qZL228Jgcd7a1YGiK/a/50QoZlHIBHezZjIgo4Oh0Otx3332orKyERtP4/7ACPsw0VFFRgczMTLz55pv485//7PS8q56Z9PR0lJSUXLQxLpfRaER2djZGjRoFlcr1v8Cp+QKpnavqjKjWm5EcpcYH608jMz4c43qkiM+fKKrG2Hc2O71u2WOD0CXVGr7Hv7cFRwuq0ComFJFqJRKj1Nh4otTpNQBw6MWRCFHK8Ub2cXyw/jQAID4iBKX1y7hTNGpsePpaANYhp398b13y/dzYznh1RY54n+OzRwMAtLVG9H3VeoTDwRdHoqiqDq8uz8HDQ9ugZ1pkwLRzSxdIP9MtGdvZN7zVzlqtFgkJCZcUZoJimEkqJiYGnTp1wokTJ1w+r1aroVarna6rVCqv/TB7895kFwjtHKdSIa6+Q3DqqM5Oz2elxbh8XYImTKx7q5gwHC2owvkK6/BSTqF10m/P1tE4fEGLm3ulYdm+CzBZBBzIr4bZIqCyzt77Uyo5UqFMZ4RSqYRMJkON0f7vknyt44om23trK+2vLaw24snvD2JvXgXWHivBkVmjxLL+bucrBdvaN9jOvuHpdr6cewXFBGCp6upqnDx5Eqmpqf6uCpETmUyGr/48AFOu6wCl3L6aSLoUu1OK8/AoAEwb2RF7XhiFN+7qhbgI60qmez7aiomfbMNSF2dGAdbDLmsM1qCjrbOPL0tXRcVH2FdFSZdyny2twd76Azilq6sA4KP1J/HBupMAgNxSHe76cAt+P1ro/oMTEflRwIeZp556CuvWrcOZM2ewefNm3HbbbVAoFLj33nv9XTUil4Z2TMBTYzrj2bH2/ZDCQ+zzaLqmuu4ujYtQIypUBZlMhvhIx97F2vp9a+7q1xphKgXu7Z+OUJX1r29x/b420mXgG46XiI+VCnuokq5yOlNiX/0kVaM34dXlRzF3xVEUauvwxOK92H66DA99ttOh3A+7zuHzzWdc3sOVQm0dbn1vE77fmXfJryEiuhQBP8x07tw53HvvvSgtLUViYiKGDh2KrVu3IjHx0g4GJPKXPw9tCwBIiQ512POla5qbMBNu70GR7jEjdVPPNLxyWw8o5TKsP1aC8xW1ePjzHSiorMNVGTEuX1NWY4AgCJDJZA5HMJwsrnZZvtwh8NRgd265UxmT2YKnvreeBj6sUyLMFgvu/3Q7HhveHvcPauPyvq/8egR78yqwN68CE/qluyxDRNQUAR9mvv32W39XgahJZDIZHr6mndP1NvER4uMotRJVeuseMrER9qGohrsN28SGh0BVf4xCfGQIzlfU4mSx9SiFTfWTiNOiQ3Ghsg7hIQroDGYYzQKq9CZoQlUOPTPrj9vPk1LIZeK+N9IenlMlNbC4WCJQIS1TXI0P151CfmUdZvx8yG2YOVNa4/I6EVFzBfwwE1FLo5DL0L2VtXfm3Yl90CY+HN1baRApORahe6tol6+NCbcHnrgI1703T4zujEUPD8Dap4aLw1tl1dYeGemcmbwy+941ZosAbf3GfNKemSP5WpfvIb3P6ZIanK+odVmupFovbuYn3cVYavPJEnHoSVtnxCNf7HTYb4eI6GICvmeGqCX68qEBOF9Ri+6torHyH8OgkssdhqKkYaZLqkYMFbGSAJMY6bxqD7BONh7cwbpLdlxECHSGWny59SwKtXVQyB2POJBu/ldWv0pK2jOz/ECBQ3mT2QKlQu4QeI4XVjtMPpa6Yf4GlFTrsf25EdBK7msb9gKA+z7eBgDokBSJX/fnI/twIbIPF2I8D9QkokvEMEPkB7ERIWIwUSudN9mTThK+Kj0GE/q2hkohc+i96ZUeg+93nXN6rXTlVHykGufKa/HpxtMOZaaN7IjU6FB0TY3GlG9242ypTgwz0l6XhodWVtQakRCpRnmN9ADNKodeF4tFgFwug95kFl+/7lixQ+CxDXuZJWNY+89V4kiB656gpXvOY/+5Sjw/rgsMZgueX3oQI7KSMLaH+1WNOQVVSNGEIjqcS3KJWjqGGaIAFKqyB5zWsWF4qH4ysdTAdnEuX6sJs/+1jnczFJUeG447+rYGYO29OVuqQ/aRIpw8I0eWyjHAtE2IwOkS63yX8hqDNcxIAs/u3AqH8pW1RsRGhDjMz9l1thxGsz24VNQYoQlVoUoScIqr9Circd3DM+27vQCA3hkxyCvX4Ydd5/DDrnM4M3ecWMZkti5Tjw5T4eD5Stz0zka0TYjAH08Nd3lPImo5GGaIAtSXf+6P5Qfy8eDgNi6fb58YKT62TfYFgKjQi8+rkU42tgWeTzedBSDH9tJcAMDY7il4cHAb9EqPwY3/2YBTJTVi7015IwdZ7j9fidwyHa5qHSNe+2m34z455ToDMuLDHYa0zpTWOPT4GEwWhCjlDqeKH87XIrfU9ZLyv329G2uOFGLd09fh573W97OFMHeq9SaEKuVQKjh9kCiY8W8wUYC6pmMi5tzeExFq1//mkMlk+PPQtpDLgK8eHoCslCi0TYhAUpR9Lo008EhFh9lDTsPAYwtFvdJjMKBdPEJVCnFIzNYjY/vv2O4p4knhNg8u3I4ZSw9iyje7xWsGs8WhjO31FQ3m3pRJwoxtuMu2xw5g3atGWsYiGabKPlwIiwB8te2sQ0iShqEP153ELe9uRIXOgCJtHa5+eTUe/sJx/xwiCj7smSEKYtPHZuHxER0RHabCsilDoZTLIJdM8u0j2XsmOkwl/pKXropKjQ5zeW/pid+x9Xvg/HfjGSxYd0oMCN3SNHh/Yh+cK6/F377ehYPntbBlh7MuelDCVArUGs1iiJEu8c4prHIoW64zIkkT6hBMzpXXOkwkrqozOc2JKa7So1oyh8c2PwcA5qw4CgB49/cTiApVodZoxtqcYofXbz5Zgt+PFOHpGzpDrVRg04kSJEWp0THZ9c7NROR/DDNEQUypkCM6zNrBGqJ07mjtKRnquSo9BmEqBcpqDMiMs58g726zvRjJJn62oajtZ8qcyshkMqTHhSMtOgwHzztP4O3eSoM6owWdkq29RMsPFKBcZ0ClzugQVBqyT0i2lzlWWAWjyd7LU6YzIDpchTpJ701xld7hNeU1BjHM2Bwp0CJTst+P2SKIK71sq6tiI0IwoksSJn5i/Vo6P6chg8kChVzmtFrMxmIRMPW7vYgLV2HWLd3tddMZsOdcKUZkJXGoi6gZGGaIWrAwyTEKmjAV3rm3t1OZPumxLl/rsKeNmx2JY6WBx02ZHq1iMOf2HgCA55ZYT/We91sOXvrlMAa3j3cqL5MBggAUaGux+WQJZHB9HAMAlNXo0TYhwiEUFVTWoaCyTlLGgMz4CIfhpryyWoRKVpFV1hqdhtt2ny13WD1mW6VlozeZoVYqoDOYMOKNdciIC8d3fxnksg125ZaLe+c8f1NX8fq07/Zj86kyPD2mMyZf18Hla4no4hhmiFq4D/6vLz7ecArP3Zjl8nnpMI1KLsBosf7ClgaVLDeHY0qXgbubbBwXIR2usj62zYOx7Vp8V7/WyIgLR0KkGquPFGH1kUK8uvwoiqv0LgOPjW31kzTMHC9yPKbBFoBsB3ICwPmKWlgk4aasxuBU/8paI4qr7Cu7pENac1ccxcJNp7FsylCcKq5GfmUd8ivrxH14bGw9PtKDP7W1RmjU1jKbT1l7uj5cdxKTr+uA11cehVIuxz9GdXL7mU+X1MBssaBDEoe9iGzYr0nUwt3QPQU//m2w27kxAPB/AzOs/+1gwd+va4fbe7dCxyT75OG+mfbeG+m5Uelx9ntKh22k4iLULh9LxUaEYMr1HXFP/wwx8NiCxOaT1sDTq3U0UqNDxR2TAeCPnCJM+GAztp4qdfvZzlfU4reDBbhQ4bjj8bly+9e2ycbS4apyncHhCIbSGnuw+WDdSehNFvxryQEUSQKPdJXX+2tPoMfMlTh0oRKHLlTa38vF0Jq2zoS8Mh3e++Mk/rPmOGr09jk/BpMF+89VwGIRYDBZcN2/12Lkm+tRKwlnRFc69swQEV64qRvu69cax3aux7jrO0Clcpxj0irGHloEAfht2jXIr6hzCDC902Nc3jtW0vMjvY+UtIcn1k0PT9uECCz+6yDIIMOzP+3HwfNaLNpmXUZ+osj50Ezb7sbzVx9DSbUBwzu7P5zWFkKk++cUV+mhlFdLyhicXne6pMbhvUtr9EisX00277ccAMA/vtuLsBD7/2ora41AjHOokwancp1BXMU2Z8URLNx0Bi/c1BXDOtk/Q0m1Hun1c59Ol9Rg0bazeHRYeyRGqfHckgMo0urx0f19HYbGiFoq9swQEUKUcnRMjoTMze89mUyGtPrDL6/tlIisFA2uy0pyKCNdBi5dHi7d5VfakyMVI1kqLh3ekooOU0GtVCBEKXc4YRywh5H+beMwqF08rumYgPsGWHubSurPpdpwvAQA0CY+HON7pWFEVhKuqg9gBZW1+GHXOYc9bLR1JocVVq429CutMTiWqXYOPMcKqx3OuJIOiaklk7Y3nigRH0vnBi3cdAYA8NIvhx32zZGWeer7ffh4w2k8+uVOlFbrsWhbLlYfKcTZMvvnEQQBp0tqxOXsL/x8EBM/2QpTg2XzUhuOF+OLLWfcPk8UKNgzQ0SXZPFfB+Gb7bn4y7XtXT7vODnWIm7k11uyWipdsopKSroSSxqEpKIlAcZd702yJlSc5Pyf1ccdnrOFqoRItVjmH9/txd68Cry1+rjD/jWurDtWhJd+OYSpIxzns2w/bV/hVeLmHgbJCizb0nKLYG0nm//ttR+u6a4up4rtvUAVtfYyu86WAwD25FZg/3n7kJZ0uOq/m85g9i+HMX1sFv40qA2+2HIWAHDwglYMdVKCIOD+T7cDsK6Ek66MIwo07JkhokvSOjYcT4/JclrmLPXSLd0AALNv7Y61Tw/H/6YMdZio6u610mMXOrnZz8XhzCk3YSZacpSDdOKxlHSVlu3xxYIMAHy1NRd5ZbV46vt9bsuU1Z9FJQgCVArX3Vy2HpXaBoeIX5CswJIOaaVoQsXH0rlB0vk5tmXvAPDH0SKn9wKA2b8cBmDda0c6NKY3Os69sfXclErapNRFj5PNe3+cwP2fbnOYb9RQSbUe+89VuH2eqLnYM0NEHvOnQW1wS69W4qqfpKjQRsv/+LdB2HW2HNdK5oJ0THa9a7F0E7+2Ca4nGzuurnLdw6NxsRlgQzf2SEFcRAjKa4xIiAzB5/W9GA21jg1DabUBqdGhOFVSg715FbjpnQ2YNLitw1lUCrkMvdNjsPNsuTjMpDO5vCUAxxBilAwD/SHZ4E96IKhCbv936ReSukpDUWKUWpxUfVRyoKe0zOrDhZj67R68dmdPh++dTjLZ+ERRFWb/cgR/v74D+mbG4vWV1rlBa44UYVxP1wd/Xv/vtdDWmfDL34c6nAhP5CnsmSEij7qcU6r7Zsbh0WHtHYaopIdsAvbgIt3HJitFA1ekYaZDkrtQJJ2f47qucREhePnWHnhvYh8kaVwHsvaJEVjz5LXYOn0Ebr4qDQCwdO8FHDyvxdzfrDsNK+Uy/Dx5CJZNGYK+bawrwsQwU58PNKHO/6a09RQJguBy9RPgGHi07spIrqdG2z/HqsOFkveyl3luyQHUGMyYsmiPw5CWNPDM+t9hrDtWjDs/2OKwkkuAPbwZTBb8drAAlbVGCIIAbf2OzI2tOiNqDoYZIvKpCfWndd/dL/2Sys+5vQcmX9ceg9sniNcaBibbnBtpUGmf6Lr3RjrM5G7ujVwyE9rdkFZcRAjUSgWiw1WIj3TsBbL1gMRGhKBXegy6pUWLQatCZ8TxompUGazvkRYThoeHtkVWShRuqu/ZKNcZsHTPeRwvqnaYQC0lDRjudlKulJSRHvGQLQkz0vtIg+RJaZhxM+R0vNBeRjo/Z+Gm0/jrV7vwyOc7UVxtDzzuzhkDrKvOrvv3WpRU692WIXKHYYaIfGrWLd3wwf/1wYs3d3Vb5s9D2wIA7rk6HQPbxePpMVkuj2uwsW2sJ51g3PB4AFsPiLT3xt1ScWmAcBd4pMc9JLgrI3kv2/v+euACbnxnMz7OUYjXn7+pK36bNkyciPvdjjxM+24vRr+1HgAQqpKjT0YMZDL7Z63QGXGssAoVOgOq64PEX69tj17pMRjbPUUsY+NqaTngGFRSJL03a47Y596USV6bJmmz3w7lS+5vf68le6ynlm8/U+awQkwaeIqr9PjnD/uxJ9c6eXn+6uM4XVKDb+qX27tisQgO9SWyYZghIp8KD1Hihu6pCA9x/6/0Z27ojP8+2A8vjHcfeGy/1JM1arwxoRcWPTwAA9vFNXgve0+DbY+WzHh74Gk4XLVgYh/0bxuHx0d0FK+ludlsUDpE1bBnxl7GHnJsYabO6LgU2qGnqL68dJWT7fqnD1yNFVOvwS31Q1objpdg9Fvr8dBnO8RyT4/pjJ8nD0GP1tZ5KRW1Rqw8VIDcUp3b4SppCJH23pxyswy8Wm9//LUkeEjDUoYkVB6QrK6S9iC9tfoYvtuZh9ve3+wweVjRYOJ0UVWdGC6f/H4f+r6c7TDnp6Htp8vwws8HxYBHVwaGGSIKOGqlAtdnJTcaeN66+yrc2z8Dn03qj/hINQZ3SICswUY5z93YBYD1uIRXb++BH/46yGGysfTsKgAY2yMVi/8yCMmSeTKdUlzPvXG3mkjKYeVUmOveG+nZU+6OhIgOUyE2IgRZKRqxR8g2HLM7twIAEKVWigdd2t7rj6NF+MuXuzDs9T/E08xHdU2GTGad5Gz9HPYQ4q73RrraSxpsJCdCoEIy90b6bfhlv733Rvpa6Q7M0v1zpLafLkP/V9bg+aXWM72W7DkPiwB86WZCNgDc9eEWfLHlLD5ef8ptmR1nyvDPH/Y32stjMFlwoqjK4UwvClxczUREQSlZEyoeYOnOff0z0L1VNLJSohCqUqBfmzinMjHhKqcDLKXUSgXUSrlTb0mUZOJuTHgIEiJDxA36bKQ9MzFuJhtL55S4KyO9j9tNBV0sOS9t8Ms6Uq3EO/f2RnGVHkfytVh+oAAl1XrMXHYIfTNjncrbdlGWhhx3y9ilQ1HlkmBj2wMHcOyZke4ntMnNhoFfbrWGlm+25+HZsV3E69LQJwgCfj2Qj36ZcUjW2O95XnJ8RVWdCTMWH8CNPVIxrmcqJnywBYA1zM68uZvLz/Psj/vx057zePe+3ripZ5rLMhQ42DNDRC2WXC7DVekxTiukpAa2dX+QpY10Lsk3jwzEqK7JeKLBYZDS3hzbCixpOGnTYDn5gEQLYsNVeFJyn2Q3K6eqJEM77gKPdC6QdK6OVGyECqEqBdLjwsW5QPvPVeKzzWfw92/2iJv7PTy0LSLVSsy6pTsA66TfN1fl4Nf9+WKwabiPjnSpeJmbHh7pUJfOYB8G+p+k90baWyJdgSWdtGySzGn67WABpizag2Gv/+EQDKXDgD/sPo9fD+Rj8qLdDvXJleyQDADHC6ugrbPW8af6eT/v/3HS5WcBgAsVtfh4/SnxNa6sPFSAIXN/xzau5PIq9swQ0RVt9q3dIZcDEwdkui0zrkcq3l97EuEhCgxqH49BLk7yHt45EYcuaMXHp0tqHM6uilQrkR4Xhrwya4/B8DQLvrrzOodzsFKjQ5GsUaNQ67iiR7rJnTRYSUnDjLvl8XrJfB13PTxqpRz/GtcFz9/UVRz+OV9Ri7d/P+FQ7ttHB+K3gwXomxmLv36122HYzd3wTaWbHp59eRX210rKSAPPl5JjFaT331u/GZ/BZMHWU/bdmKWnpEt3YJa2pbSHJ6egCmPmr0fn5CismHqNeD1B0oNkNFvwzA/7MaBtHO7pn4G7P9qCvLJanK+oddvD85cvdwEAnvlxP9Y9fZ3LMtR8DDNEdEVLjFLj/Yl9Gy0zdWRHhKoUGNU12X2ZEZ1gEYARWUnokqrB9VlJGNjOMfRkxIWLYSbWRZaQyWTolBwlhpmpIzriP2uO41832odYNKEqdEiKdDpc03FnY9dBRbovjLv5OTHhKnHukbt9eEJVcvTNjEPfzDgcrg9wRdo63P/pNmTGh4s9MMM6JWL9sWKM7Z6CFQcLHHpmyl2cdQWgQSiyP953zj6RWBqE1Ep7r9tH6+29KNKeIoNk48HvdtgnLUvPpVp3zLp6K6ewCseK7OdtSYcT1xwpwpI957Fkz3nc1qeV+L2UDpMBwG8H85EeF45uafYNAo0NhinXHytGsiYUnVOisOtsOV5dfgQzburq8mgJANh1tgzP/XQQM27qiqEdE1yWuZJxmImI6CLUSgUeH9ERXVJdb9YHWPe6+ecNWejXJg4RaiWu6ZgIldPycHs4CHPzT0np0MrUER2x4Znr8H8DHXuNekh20bVNaI6WTDBOjFQjzMXQmnTPnJgwlcMvahtp4NGEqhCqcv414TCHp/7YCG2dCRuOl+Crrbni6qN37umNRQ8PEIfkKnRGHC3Q4ly5zu1QlLTXxd38HIdJy5IyB8/bVzlJXysNSIskK7DKJNelQ5Ffb7WXkW5I6LAb81H7bszS5eqHL2jx1692Y9zbGx1WaSVKenhyS3X403+3Y8z89bBYBNz5wWbsOluOx7/Z4/LzAsC9H29DTmEVHvlip9syhy9occt7m7DheLHbMgCQX1nb4iY2M8wQEflI5xTX505JPTa8A8JDFBjXMxVyuQzpceFOq7SkQ00vjO+K6zon4r7+GeK1EKUcV7e1T3b+4qH+GJGVhA/vt/dA2eYTNST9HSeXyxx6F2zyJedIuRuuilIrER2uwuAOCWJPUWWtEePf2Yihr/0hbiz48q3d0S1Ng9fv7AnAGlSOF1bhbGnNJa2uchd4KtwMe0mHn8pq7MFNuhmgbeKxrT7iPSXB5t0/7AeZKiU7WEsnHv9vn/3wULUkLJ0rt8/VOVKgFdu8qMreroIgYOayQ1iw1trbZBsqq21wBtb81cewcNNpAMAjX+zEvrwK8YBQV37ZfwGD5vyO+Q0OYpWq0Bnw/NIDDsN/gY5hhojIRx65ph3G9UjFmxPcr8JqkxCBLdNH4K27rnJbZuKADISpFLg+KwntEyOxcFJ/cW8Zm66SXqQB7eLw6YNXO63m6p0RKz4eXT+EdkO3FIcyPVs3fpZSqErhcjiqSrLPi3Q+j/TMKgC4s29r/Pr4Nbi2s7WHqVxnxI1vb8C1r68Vl2/PuKkr+mTEYG796jVpUGkYZmzBwnHJuZshrRr393FVxm0vkJvdmD/eYF8eLh32kk4Y3nDcPkQlnQB+srgan20+g9d+O4rSamlvmb03raCyDvNXH8es/x1GWY3BIUhJrTtWjI3172Pr/fnPGscw88v+C1haP+n5hZ8P4autubjzg80u7wcAepMZH647iROSITl/4pwZIiIfiVAr8d7EPjAajVh+zv2QQrSb1Ug2rWPDsWX69Y2u0hrbPQUfrDsJucxxXomU9Pyqt+/tjfXHinF1g8AjHdJ6597eePnXw5hyfUeHMv3bxmHloUK4E6KUI1KtdLmRne0zSHt4bIHHVv6G7in489C2KNJaey7KdAY8uHA7BMHem/HmXb1wvKgaI7sk444Fm1FeYxSHUmzBpuHkaoceHje9QKWS3ptL2UVZ2ttzrFB6vpU0ONkffyXpBZLuqyQtv3jnOfGxdEdq6dEP7s69qtGb8MgXO2G2CFj39HC4Oh1Db7KexwUAfTJixZPXGwbPU8XVqDWa0S0tGvNXH8eCtSfx8YZT2PLP4S7f25cYZoiIgpC7Sb42vdJj8N2jA93uTgwAI7skoVNyJNolRCJUpcDoBr0yANA1zd7DM6ZbCsb3ct5zZXD7BDHM3D8wE19uPet09lb7pMhGhy1UCjmi1EqHHh0bW8+P7TMLArA2x3FeSOeUKNzep7V4ZILBbMFba07g6Bm5OCz22h09seNMGcb3SsMN8zeg1mjG9J8OoLzGIC7r7t8mDtvPlOHufun4bmce6owW1BrMCAtRiD1CDYOZNBQ13KvHpkJngCAIkMlkDqFIunmgdLWXdNhL2sMj3aVZep9f9tuHtKS9NyXVenGISnqiunRoTPpe28+UufweCIKAsf/ZAL3JgnVPD8ePu87V3z8wjpdgmCEiaqEGtGt8D53wECVW/ePaRstkpWgw+9buiAlTuT0fa4DkGIknR3fCxIEZSI8NdyjTLU1z0TkYrWLDcLTAedjCNpk5ROk+8NhWZ4WHKBCikMNgtmDButOwzqawhpD2iZEY3jkJgiBApZDBaBbwzXbHs6CmXN8ByZpQtE+MwJI952EwW3C6pAZxESFiaHl2bBaMZgv6t43DuLc3QltngtFsgUohR5mbX+5Gs4AagxmRaqXbIa3SGteBx3EyswEWiwC5XOZwffmBAvFxndEi3kda5iPJrsjS3a+lZRquzLKp1pvEjSN/2Z/vMFE8EDDMEBFRo+4f6H4PHgDonByFx6/vgLAQJWLCQ1z2GnWXTCR++dbu+Gn3OUxo0HvTo1W0yzAjnQAdHxnipvcmRCwbG6Fy2qsHsAcemUyGuIgQt2VsE7Vt97nx7Q0IVcmhklvDXIomFCO7JsNsEcRdkj/deBqF2jpx3spTozvhZHENJvRrjUkLd0BvsqC8xoBItVIcltKEKqGV9LToTRboDGZENBJ4LIJ1zk1MeIjbMgazBVV6EzShKrdDY1WSACbtTbIdEmqvkxlqpcLhvZbtveBQJhBWRjHMEBFRs8hkMjwxunOjZbpJhqv6t41zWm4OWCcbf18/fJEWHYoLklVT9vtE40ypzum6dP5QYpTzxoOA48GjseGuw0x8ZIjLMnVGC+pg7ZmwLUdXyGViqJi74qjDfTolR4lzi2LDQ1CgrcOqw4VQyOxHWDw5ujPOlupwY48UTPxkG/QmC8pqDIhQK8WhH9sxGV1TNThbWoMagxnlusbDDACUVRugCVU5zM9pqEJnRGKU2mGCsdN9agxIjQ5zeK+cQsfAWa03N3yZzzHMEBGR10mXpUtP1Zbq0TpGfPzW3Vdh08lS8XR0m6vSY/DrgXw0pkuKxmG1kY20h6ddYoTLXiDpROSU6FCXZWIczspSuQwV0lAUE65CgbYOs3857FCmVUwYHhjcBoC1Ryi/sg7/WXMcZTUG6E3WgPDosHbIiItAz9bRuPujLagpq0V+RS1UCpnYozKhb2vsPFuOUV2TseJgPvLKalFaY0CbhAhxQvKQDvEortJjcPsE/Lz3PMp1RpTrDEiMUjcaikqrncNMQ+4mT/sSwwwREXldqEqBDc9cB0GA21VYXVLtgad9UqTLOT9XZcSIj9+6uxe+2Z6H67OSHMp0S9Pge+spAhjVyoK27dqhd4bjKq2erWMc5plI62mTlaJxmmgMAHGSMBMXEYKTxc6nfsdF2Cdeu9uLR7oyyRZmfth1zqFMXIQaN3RPEd83r6wWDyzcDrNFEFcm9Wgdjdcn9AIAbDtdhryyWry24ihKa/TomGRt007JUfj64YEAgPXHi1GuM4oBxRaKhndOxI7TZchK1aC6zoScwiqnMnERISjXGRAfEQKDyQJtnanRoOMrDDNEROQT6W56ZGzUSgUW/2UQagwmJLhZhSWde9MtLRqL/9LauYxkOXlSqICnR3dyOAMLcNw/55kbOmPloUL0arhXT5rrHZ81kqXz6XHh2HGm3KmM9LgI6e6/UvENwowrcRHOx1Q0XDItfa3tntvPWM+psgWt+AZlThXX4L0/TuD5pQeRXz/Pp29GLP5zd2+oVXL8+fMdyCkEft2fjx93nxPfY3inRNw/KBPxEWpMXrQbB85XMswQERFJ9W8b1+jzYSEK/Oeeq1BcpUenZNc7KkuPnYh0s2WPdP+cFE0ofp48xKmMdOPBiQMysGh7LrqnRUMhWdbcNzMWP+22Tpq1LesGHJdHd06JAvY510HaM5PoJrxJe3jcBp5w5zDT2HvZeoqkG/YBQFxkiHhIqe19v9uZ53Qf22aLtvqU1RjheCa87wXVDsBz586FTCbDtGnT/F0VIiLyk1uuaoWHr2nn9vkItRLjeqaiTXw42mtcr7SJClXh6jaxUCvlbg9ubJtg/xXdISkS26aPwNePDHAoI91kcGjHBHz76ED8b8pQh/k5WZL5Qtd2SkRWShT6ZcY6BJ6GOzjbSMNJZrzrnq04yfyc1rFuyoRfvBeoYe+Ny/tIy0Tawgx7Zi7Zjh078OGHH6Jnz57+rgoREQW49+7rA4PBgBUrVrgt8+WfB6BGb3K7saBCLsNd/Vrj96NFGN8rzeXQV4dE+y7K4SEKp5PSASBL0sPTNiECCx+8GnK543lb/TLtoej6rCT8Xr8Lr/Q9pT1OSVFqca+XpCj7MQg9WrseGou9hGEv6fCZ9GgFKVdDWuU6A9JdlvadoAgz1dXVmDhxIj7++GO8/PLL/q4OEREFgYYHdDYUqlI0eiQEAMy7sxfMFsFhaElKLpfhnzdkYdXhAqd9c2zSJAeD6k0WpyADAFmSyc+RaiWWTRkCi+C4uV2XFHtQ6dk6GhMHZEJnMDsEDOl8IenxDdIjMrIk95HudSMNZtLJ2FKu5gLlV9ah58XPUPWqoAgzkydPxrhx4zBy5MiLhhm9Xg+93r5mXqu1Ls8zGo0wGt2vt28K2/08fV9yxHb2Dbaz77CtfcOT7WxpZCuVh4dk4OEhGY2+V8/WGuw/p8X4Hsluy7SOCcW5ijqM6JyALskRTvdLjrT/yjaYzBjaPtapTGyoZL+dSDV6topGgbYOGTFqsVznZPtQVOvYMLx/31Wo0BkRG6awl0myl4kKVaKqPvCoZIJYpmOitY4HL2gxprPnf54v534BH2a+/fZb7N69Gzt27Lik8nPmzMGsWbOcrq9atQrh4Y3PpG+q7Oxsr9yXHLGdfYPt7Dtsa98IhHa+LxUYHQsUH96C5Yddl/lLeyC3RgYhdzeW57kuY/u1baosxvLly12WiFQpUG2UIUVWgZtiLEAMsHrVb+Lz1g17rfcprdBi3+Y/AABn97p+L4vJiBFpAnKrgbKc7Vhef+C2dT8+JXLLaqEzeb6ddTrnzRHdCegwk5eXh6lTpyI7Oxuhoa7H7xqaPn06nnjiCfFrrVaL9PR0jB49GhqN67HEpjIajcjOzsaoUaOclv2R57CdfYPt7Dtsa99oie0c2r4Y3+04h5dv6ep27ku3ATr8dqgQ/zcgHRFq17/mp21dBQBIiNXgxhsHuSwzdYu1TGpsJD74m/NqLwB4/+QGnCuvxbkaGSbfOdKj7WwbWbkUAR1mdu3ahaKiIvTp00e8ZjabsX79erz77rvQ6/VQKBzHO9VqNdRq52+wSqXy2g+zN+9Ndmxn32A7+w7b2jdaUjuP6Z6GMd2dTy6X6pASjSkprldH2bx2Rw/MWXEUs27u7rZtPvlTP7y64gjmTejltkzP1tE4V16LvGrPt/Pl3Cugw8yIESNw4MABh2uTJk1CVlYW/vnPfzoFGSIiIrq4u6/OwF390hudJD2yazJGdk1u9D49WsVgzZEi1Jotnq7iZQnoMBMVFYXu3bs7XIuIiEB8fLzTdSIiIrp0F1vtdSkeGJyJBwa2RvbK3y5e2IuCatM8IiIiChzhIUqoFP6PEgHdM+PK2rVr/V0FIiIiCiD+j1NEREREzcAwQ0REREGNYYaIiIiCGsMMERERBTWGGSIiIgpqDDNEREQU1BhmiIiIKKgxzBAREVFQY5ghIiKioMYwQ0REREGNYYaIiIiCGsMMERERBbWgO2jycgmCAADQarUev7fRaIROp4NWq4VKpfL4/cmK7ewbbGffYVv7BtvZN7zVzrbf27bf441p8WGmqqoKAJCenu7nmhAREdHlqqqqQnR0dKNlZMKlRJ4gZrFYcOHCBURFRUEmk3n03lqtFunp6cjLy4NGo/HovcmO7ewbbGffYVv7BtvZN7zVzoIgoKqqCmlpaZDLG58V0+J7ZuRyOVq3bu3V99BoNPyL4gNsZ99gO/sO29o32M6+4Y12vliPjA0nABMREVFQY5ghIiKioMYw0wxqtRovvvgi1Gq1v6vSorGdfYPt7Dtsa99gO/tGILRzi58ATERERC0be2aIiIgoqDHMEBERUVBjmCEiIqKgxjBDREREQY1hponee+89tGnTBqGhoRgwYAC2b9/u7yoFlfXr12P8+PFIS0uDTCbD0qVLHZ4XBAEvvPACUlNTERYWhpEjR+L48eMOZcrKyjBx4kRoNBrExMTgz3/+M6qrq334KQLfnDlzcPXVVyMqKgpJSUm49dZbkZOT41Cmrq4OkydPRnx8PCIjI3HHHXegsLDQoUxubi7GjRuH8PBwJCUl4emnn4bJZPLlRwl4CxYsQM+ePcWNwwYNGoQVK1aIz7OdvWPu3LmQyWSYNm2aeI1t3XwzZ86ETCZz+JOVlSU+H3BtLNBl+/bbb4WQkBDhv//9r3Do0CHhkUceEWJiYoTCwkJ/Vy1oLF++XPjXv/4l/PTTTwIAYcmSJQ7Pz507V4iOjhaWLl0q7Nu3T7j55puFtm3bCrW1tWKZG264QejVq5ewdetWYcOGDUKHDh2Ee++918efJLCNGTNGWLhwoXDw4EFh7969wo033ihkZGQI1dXVYpm//vWvQnp6urBmzRph586dwsCBA4XBgweLz5tMJqF79+7CyJEjhT179gjLly8XEhIShOnTp/vjIwWsZcuWCb/++qtw7NgxIScnR3juuecElUolHDx4UBAEtrM3bN++XWjTpo3Qs2dPYerUqeJ1tnXzvfjii0K3bt2E/Px88U9xcbH4fKC1McNME/Tv31+YPHmy+LXZbBbS0tKEOXPm+LFWwathmLFYLEJKSorw+uuvi9cqKioEtVotfPPNN4IgCMLhw4cFAMKOHTvEMitWrBBkMplw/vx5n9U92BQVFQkAhHXr1gmCYG1XlUolfP/992KZI0eOCACELVu2CIJgDZ5yuVwoKCgQyyxYsEDQaDSCXq/37QcIMrGxscInn3zCdvaCqqoqoWPHjkJ2drZw7bXXimGGbe0ZL774otCrVy+XzwViG3OY6TIZDAbs2rULI0eOFK/J5XKMHDkSW7Zs8WPNWo7Tp0+joKDAoY2jo6MxYMAAsY23bNmCmJgY9OvXTywzcuRIyOVybNu2zed1DhaVlZUAgLi4OADArl27YDQaHdo6KysLGRkZDm3do0cPJCcni2XGjBkDrVaLQ4cO+bD2wcNsNuPbb79FTU0NBg0axHb2gsmTJ2PcuHEObQrwZ9qTjh8/jrS0NLRr1w4TJ05Ebm4ugMBs4xZ/0KSnlZSUwGw2O3yDACA5ORlHjx71U61aloKCAgBw2ca25woKCpCUlOTwvFKpRFxcnFiGHFksFkybNg1DhgxB9+7dAVjbMSQkBDExMQ5lG7a1q++F7TmyO3DgAAYNGoS6ujpERkZiyZIl6Nq1K/bu3ct29qBvv/0Wu3fvxo4dO5ye48+0ZwwYMACfffYZOnfujPz8fMyaNQvXXHMNDh48GJBtzDBDdIWYPHkyDh48iI0bN/q7Ki1W586dsXfvXlRWVuKHH37AAw88gHXr1vm7Wi1KXl4epk6diuzsbISGhvq7Oi3W2LFjxcc9e/bEgAEDkJmZicWLFyMsLMyPNXONw0yXKSEhAQqFwmnWdmFhIVJSUvxUq5bF1o6NtXFKSgqKioocnjeZTCgrK+P3wYUpU6bgl19+wR9//IHWrVuL11NSUmAwGFBRUeFQvmFbu/pe2J4ju5CQEHTo0AF9+/bFnDlz0KtXL/znP/9hO3vQrl27UFRUhD59+kCpVEKpVGLdunV4++23oVQqkZyczLb2gpiYGHTq1AknTpwIyJ9nhpnLFBISgr59+2LNmjXiNYvFgjVr1mDQoEF+rFnL0bZtW6SkpDi0sVarxbZt28Q2HjRoECoqKrBr1y6xzO+//w6LxYIBAwb4vM6BShAETJkyBUuWLMHvv/+Otm3bOjzft29fqFQqh7bOyclBbm6uQ1sfOHDAITxmZ2dDo9Gga9euvvkgQcpisUCv17OdPWjEiBE4cOAA9u7dK/7p168fJk6cKD5mW3tedXU1Tp48idTU1MD8efb4lOIrwLfffiuo1Wrhs88+Ew4fPiw8+uijQkxMjMOsbWpcVVWVsGfPHmHPnj0CAOHNN98U9uzZI5w9e1YQBOvS7JiYGOHnn38W9u/fL9xyyy0ul2b37t1b2LZtm7Bx40ahY8eOXJrdwN/+9jchOjpaWLt2rcMSS51OJ5b561//KmRkZAi///67sHPnTmHQoEHCoEGDxOdtSyxHjx4t7N27V/jtt9+ExMRELmNt4NlnnxXWrVsnnD59Wti/f7/w7LPPCjKZTFi1apUgCGxnb5KuZhIEtrUnPPnkk8LatWuF06dPC5s2bRJGjhwpJCQkCEVFRYIgBF4bM8w00TvvvCNkZGQIISEhQv/+/YWtW7f6u0pB5Y8//hAAOP154IEHBEGwLs+eMWOGkJycLKjVamHEiBFCTk6Owz1KS0uFe++9V4iMjBQ0Go0wadIkoaqqyg+fJnC5amMAwsKFC8UytbW1wmOPPSbExsYK4eHhwm233Sbk5+c73OfMmTPC2LFjhbCwMCEhIUF48sknBaPR6ONPE9geeughITMzUwgJCRESExOFESNGiEFGENjO3tQwzLCtm+/uu+8WUlNThZCQEKFVq1bC3XffLZw4cUJ8PtDaWCYIguD5/h4iIiIi3+CcGSIiIgpqDDNEREQU1BhmiIiIKKgxzBAREVFQY5ghIiKioMYwQ0REREGNYYaIiIiCGsMMERERBTWGGSLym+LiYvztb39DRkYG1Go1UlJSMGbMGGzatAkAIJPJsHTpUv9WkogCntLfFSCiK9cdd9wBg8GAzz//HO3atUNhYSHWrFmD0tJSf1eNiIIIe2aIyC8qKiqwYcMGvPbaa7juuuuQmZmJ/v37Y/r06bj55pvRpk0bAMBtt90GmUwmfg0AP//8M/r06YPQ0FC0a9cOs2bNgslkEp+XyWRYsGABxo4di7CwMLRr1w4//PCD+LzBYMCUKVOQmpqK0NBQZGZmYs6cOb766ETkYQwzROQXkZGRiIyMxNKlS6HX652e37FjBwBg4cKFyM/PF7/esGED/vSnP2Hq1Kk4fPgwPvzwQ3z22Wd45ZVXHF4/Y8YM3HHHHdi3bx8mTpyIe+65B0eOHAEAvP3221i2bBkWL16MnJwcfP311w5hiYiCCw+aJCK/+fHHH/HII4+gtrYWffr0wbXXXot77rkHPXv2BGDtYVmyZAluvfVW8TUjR47EiBEjMH36dPHaV199hWeeeQYXLlwQX/fXv/4VCxYsEMsMHDgQffr0wfvvv4/HH38chw4dwurVqyGTyXzzYYnIa9gzQ0R+c8cdd+DChQtYtmwZbrjhBqxduxZ9+vTBZ5995vY1+/btw0svvST27ERGRuKRRx5Bfn4+dDqdWG7QoEEOrxs0aJDYM/Pggw9i79696Ny5Mx5//HGsWrXKK5+PiHyDYYaI/Co0NBSjRo3CjBkzsHnzZjz44IN48cUX3Zavrq7GrFmzsHfvXvHPgQMHcPz4cYSGhl7Se/bp0wenT5/G7NmzUVtbi7vuugt33nmnpz4SEfkYwwwRBZSuXbuipqYGAKBSqWA2mx2e79OnD3JyctChQwenP3K5/X9pW7dudXjd1q1b0aVLF/FrjUaDu+++Gx9//DG+++47/PjjjygrK/PiJyMib+HSbCLyi9LSUkyYMAEPPfQQevbsiaioKOzcuRPz5s3DLbfcAgBo06YN1qxZgyFDhkCtViM2NhYvvPACbrrpJmRkZODOO++EXC7Hvn37cPDgQbz88svi/b///nv069cPQ4cOxddff43t27fj008/BQC8+eabSE1NRe/evSGXy/H9998jJSUFMTEx/mgKImougYjID+rq6oRnn31W6NOnjxAdHS2Eh4cLnTt3Fp5//nlBp9MJgiAIy5YtEzp06CAolUohMzNTfO1vv/0mDB48WAgLCxM0Go3Qv39/4aOPPhKfByC89957wqhRowS1Wi20adNG+O6778TnP/roI+Gqq64SIiIiBI1GI4wYMULYvXu3zz47EXkWVzMRUYvjahUUEbVcnDNDREREQY1hhoiIiIIaJwATUYvD0XOiKwt7ZoiIiCioMcwQERFRUGOYISIioqDGMENERERBjWGGiIiIghrDDBEREQU1hhkiIiIKagwzREREFNQYZoiIiCio/T+wVxBY4nSWQgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FineWeb-Edu dataset\n",
        "\n",
        "Using the sample-10BT: a subset randomly sampled from the whole dataset of around 10B gpt2 tokens.\n",
        "\n",
        "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu"
      ],
      "metadata": {
        "id": "vSL2AEIWVtW_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_53UtZLjU3dn"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "DATA HANDLER FOR FineWeb-Edu dataset\n",
        "Downloading the dataset, tokenizing all the documents inside this dataset, starting with an eot\n",
        "token, encoding using uint16 to save space. Then, the tokenized datased is saved into shards (numpy\n",
        "files very similar to tensors).\n",
        "\"\"\"\n",
        "\n",
        "!pip3 install datasets\n",
        "\n",
        "import multiprocessing as mp\n",
        "from datasets import load_dataset\n",
        "\n",
        "local_dir= \"edu_fineweb10B\"\n",
        "remote_name= \"sample-10BT\"\n",
        "shard_size= int(1e8) # 100M tokens per shard, total of 100 shard files\n",
        "\n",
        "# create the cache the local directory if it doesn't exist yet\n",
        "#DATA_CACHE_DIR= os.path.join(os.path.dirname('__init__'), local_dir)\n",
        "DATA_CACHE_DIR= os.path.join(os.path.dirname('data/'), local_dir)\n",
        "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# download the dataset\n",
        "fw= load_dataset(\"HuggingFaceFW/fineweb-edu\", name=remote_name, split=\"train\")\n",
        "\n",
        "# init the tokenizer\n",
        "enc= tiktoken.get_encoding(\"gpt2\")\n",
        "eot= enc._special_tokens['<|endoftext|>'] # end of text token\n",
        "def tokenize(doc):\n",
        "    # tokenizes a single document and returns a numpy array of uint16 tokens\n",
        "    tokens= [eot] # the special <|endoftext|> token delimits all documents\n",
        "    tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n",
        "    tokens_np= np.array(tokens)\n",
        "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
        "    tokens_np_uint16= tokens_np.astype(np.uint16)\n",
        "    return tokens_np_uint16\n",
        "\n",
        "def write_datafile(filename, tokens_np):\n",
        "    np.save(filename, tokens_np)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3CCq4dEJD2Vs"
      },
      "outputs": [],
      "source": [
        "# tokenize all documents and write output shards, each of shard_size tokens (last shard has remainder)\n",
        "nprocs= max(1, os.cpu_count()//2)\n",
        "with mp.Pool(nprocs) as pool:\n",
        "    shard_index= 0\n",
        "    # preallocate buffer to hold current shard\n",
        "    all_tokens_np= np.empty((shard_size,), dtype=np.uint16)\n",
        "    token_count= 0\n",
        "    progress_bar= None\n",
        "    for tokens in pool.imap(tokenize, fw, chunksize=16):\n",
        "\n",
        "        # is there enough space in the current shard for the new tokens?\n",
        "        if token_count + len(tokens) < shard_size:\n",
        "            # simply append tokens to current shard\n",
        "            all_tokens_np[token_count:token_count+len(tokens)]= tokens\n",
        "            token_count += len(tokens)\n",
        "            # update progress bar\n",
        "            if progress_bar is None:\n",
        "                progress_bar= tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
        "            progress_bar.update(len(tokens))\n",
        "        else:\n",
        "            # write the current shard and start a new one\n",
        "            split= \"val\" if shard_index == 0 else \"train\"\n",
        "            filename= os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
        "            # split the document into whatever fits in this shard; the remainder goes to next one\n",
        "            remainder= shard_size - token_count\n",
        "            progress_bar.update(remainder)\n",
        "            all_tokens_np[token_count:token_count+remainder]= tokens[:remainder]\n",
        "            write_datafile(filename, all_tokens_np)\n",
        "            shard_index += 1\n",
        "            progress_bar= None\n",
        "            # populate the next shard with the leftovers of the current doc\n",
        "            all_tokens_np[0:len(tokens)-remainder] = tokens[remainder:]\n",
        "            token_count= len(tokens)-remainder\n",
        "\n",
        "    # write any remaining tokens as the last shard\n",
        "    if token_count != 0:\n",
        "        split= \"val\" if shard_index == 0 else \"train\"\n",
        "        filename= os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
        "        write_datafile(filename, all_tokens_np[:token_count])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified DataLoader for iterating over all the shards from HF FineWeb.\n",
        "\n",
        "TODO: randomly permute the documents in every single shard or every single new epoch. It is better for the optimization so that we are not seeing thing identically, by introducing some randomness in how the documents follow each other. This strategy break up the dependence between two consecutive documents because it's a kind of spurious correlation."
      ],
      "metadata": {
        "id": "n1PE93ieV8ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_tokens(filename):\n",
        "    npt= np.load(filename)\n",
        "    npt= npt.astype(np.int32)\n",
        "    ptt= torch.tensor(npt, dtype=torch.long)\n",
        "\n",
        "    return ptt\n",
        "\n",
        "\n",
        "class DataLoaderLite_FW:\n",
        "\n",
        "    def __init__(self, B, T, device, process_rank, num_processes, split) -> None:\n",
        "        self.B= B\n",
        "        self.T= T\n",
        "        self.device= device\n",
        "        self.process_rank= process_rank\n",
        "        self.num_processes= num_processes\n",
        "        assert split in {'train', 'val'}\n",
        "\n",
        "        # get the shard filenames\n",
        "        data_root= 'edu_fineweb10B'\n",
        "        shards= os.listdir(data_root)\n",
        "        shards= [s for s in shards if split in s]\n",
        "        shards= sorted(shards)\n",
        "        shards= [os.path.join(data_root, s) for s in shards]\n",
        "        self.shards= shards\n",
        "        assert len(shards) > 0, f'No shards found for split {split}'\n",
        "        if master_process:\n",
        "            print(f'Found {len(shards)} shards for split {split}')\n",
        "        self.reset()\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        # state, init at shard zero\n",
        "        self.current_shard= 0\n",
        "        self.tokens= load_tokens(self.shards[self.current_shard])\n",
        "        self.current_posistion= self.B * self.T * self.process_rank\n",
        "\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T= self.B, self.T\n",
        "        buf= self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        x= (buf[:-1]).view(B, T) # inputs\n",
        "        y= (buf[1:]).view(B, T)  # targets\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B * T * self.num_processes\n",
        "        # if loading the next token would be out of bounds, reset\n",
        "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "            self.current_position= B * T * self.process_rank\n",
        "\n",
        "        return x.to(device), y.to(device)\n"
      ],
      "metadata": {
        "id": "TzlX-RMCLEx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation Loss** measures how good we are at predicting the next token in a sequence on some validation data that the model has not seen during training."
      ],
      "metadata": {
        "id": "6MOtCS2mDLHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_supervised_training_FW(model, train_loader, val_loader, optimizer, scheduler, steps,\n",
        "                                grad_accum_steps, ddp, ddp_rank, master_process,\n",
        "                                use_compile, eval_interval=200):\n",
        "    # --- training loop ---\n",
        "    for step in range(steps):\n",
        "        start= time.time()\n",
        "        last_step= (step== steps - 1)\n",
        "\n",
        "        # --- once in a while evaluate our validation loss ---\n",
        "        if step % eval_interval== 0 or last_step:\n",
        "            model.eval()\n",
        "            val_loader.reset()\n",
        "            with torch.no_grad():\n",
        "                val_loss_accum= 0.0\n",
        "                val_loss_steps= 20\n",
        "                for _ in range(val_loss_steps):\n",
        "                    x, y= val_loader.next_batch()\n",
        "                    logits, loss= model(x, y)\n",
        "                    loss= loss / val_loss_steps\n",
        "                    val_loss_accum += loss.detach()\n",
        "            if ddp:\n",
        "                dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n",
        "            if master_process:\n",
        "                print(f\"Validation loss: {val_loss_accum.item():.4f}\")\n",
        "\n",
        "\n",
        "        # --- once in a while generate from the model (except step 0, which is noise)\n",
        "        if ((step> 0 and step % eval_interval==0) or last_step) and (not use_compile):\n",
        "            model.eval()\n",
        "            enc= train_loader.encoder\n",
        "            num_return_sequences= 4\n",
        "            max_length= 32\n",
        "            tokens= enc.encode(\"Hello, I'm a language model,\")\n",
        "            tokens= torch.tensor(tokens, dtype=torch.long)\n",
        "            tokens= tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "            xgen= tokens.to(device)\n",
        "            sample_rng= torch.Generator(device=device)\n",
        "            sample_rng.manual_seed(42 + ddp_rank)\n",
        "            # same code as seen before in \"Generating Some Text Sequences\"\n",
        "            while xgen.size(1) < max_length:\n",
        "                # forward the model to get the logits -- in every iteration we're going to be adding a\n",
        "                # column of new indices (by resampling) into each one of the rows in x\n",
        "                with torch.no_grad():\n",
        "                    logits, _= model(xgen) # (B, T, vocab_size)\n",
        "                    # take the logits at the last position\n",
        "                    logits= logits[:, -1, :] # (B, vocab_size)\n",
        "                    # get the probabilities\n",
        "                    probs= F.softmax(logits, dim=-1)\n",
        "                    # do top-k sampling of 50 (HF pipeline default) -- we only want to keep the top 50\n",
        "                    # most likely tokens; so, that way we are never sampling very rare tokens\n",
        "                    # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "                    topk_probs, topk_indices= torch.topk(probs, 50, dim=-1)\n",
        "                    # select a token from the top-k probabilities\n",
        "                    ix= torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
        "                    # gather the corresponding indices\n",
        "                    xcol= torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "                    # append to the sequence\n",
        "                    xgen= torch.cat((xgen, xcol), dim=1)\n",
        "            # print the generated text\n",
        "            for i in range(num_return_sequences):\n",
        "                tokens= xgen[i, :max_length].tolist()\n",
        "                decoded= enc.decode(tokens)\n",
        "                print(f\"Rank {ddp_rank} sample {i}: {decoded}\")\n",
        "\n",
        "\n",
        "        # --- training step ---\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        loss_accum= 0.0\n",
        "\n",
        "        # iterating over all batches accumulating gradients\n",
        "        for micro_step in range(grad_accum_steps):\n",
        "            # --- minibatch construction ---\n",
        "            Xmb, Ymb= train_loader.next_batch()\n",
        "\n",
        "            # --- forward pass and get loss ---\n",
        "            # DDP does the backward pass and synchronizes the gradient\n",
        "            if ddp: # backward_grad_sync only turns on (True) when the micro_step is the last\n",
        "                model.require_backward_grad_sync= (micro_step== grad_accum_steps-1)\n",
        "            logits, loss= model(Xmb, Ymb)\n",
        "\n",
        "            # --- gradient pass to calculate the gradients ---\n",
        "            \"\"\" the loss reduction is the mean by default. We need to compensate it by\n",
        "            the number of gradient accumulation steps before accumulating on each backward().\n",
        "            addition of grads corresponds to a SUM in the objective, but instead of a SUM\n",
        "            we want MEAN. Scale the loss here so it comes out right \"\"\"\n",
        "            loss= loss / grad_accum_steps\n",
        "            loss_accum += loss.detach()\n",
        "            loss.backward() # this is a plus equals, i.e., accumulates the grads\n",
        "        if ddp:\n",
        "            \"\"\" reduce the accum loss over all the processes to the average over that loss\n",
        "            the loss_accum tensor exists on all the ranks, when we call all_reduce of AVG it\n",
        "            creates the average of those numbers and deposits that average on all the ranks\"\"\"\n",
        "            dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
        "        \"\"\" calculating the global norm of the parameters to preventing the model from getting\n",
        "        too big shocks in terms of gradient magnitude \"\"\"\n",
        "        norm= torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        # determine and set the learning rate for this interaction\n",
        "        scheduler.step(step)\n",
        "\n",
        "        # --- update the parameters ---\n",
        "        optimizer.step()\n",
        "\n",
        "        # --- evaluation and track stats ---\n",
        "        if Xmb.device.type== 'cuda':\n",
        "            torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "        end= time.time()\n",
        "        dt= end - start # time difference in seconds\n",
        "        tokens_processed= train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
        "        tokens_per_sec= tokens_processed / dt\n",
        "        if master_process:\n",
        "            print(f\"Step {step:4d} | loss: {loss_accum.item():.4f} | lr: {scheduler.get_last_lr():.3e} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.1f}\")\n"
      ],
      "metadata": {
        "id": "yXQI9VK-HXZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SIMILAR TO PREVIOUS CODE, CHANGING PARAMETERS TO HF FW DATA\n",
        "\n",
        "total_batch_size= 524288  # (nice number) 2**19 ~ 0.5M, in number of tokens\n",
        "B=16   # micro batch size\n",
        "T=1024 # sequence length\n",
        "\n",
        "assert total_batch_size % (B * T * ddp_world_size)== 0, \"Make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
        "grad_accum_steps= total_batch_size // (B * T * ddp_world_size)\n",
        "if master_process: # only the master_process will print it, otherwise all process will print\n",
        "    print(f\"Total desired batch size: {total_batch_size}\")\n",
        "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\"\"\"\n",
        "We have B x T sequences to forward and backward the Transformer but we are not going to do an\n",
        "update. We're goiing to do many forward and backwards and those gradients are all going to be\n",
        "accumulated on the parameter gradients for a single update once all that is accumulated.\n",
        "So, we have to do grad_accum_steps forward backward and then a single update\n",
        "\"\"\"\n",
        "\n",
        "train_loader= DataLoaderLite_FW(B=B, T=T, device=device, process_rank=ddp_rank,\n",
        "                                num_processes=ddp_world_size, split='train')\n",
        "val_loader= DataLoaderLite_FW(B=B, T=T, device=device, process_rank=ddp_rank,\n",
        "                              num_processes=ddp_world_size, split='val')\n",
        "\n",
        "if device== 'cuda': # TF32 computationally more efficient (slightly the same precision of FP32)\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# create model\n",
        "model= GPT2(GPTConfig(vocab_size=50304)).to(device)\n",
        "use_compile= False # # torch.compile interferes with eval and Generation. TODO fix\n",
        "if device== 'cuda' and use_compile:\n",
        "    model= torch.compile(model)\n",
        "if ddp:\n",
        "    # wrap the model into the DDP container\n",
        "    model= DDP(model, device_ids=[ddp_local_rank])\n",
        "raw_model= model.module if ddp else model # always contains the raw unwrapped model\n",
        "\n",
        "learning_rate=6e-4\n",
        "\n",
        "max_lr= learning_rate\n",
        "min_lr= max_lr * 0.1\n",
        "warmup_steps= 715\n",
        "max_steps= 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n",
        "\n",
        "optimizer= raw_model.configure_optimizers(\n",
        "    weight_decay=0.1, learning_rate=learning_rate, device=device\n",
        ")\n",
        "\n",
        "scheduler= Cosine_LR_Decay(optimizer, min_lr, max_lr, warmup_steps, max_steps)\n",
        "\n",
        "self_supervised_training_FW(model, train_loader, val_loader, optimizer, scheduler, steps,\n",
        "                            grad_accum_steps, ddp, ddp_rank, master_process,\n",
        "                            use_compile, eval_interval=200)\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()"
      ],
      "metadata": {
        "id": "BgVAm9UmHXUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0vvU0xxQ8Ea"
      },
      "outputs": [],
      "source": [
        "# https://www.youtube.com/watch?v=l8pRSuU81PU\n",
        "# GPT-2 paper: https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf\n",
        "\"\"\"\n",
        "Very few changes between GPT-2 and GPT-3: the context length was expanded from 1024 to 2048,\n",
        "more datails about training. GPT-2 and GPT-3 are very similar models.\n",
        "\"\"\"\n",
        "# GPT-3 paper: https://arxiv.org/abs/2005.14165"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}