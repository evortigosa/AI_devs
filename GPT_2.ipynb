{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na6RJnHmOMLX"
      },
      "source": [
        "# Building the 124M Parameters GPT-2 Model\n",
        "\n",
        "GPT-2 is a decoder only Transformer model, which means that the encoder and cross-attention modules where missing compared to the figure in **Attention is All You Need**. Also, there are two main differences:\n",
        "- There is a reshuffling of the layer Norms (Sec 2.3 from GPT-2 paper). Layer Norms were moved to the input of each sub-block, similar to a pre-activation residual network.\n",
        "- An additional layer norm was added after the final self-attn block (hight before the final classifier)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhnBi7dgOK7V"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import inspect\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN6iz6y4Q7xP"
      },
      "outputs": [],
      "source": [
        "device= 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBs3dK2gr7RG"
      },
      "source": [
        "# GPT-2 Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyklSBfBQ7z7"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size:int= 1024  # max sequence length\n",
        "    vocab_size:int= 50257 # number of tokens: 50k BPE merges + 256 bytes tokens + 1 <|endoftext|>\n",
        "    n_layer:int= 12       # number of layers\n",
        "    n_head:int= 12        # number of heads\n",
        "    n_embed:int= 768      # embedding dimension\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO8s-KeZctxG"
      },
      "source": [
        "\n",
        "# Input Embedding\n",
        "\n",
        "An embedding is a mapping of discrete objects, such as words, phrases, or documents, into a continuous vector space. In this space, semantically similar items are located close to each other. This is achieved by training machine learning models on large corpora of text data to learn patterns and contexts in which words or phrases appear.\n",
        "\n",
        "- **Token Embedding**: 50257 tokens in the vocabulary size, and for each token we have a 768 dimensional embedding that is the distributed representation that stands in for that tokens. Each token is a little string piece and then the 768 numbers are the vector that represents that token. So, this is our lookup table for tokens.\n",
        "- **Positional Embedding**: 1024 x 768 dimensional lookup table. GPT-2 has a maximum sequence length of 1024; so, we have 1024 positions that each token can be attending to in the past. Everyone of those positions in GPT-2 has a fized vector of 768 weights learned by optimization. Each of the 1024 rows represent the positions of tokens and it is precessed by the Transformer to recover all the relative positions and realizer which token is where and attend to them depending on their position, not just their content. In the **Attention is All You Need** paper the positional embedding is initialized with cosine and sine of different frequencies and it is fixed, but in GPT-2 these are just parameters and they are trained from scratch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfroWJp62v3B"
      },
      "source": [
        "# Decoder Block\n",
        "\n",
        "There is multiple Attention heads inside a Decoder block and they are all functioning in parallel, their outputs are just being concatenated, and that becomes the output of Multi-Headed Attention. The heads are like parallel streams and their outputs get concatenated.\n",
        "\n",
        "**FlashAttention (FA)** is a kernel fusion operation that makes the Attention implementation faster by avoiding loads and stores that torch.compile is not able to optimize. It relies on a \"online softmax\" trick (page 4 of FA paper) that incrementally evaluates softmax.\n",
        "\n",
        "https://arxiv.org/abs/2205.14135\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhgqdFShddUl"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Defining the Attention operation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config) -> None:\n",
        "        super(CausalSelfAttention, self).__init__()\n",
        "        assert config.n_embed % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn= nn.Linear(config.n_embed, 3 * config.n_embed)\n",
        "        # output projection\n",
        "        self.c_proj= nn.Linear(config.n_embed, config.n_embed)\n",
        "        self.c_proj.GPT_SCALE_INIT= 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embed= config.n_embed\n",
        "        # not really a 'bias', more a mask, but following the OpenAI/HF naming\n",
        "        self.register_buffer('bias',\n",
        "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "            .view(1, 1, config.block_size, config.block_size)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C= x.size() # batch_size, sequence_length, embedding dim (n_embed)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is 'number of heads', hs is 'head size', and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head= 12, hs= 64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv= self.c_attn(x)\n",
        "        q, k, v= qkv.split(self.n_embed, dim=2)\n",
        "        k= k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q= q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v= v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        # Attention (materializes the large (T,T) matrix for all the queries and keys)\n",
        "        \"\"\"\n",
        "        # this is the original implementation of Attention\n",
        "        att= (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att= att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att= F.softmax(att, dim=-1)\n",
        "        y= att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        # and the following implements FlashAttention\n",
        "        \"\"\"\n",
        "        y= F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        y= y.transpose(1, 2).contiguous().view(B, T, C) # re-assembly all head outputs side by side\n",
        "        # output projection\n",
        "        y= self.c_proj(y)\n",
        "\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AShDvhV0DSlq"
      },
      "source": [
        "Tokens are lined up in a sequence and each token at this stage of the attention emits three vectors, the query, key, and value. First, the queries and the keys have to multiply each other to get the Attention amount like how interesting they found each other; so, they have to interact multiplicatively.\n",
        "\n",
        "After calculating and splitting qkv, we make the number of heads nh into a batch dimension, and it is a batch dimension just like B. So, in the operations that follow, PyTorch treats B and nh as batches and it applies all the operations on all of them in parallel in both the batches and the heads. Next, queries and keys interact to give us their attention. The autoregressive mask makes sure that the tokens only attend to tokens before them and never to tokens in the future. Softmax normalizes the attention (so, it sums to one). Then, the attention matrix (att) multiply with the values, which is basically a way to do a weighted sum of the values of the tokens that we found interesting at every single tokens. The last transpose operation performs the concatenation operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8n3Lz0Mddba"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Defining the MLP (aka FeedForward).\n",
        "    It is a two linear projectons sandwiched in between the GELU nonlinearity approximating\n",
        "    a tanh (basically like a ReLU except there is no exactly flat tail at zero). Today there is\n",
        "    no good reason to use the approximate version of GELU (but GPT-2 used it).\n",
        "    https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config) -> None:\n",
        "        super(MLP, self).__init__()\n",
        "        self.c_fc=   nn.Linear(config.n_embed, 4 * config.n_embed)\n",
        "        self.gelu=   nn.GELU(approximate='tanh')\n",
        "        self.c_proj= nn.Linear(4 * config.n_embed, config.n_embed)\n",
        "        self.c_proj.GPT_SCALE_INIT= 1\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.c_fc(x)\n",
        "        x= self.gelu(x)\n",
        "        x= self.c_proj(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EarH_AInhgCd"
      },
      "source": [
        "Attention is a communication operation. It is where the tokens and the 1024 positional tokens lined up in a sequence and this is where tokens communicate, this is where they exchange information. Attention is an aggregation function, it is like a pooling (reducing operation) function or a weghted sum function. The MLP happens at every single token individually, i.e., there is no information being collected or exchanged between the tokens. Therefore, the Attention is the reduce and the MLP is the map.\n",
        "\n",
        "The Transformer just ends up being a repeated application of map produce. MLP is thinking about the information that Attention gathered. The sequence of decoder blocks iteratively refines the representation is at the residual stream."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imtYxD-UQ755"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Defining the Decoder. Pre-normalization version.\n",
        "    LayerNorms are before the application of Attention and FeedForward in order to keep\n",
        "    a clean residual stream all the way down to the input tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config) -> None:\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.ln_1= nn.LayerNorm(config.n_embed)\n",
        "        self.attn= CausalSelfAttention(config)\n",
        "        self.ln_2= nn.LayerNorm(config.n_embed)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= x + self.attn(self.ln_1(x))\n",
        "        x= x + self.mlp(self.ln_2(x))\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOKz-Sg935ZQ"
      },
      "source": [
        "# Build the GPT-2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2QG3UDlQ72p"
      },
      "outputs": [],
      "source": [
        "class GPT2(nn.Module):\n",
        "    \"\"\"\n",
        "    GPT-2 architecture skeleton.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config) -> None:\n",
        "        super(GPT2, self).__init__()\n",
        "        self.config= config\n",
        "        # ModuleDict allows us to index submodules using keys just like a dictionary\n",
        "        # ModuleList indexes itens using integers\n",
        "        self.transformer= nn.ModuleDict(dict(\n",
        "            wte= nn.Embedding(config.vocab_size, config.n_embed), # Token Embedding\n",
        "            wpe= nn.Embedding(config.block_size, config.n_embed), # Positional Embedding\n",
        "            h= nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)]),\n",
        "            ln_f= nn.LayerNorm(config.n_embed)\n",
        "        )) # projection from n_embed numbers of embedding dimensions to vocab_size\n",
        "        self.lm_head= nn.Linear(config.n_embed, config.vocab_size, bias=False)\n",
        "        # weight sharing scheme\n",
        "        self.transformer.wte.weight= self.lm_head.weight\n",
        "        # init params by iterating over them\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std= 0.02\n",
        "            if hasattr(module, 'GPT_SCALE_INIT'):\n",
        "                # std scaling down -- we have two residual connections per decoder block\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias) # zeros init is not default in PyTorch\n",
        "        elif isinstance(module, nn.Embedding):    # 0.02 is close of that used in xavier init\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is our token indices of shape (B, T)\n",
        "        B, T= idx.size()\n",
        "        assert T <= self.config.block_size, f'Cannot forward sequence of length {T}, block size is only {self.config.block_size}'\n",
        "        # forward the token and the position embeddings\n",
        "        pos= torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb= self.transformer.wpe(pos) # position embeddings of shape (T, n_embed)\n",
        "        tok_emb= self.transformer.wte(idx) # token embeddings of shape (B, T, n_embed)\n",
        "        x= tok_emb + pos_emb # pos_emb is broadcasted at every row stacked up in tok_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x= block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x= self.transformer.ln_f(x)\n",
        "        logits= self.lm_head(x) # (B, T, vocab_size)\n",
        "        # at every B by T we will calculate the logits for what tokens come next in the sequence\n",
        "        loss= None\n",
        "        if targets is not None:\n",
        "            loss= F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "            # we are flattening out 3D logits tensor into 2D (B*T, vocab_size)\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"\n",
        "        Loads pretrained GPT-2 model weights from HF.\n",
        "        \"\"\"\n",
        "\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print('Loading weights from pretrained GPT: %s' % model_type)\n",
        "\n",
        "        # n_layer, n_head, and n_embed are determined from model_type\n",
        "        config_args= {\n",
        "            'gpt2':        dict(n_layer=12, n_head=12, n_embed=768),  #  124M params\n",
        "            'gpt2-medium': dict(n_layer=24, n_head=16, n_embed=1024), #  350M params,\n",
        "            'gpt2-large':  dict(n_layer=36, n_head=20, n_embed=1280), #  774M params,\n",
        "            'gpt2-xl':     dict(n_layer=48, n_head=25, n_embed=1600), # 1558M params,\n",
        "        }[model_type]\n",
        "        config_args['vocab_size']= 50257 # always 50257 for GPT models\n",
        "        config_args['block_size']= 1024  # always 1024 for GPT models\n",
        "        # create a from-scratch initializated miniGPT model\n",
        "        config= GPTConfig(**config_args)\n",
        "        model= GPT2(config)\n",
        "        sd= model.state_dict()\n",
        "        sd_keys= sd.keys()\n",
        "        sd_keys= [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask\n",
        "\n",
        "        # init a HF/Transformers model\n",
        "        model_hf= GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf= model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf= sd_hf.keys()\n",
        "        sd_keys_hf= [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # just a buffer\n",
        "        sd_keys_hf= [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same\n",
        "        transposed= ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the OpenAI checkpoints use a 'Conv1D' module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"Mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for thr Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
        "        \"\"\"\n",
        "        Splitting up the parameters tha should be weight decayed and those that should not.\n",
        "        Weight decay acts like a regularization because we're pulling down all the weights,\n",
        "        forcing the optimization to more of the weights and not allowing any one of the weights\n",
        "        individually to be way too large (we're forcing the network to distribute the work across)\n",
        "        more channels because there is a pull of gravity on the weights themselves.\n",
        "        \"\"\"\n",
        "        # start with a lot of the candidate parameters (that require grad)\n",
        "        param_dict= {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict= {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any 2D parameters will be weight dacayed, otherwise no\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layerNorms don't\n",
        "        # but most of the parameters will be decayed\n",
        "        decay_params  = [p for n, p in param_dict.items() if p.dim()>= 2]\n",
        "        nodecay_params= [p for n, p in param_dict.items() if p.dim() < 2] # one-dim tensors\n",
        "        optim_groups= [\n",
        "            {'params':   decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params  = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params= sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"Num decayed parameter tensors: {len(decay_params)}, with {num_decay_params} parameters\")\n",
        "        print(f\"Num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params} parameters\")\n",
        "        # create AdamW optimizer and use the fused version of it is available\n",
        "        fused_available= 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        # fused is a lot faster when it is available and when running on cuda\n",
        "        use_fused= fused_available and device== \"cuda\"\n",
        "        print(f\"Using fused AdamW: {use_fused}\")\n",
        "        # create a AdamW PyTorch optimizer -- bug fix of Adam\n",
        "        optimizer= torch.optim.AdamW(\n",
        "            optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused\n",
        "        )\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eQBj848Q78W",
        "outputId": "27c17f2b-d08c-4335-fe35-fda55a0ee6a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x DecoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='tanh')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# creating a model with random initialization\n",
        "model= GPT2(GPTConfig())\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isYKBS46Q7-y",
        "outputId": "0aab24e5-2175-4545-f326-b6f1ede8c6e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters: 124439808\n"
          ]
        }
      ],
      "source": [
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SUTFSaz4Ctn"
      },
      "source": [
        "# Generating Some Text Sequences\n",
        "\n",
        "Loading the weights, biases, and everithing else from HF to our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIJVs6-JJehq",
        "outputId": "d55dc887-aa54-40b0-b7fd-6b4127dce0e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken\n",
        "\n",
        "# prefix tokens using tiktoken\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10xUaelBQ8Bs",
        "outputId": "efcd1d88-be05-44ce-f37f-61fd5a5eb1de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading weights from pretrained GPT: gpt2\n"
          ]
        }
      ],
      "source": [
        "num_return_sequences= 5\n",
        "max_length= 30\n",
        "\n",
        "model= GPT2.from_pretrained('gpt2').to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqpanmK-3Zik",
        "outputId": "e667c592-2b2f-4e38-aca5-1db0c474b04b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Hello, I'm a language model, not a programming platform! I just make decisions based on other projects. I try to do that.\"\n",
            "\n",
            "\n",
            "> Hello, I'm a language model, a kind of a \"first class citizen\" of the world and a person that comes from a much more egalitarian\n",
            "> Hello, I'm a language model, and I'm starting to talk about the notion of the syntax, and I'm also working on an extension that\n",
            "> Hello, I'm a language model, because I'm writing real-time. I'm writing all languages. And I'm working with languages for me\n",
            "> Hello, I'm a language model, I don't know where to begin but I know there is a big deal going on with our society. What\n"
          ]
        }
      ],
      "source": [
        "# getting a list of integer encoding the following string\n",
        "enc= tiktoken.get_encoding('gpt2')\n",
        "tokens= enc.encode(\"Hello, I'm a language model,\")\n",
        "tokens= torch.tensor(tokens, dtype=torch.long) # (8,)\n",
        "tokens= tokens.unsqueeze(0).repeat(num_return_sequences, 1) # (5,8)\n",
        "x= tokens.to(device)\n",
        "\n",
        "# generate! right now x is (B, T -- batch x time) where B= 5, T= 8\n",
        "# set the seed to 42\n",
        "if device== 'cuda':\n",
        "    torch.cuda.manual_seed(42)\n",
        "else:\n",
        "    torch.manual_seed(42)\n",
        "while x.size(1) < max_length:\n",
        "    # forward the model to get the logits -- in every iteration we're going to be adding a\n",
        "    # column of new indices (by resampling) into each one of the rows in x\n",
        "    with torch.no_grad():\n",
        "        logits= model(x) # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        logits= logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs= F.softmax(logits, dim=-1)\n",
        "        # do top-k sampling of 50 (HF pipeline default) -- we only want to keep the top 50\n",
        "        # most likely tokens; so, that way we are never sampling very rare tokens\n",
        "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "        topk_probs, topk_indices= torch.topk(probs, 50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        ix= torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol= torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequence\n",
        "        x= torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens= x[i, :max_length].tolist()\n",
        "    decoded= enc.decode(tokens)\n",
        "    print(\">\", decoded)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2Tr_AgYHWib"
      },
      "source": [
        "# Training the Model from Scratch\n",
        "\n",
        "Using a tine (and nice dataset for debugging) to get us off the ground. We are going to encode the dataset using tiktoken for GPT2 in order to transform our data into sequences of integer tokens, process these token sequences and feed them into a Transformer by rearranging these tokens into the **idx** variable that we're going to be feeding into the Transformer. Specifically, we don't want a single very long onedimensional sequence, but an entire batch where each sequence is up to **T** tokens and **T** cannot be larger than the maximum sequence length (i.e., T-long sequences of tokens and B independent examples of sequences)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHh57H-K3Zlg",
        "outputId": "b16cc085-bc82-49d1-8e34-ec596897544f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-11-14 09:41:57--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  4.05MB/s    in 0.3s    \n",
            "\n",
            "2024-11-14 09:41:58 (4.05 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# read it in to inspect\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text= f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHiWaClxW56L"
      },
      "source": [
        "We can create the data batches by rearranging our dataset using a view operation. Regarding the labels we need for the target to calculate the loss function, we get that as the next token in a sequence (the right of the current one), but notice we don't have a next token for the last one in a sequence, indeed it is the first one of the next sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_WuTVgr3ZoW",
        "outputId": "e266761a-d5e7-4cce-999d-b90e9d6e151a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
            "        [ 5120,   597,  2252,    11,  3285,   502],\n",
            "        [ 2740,    13,   198,   198,  3237,    25],\n",
            "        [  198,  5248,   461,    11,  2740,    13]])\n",
            "tensor([[22307,    25,   198,  8421,   356,  5120],\n",
            "        [  597,  2252,    11,  3285,   502,  2740],\n",
            "        [   13,   198,   198,  3237,    25,   198],\n",
            "        [ 5248,   461,    11,  2740,    13,   198]])\n"
          ]
        }
      ],
      "source": [
        "tokens= enc.encode(text)\n",
        "buf= torch.tensor(tokens[:24 + 1]) # +1 in order to have the last label\n",
        "x= buf[:-1].view(4, 6)\n",
        "y= buf[1:].view(4, 6)\n",
        "\n",
        "print(x) # input tensors\n",
        "print(y) # labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQdZJ7OjuGSI"
      },
      "outputs": [],
      "source": [
        "# making more clean\n",
        "tokens= enc.encode(text)\n",
        "B, T= 4, 32 # batch_size and seq_len\n",
        "buf= torch.tensor(tokens[:B*T + 1]).to(device) # +1 in order to have the last label\n",
        "x= buf[:-1].view(B, T)\n",
        "y= buf[1:].view(B, T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFcYKYs-3Zr-",
        "outputId": "286b022a-47cd-4c9d-ed02-ebe9e3069056"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 32, 50257])\n",
            "tensor(11.0581, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# get logits\n",
        "model= GPT2(GPTConfig()).to(device)\n",
        "model.eval()\n",
        "logits, loss= model(x, y)\n",
        "\n",
        "print(logits.shape)\n",
        "print(loss) # expected loss -ln(1/50257) ~ 10.8249 (cross entropy is the negative log likelihood)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvlSPyPFvSGJ"
      },
      "outputs": [],
      "source": [
        "class DataLoaderLite:\n",
        "\n",
        "    def __init__(self, B, T, data_file, device, process_rank, num_processes) -> None:\n",
        "        self.B= B\n",
        "        self.T= T\n",
        "        self.device= device\n",
        "        self.process_rank= process_rank\n",
        "        self.num_processes= num_processes\n",
        "\n",
        "        # at init load tokens from disk and store them in the memory\n",
        "        with open(data_file, 'r') as f:\n",
        "            text= f.read()\n",
        "        enc= tiktoken.get_encoding('gpt2')\n",
        "        tokens= enc.encode(text)\n",
        "        self.tokens= torch.tensor(tokens)\n",
        "        print(f'Loaded {len(self.tokens)} tokens')\n",
        "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
        "        # state -- we want to stride out all the processes, i.e., process 0 starts at 0,\n",
        "        # but process rank 1 starts at B*T*1, ...\n",
        "        self.current_position= B * T * process_rank\n",
        "\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T= self.B, self.T\n",
        "        buf= self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        x= (buf[:-1]).view(B, T) # inputs\n",
        "        y= (buf[1:]).view(B, T)  # targets\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B * T * self.num_processes\n",
        "        # if loading the next token would be out of bounds, reset\n",
        "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "            self.current_position= B * T * self.process_rank\n",
        "\n",
        "        return x.to(device), y.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOC7KIcg8UGY"
      },
      "outputs": [],
      "source": [
        "class Cosine_LR_Decay:\n",
        "    \"\"\"\n",
        "    Modulates learning rate (LR) based on the iteration number which LR there should be.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, min_lr, max_lr=3e-4, warmup_steps=10,\n",
        "                 max_steps=50) -> None:\n",
        "        self.optimizer= optimizer\n",
        "        self.min_lr= min_lr\n",
        "        self.max_lr= max_lr\n",
        "        self.warmup_steps= warmup_steps\n",
        "        self.max_steps= max_steps\n",
        "        self.last_lr= None\n",
        "\n",
        "\n",
        "    def get_last_lr(self):\n",
        "        return self.last_lr\n",
        "\n",
        "\n",
        "    def get_lr(self, it):\n",
        "        # 1) linear warmup for warmup_iters steps\n",
        "        if it< self.warmup_steps:\n",
        "            return self.max_lr * (it+1) / self.warmup_steps\n",
        "        # 2) if it > lr_decay_iters, return min learning rate\n",
        "        if it> self.max_steps:\n",
        "            return self.min_lr\n",
        "        # 3) in between, use cosine decay down to min learning rate\n",
        "        decay_ratio= (it - self.warmup_steps) / (self.max_steps - self.warmup_steps)\n",
        "        assert 0 <= decay_ratio <= 1\n",
        "        # coeff starts at 1 and goes to 0\n",
        "        coeff= 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "\n",
        "        return self.min_lr + coeff * (self.max_lr - self.min_lr)\n",
        "\n",
        "\n",
        "    def step(self, it):\n",
        "        self.last_lr= self.get_lr(it)\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr']= self.last_lr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s91bvrCx3Zu9"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def self_supervised_training(model, train_loader, optimizer, scheduler, steps,\n",
        "                             grad_accum_steps, ddp, master_process):\n",
        "    # --- training loop ---\n",
        "    for step in range(steps):\n",
        "        start= time.time()\n",
        "        model.train(True)\n",
        "        optimizer.zero_grad()\n",
        "        loss_accum= 0.0\n",
        "\n",
        "        # iterating over all batches accumulating gradients\n",
        "        for micro_step in range(grad_accum_steps):\n",
        "            # --- minibatch construction ---\n",
        "            Xmb, Ymb= train_loader.next_batch()\n",
        "\n",
        "            # --- forward pass and get loss ---\n",
        "            # DDP does the backward pass and synchronizes the gradient\n",
        "            if ddp: # backward_grad_sync only turns on (True) when the micro_step is the last\n",
        "                model.require_backward_grad_sync= (micro_step== grad_accum_steps-1)\n",
        "            logits, loss= model(Xmb, Ymb)\n",
        "\n",
        "            # --- gradient pass to calculate the gradients ---\n",
        "            \"\"\" the loss reduction is the mean by default. We need to compensate it by\n",
        "            the number of gradient accumulation steps before accumulating on each backward().\n",
        "            addition of grads corresponds to a SUM in the objective, but instead of a SUM\n",
        "            we want MEAN. Scale the loss here so it comes out right \"\"\"\n",
        "            loss= loss / grad_accum_steps\n",
        "            loss_accum += loss.detach()\n",
        "            loss.backward() # this is a plus equals, i.e., accumulates the grads\n",
        "        if ddp:\n",
        "            \"\"\" reduce the accum loss over all the processes to the average over that loss\n",
        "            the loss_accum tensor exists on all the ranks, when we call all_reduce of AVG it\n",
        "            creates the average of those numbers and deposits that average on all the ranks\"\"\"\n",
        "            dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
        "        \"\"\" calculating the global norm of the parameters to preventing the model from getting\n",
        "        too big shocks in terms of gradient magnitude \"\"\"\n",
        "        norm= torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        # determine and set the learning rate for this interaction\n",
        "        scheduler.step(step)\n",
        "\n",
        "        # --- update the parameters ---\n",
        "        optimizer.step()\n",
        "\n",
        "        # --- evaluation and track stats ---\n",
        "        if Xmb.device.type== 'cuda':\n",
        "            torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "        end= time.time()\n",
        "        dt= end - start # time difference in seconds\n",
        "        tokens_processed= train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
        "        tokens_per_sec= tokens_processed / dt\n",
        "        if master_process:\n",
        "            print(f\"Step {step:4d} | loss: {loss_accum.item():.4f} | lr: {scheduler.get_last_lr():.3e} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1l2rpRdZ120"
      },
      "source": [
        "TF32 uses the same 10-bit mantissa as the half-precision (FP16) math, shown to have more than sufficient margin for the precision requirements of AI workloads. And TF32 adopts the same 8-bit exponent as FP32 so it can support the same numeric range.\n",
        "\n",
        "Torch.compile(model) will analyze the entire model and look at what operations we like to use and with the benefit of knowing exactly what is going to happen. It materializes all the operations as it goes through and the calculations are dispatched and run in order. Specifically, the Python interpreter does not know what kind of operations are going to happen later, but torch.compile sees your code entirely at the same time, and it is able to know what operations are intended to run, and it will optimize that process. As a result, torch.compile significantly optimizes the round trips between GPU cores and memory, what is called **kernel fusion** (a major way in speed up).\n",
        "\n",
        "**Values optimization:** fixing ugly numbers (odds or not power of two) through the model. We increase a value to the next nice number.\n",
        "- vocab_size: from 50257 (ugly) to 50304 (nice). Additional \"fake\" tokens we are introducing will never be used and have to be driven to zero in probability.\n",
        "\n",
        "**Gradient accumulation:** simulate in a serial way any arbitrary batch size that we set and so we can do a very large batch size running longer and processing multiple sequences and adding up all the gradients from them to simulate a large batch size.\n",
        "\n",
        "**DDP:** in a forward pass it behaves identically, nothing should be changedd in the forward pass, but in the backward pass, once the backward passes over on each independent GPU, each of them has the gradient for all the parameters and what DDP does is once the backward pass is over it will call the \"all reduce\" and it does an average across all the ranks of their gradients. Then, it will deposit that average on every single, which ends up with with the average on it. So, DDP synchronizes and averages the gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWeQW2yNbEMj",
        "outputId": "be4c7991-8d24-4608-bc60-8e391979ebdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting triton\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton\n",
            "Successfully installed triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install triton"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "simple launch:\n",
        "python file_name.py\n",
        "DDP launch for 8 GPUs:\n",
        "torchrun --standalone --nproc_per_node=8 file_name.py\n",
        "\"\"\"\n",
        "\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.distributed as dist\n",
        "\n",
        "# set up DDP (distributed data parallel)\n",
        "# torchrun command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n",
        "ddp= int(os.environ.get('RANK', -1))!= -1\n",
        "print(f\"Is this a ddp run? {ddp}\")\n",
        "if ddp:\n",
        "    # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
        "    assert torch.cuda.is_available(), \"For now we need CUDA for DDP\"\n",
        "    init_process_group(backend='nccl')\n",
        "    # create environ where each of these processes can look up\n",
        "    ddp_rank= int(os.environ['RANK'])\n",
        "    ddp_local_rank= int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size= int(os.environ['WORLD_SIZE'])\n",
        "    device= f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process= ddp_rank== 0 # this process will do logging, checkpoiniting\n",
        "else:\n",
        "    # vanilla non-DDP run\n",
        "    ddp_rank= 0\n",
        "    ddp_local_rank= 0\n",
        "    ddp_world_size= 1\n",
        "    master_process= True\n",
        "    # attempt to autodetect device\n",
        "    device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n"
      ],
      "metadata": {
        "id": "swHTOlxrkkyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx_PKsJn3ZyT",
        "outputId": "e296c838-7f05-4657-a3c3-923f8d8e7018"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Is this a ddp run? False\n",
            "Total desired batch size: 128\n",
            "=> calculated gradient accumulation steps: 1\n",
            "Loaded 338025 tokens\n",
            "1 epoch = 2640 batches\n",
            "Num decayed parameter tensors: 50, with 124354560 parameters\n",
            "Num non-decayed parameter tensors: 98, with 121344 parameters\n",
            "Using fused AdamW: False\n",
            "Step    0 | loss: 10.9521 | lr: 6.000e-05 | dt: 3944.84ms | tok/sec: 32.4\n",
            "Step    1 | loss: 9.9007 | lr: 1.200e-04 | dt: 2760.42ms | tok/sec: 46.4\n",
            "Step    2 | loss: 9.0254 | lr: 1.800e-04 | dt: 3440.63ms | tok/sec: 37.2\n",
            "Step    3 | loss: 9.2049 | lr: 2.400e-04 | dt: 2901.61ms | tok/sec: 44.1\n",
            "Step    4 | loss: 8.9321 | lr: 3.000e-04 | dt: 2772.40ms | tok/sec: 46.2\n",
            "Step    5 | loss: 8.6602 | lr: 3.600e-04 | dt: 2757.85ms | tok/sec: 46.4\n",
            "Step    6 | loss: 9.1818 | lr: 4.200e-04 | dt: 2967.16ms | tok/sec: 43.1\n",
            "Step    7 | loss: 8.9422 | lr: 4.800e-04 | dt: 3407.10ms | tok/sec: 37.6\n",
            "Step    8 | loss: 8.1773 | lr: 5.400e-04 | dt: 2754.96ms | tok/sec: 46.5\n",
            "Step    9 | loss: 7.9806 | lr: 6.000e-04 | dt: 2860.93ms | tok/sec: 44.7\n"
          ]
        }
      ],
      "source": [
        "total_batch_size= 4*32\n",
        "B= 4   # micro batch size\n",
        "T= 32  # sequence length\n",
        "\n",
        "assert total_batch_size % (B * T * ddp_world_size)== 0, \"Make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
        "grad_accum_steps= total_batch_size // (B * T * ddp_world_size)\n",
        "if master_process: # only the master_process will print it, otherwise all process will print\n",
        "    print(f\"Total desired batch size: {total_batch_size}\")\n",
        "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\"\"\"\n",
        "We have B x T sequences to forward and backward the Transformer but we are not going to do an\n",
        "update. We're goiing to do many forward and backwards and those gradients are all going to be\n",
        "accumulated on the parameter gradients for a single update once all that is accumulated.\n",
        "So, we have to do grad_accum_steps forward backward and then a single update\n",
        "\"\"\"\n",
        "\n",
        "train_loader= DataLoaderLite(B=B, T=T, data_file='input.txt', device=device,\n",
        "                             process_rank=ddp_rank, num_processes=ddp_world_size)\n",
        "\n",
        "if device== 'cuda': # TF32 computationally more efficient (slightly the same precision of FP32)\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# create model\n",
        "model= GPT2(GPTConfig(vocab_size=50304)).to(device)\n",
        "if device== 'cuda':\n",
        "    model= torch.compile(model)\n",
        "if ddp:\n",
        "    # wrap the model into the DDP container\n",
        "    model= DDP(model, device_ids=[ddp_local_rank])\n",
        "raw_model= model.module if ddp else model # always contains the raw unwrapped model\n",
        "\n",
        "learning_rate=6e-4\n",
        "steps=10\n",
        "\n",
        "max_lr= learning_rate\n",
        "min_lr= max_lr * 0.1\n",
        "warmup_steps= 10\n",
        "max_steps= steps\n",
        "\n",
        "optimizer= raw_model.configure_optimizers(\n",
        "    weight_decay=0.1, learning_rate=learning_rate, device=device\n",
        ")\n",
        "\n",
        "scheduler= Cosine_LR_Decay(optimizer, min_lr, max_lr, warmup_steps, max_steps)\n",
        "\n",
        "self_supervised_training(model, train_loader, optimizer, scheduler, steps,\n",
        "                         grad_accum_steps, ddp, master_process)\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FineWeb-Edu dataset\n",
        "\n",
        "Using the sample-10BT: a subset randomly sampled from the whole dataset of around 10B gpt2 tokens.\n",
        "\n",
        "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu"
      ],
      "metadata": {
        "id": "vSL2AEIWVtW_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_53UtZLjU3dn"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "DATA HANDLER\n",
        "Downloading the dataset, tokenizing all the documents inside this dataset, starting with an eot\n",
        "token, encoding using uint16 to save space. Then, the tokenized datased is saved into shards (numpy\n",
        "files very similar to tensors).\n",
        "\"\"\"\n",
        "\n",
        "!pip3 install datasets\n",
        "\n",
        "import multiprocessing as mp\n",
        "from datasets import load_dataset\n",
        "\n",
        "local_dir= \"edu_fineweb10B\"\n",
        "remote_name= \"sample-10BT\"\n",
        "shard_size= int(1e8) # 100M tokens per shard, total of 100 shard files\n",
        "\n",
        "# create the cache the local directory if it doesn't exist yet\n",
        "#DATA_CACHE_DIR= os.path.join(os.path.dirname('__init__'), local_dir)\n",
        "DATA_CACHE_DIR= os.path.join(os.path.dirname('data/'), local_dir)\n",
        "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# download the dataset\n",
        "fw= load_dataset(\"HuggingFaceFW/fineweb-edu\", name=remote_name, split=\"train\")\n",
        "\n",
        "# init the tokenizer\n",
        "enc= tiktoken.get_encoding(\"gpt2\")\n",
        "eot= enc._special_tokens['<|endoftext|>'] # end of text token\n",
        "def tokenize(doc):\n",
        "    # tokenizes a single document and returns a numpy array of uint16 tokens\n",
        "    tokens= [eot] # the special <|endoftext|> token delimits all documents\n",
        "    tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n",
        "    tokens_np= np.array(tokens)\n",
        "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
        "    tokens_np_uint16= tokens_np.astype(np.uint16)\n",
        "    return tokens_np_uint16\n",
        "\n",
        "def write_datafile(filename, tokens_np):\n",
        "    np.save(filename, tokens_np)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3CCq4dEJD2Vs"
      },
      "outputs": [],
      "source": [
        "# tokenize all documents and write output shards, each of shard_size tokens (last shard has remainder)\n",
        "nprocs= max(1, os.cpu_count()//2)\n",
        "with mp.Pool(nprocs) as pool:\n",
        "    shard_index= 0\n",
        "    # preallocate buffer to hold current shard\n",
        "    all_tokens_np= np.empty((shard_size,), dtype=np.uint16)\n",
        "    token_count= 0\n",
        "    progress_bar= None\n",
        "    for tokens in pool.imap(tokenize, fw, chunksize=16):\n",
        "\n",
        "        # is there enough space in the current shard for the new tokens?\n",
        "        if token_count + len(tokens) < shard_size:\n",
        "            # simply append tokens to current shard\n",
        "            all_tokens_np[token_count:token_count+len(tokens)]= tokens\n",
        "            token_count += len(tokens)\n",
        "            # update progress bar\n",
        "            if progress_bar is None:\n",
        "                progress_bar= tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
        "            progress_bar.update(len(tokens))\n",
        "        else:\n",
        "            # write the current shard and start a new one\n",
        "            split= \"val\" if shard_index == 0 else \"train\"\n",
        "            filename= os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
        "            # split the document into whatever fits in this shard; the remainder goes to next one\n",
        "            remainder= shard_size - token_count\n",
        "            progress_bar.update(remainder)\n",
        "            all_tokens_np[token_count:token_count+remainder]= tokens[:remainder]\n",
        "            write_datafile(filename, all_tokens_np)\n",
        "            shard_index += 1\n",
        "            progress_bar= None\n",
        "            # populate the next shard with the leftovers of the current doc\n",
        "            all_tokens_np[0:len(tokens)-remainder] = tokens[remainder:]\n",
        "            token_count= len(tokens)-remainder\n",
        "\n",
        "    # write any remaining tokens as the last shard\n",
        "    if token_count != 0:\n",
        "        split= \"val\" if shard_index == 0 else \"train\"\n",
        "        filename= os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
        "        write_datafile(filename, all_tokens_np[:token_count])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified DataLoader for iterating over all the shards from HF FineWeb.\n",
        "\n",
        "TODO: randomly permute the documents in every single shard or every single new epoch. It is better for the optimization so that we are not seeing thing identically, by introducing some randomness in how the documents follow each other. This strategy break up the dependence between two consecutive documents because it's a kind of spurious correlation."
      ],
      "metadata": {
        "id": "n1PE93ieV8ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_tokens(filename):\n",
        "    npt= np.load(filename)\n",
        "    npt= npt.astype(np.int32)\n",
        "    ptt= torch.tensor(npt, dtype=torch.long)\n",
        "\n",
        "    return ptt\n",
        "\n",
        "\n",
        "class DataLoaderLite_FW:\n",
        "\n",
        "    def __init__(self, B, T, device, process_rank, num_processes, split) -> None:\n",
        "        self.B= B\n",
        "        self.T= T\n",
        "        self.device= device\n",
        "        self.process_rank= process_rank\n",
        "        self.num_processes= num_processes\n",
        "        assert split in {'train', 'val'}\n",
        "\n",
        "        # get the shard filenames\n",
        "        data_root= 'edu_fineweb10B'\n",
        "        shards= os.listdir(data_root)\n",
        "        shards= [s for s in shards if split in s]\n",
        "        shards= sorted(shards)\n",
        "        shards= [os.path.join(data_root, s) for s in shards]\n",
        "        self.shards= shards\n",
        "        assert len(shards) > 0, f'No shards found for split {split}'\n",
        "        if master_process:\n",
        "            print(f'Found {len(shards)} shards for split {split}')\n",
        "        self.reset()\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        # state, init at shard zero\n",
        "        self.current_shard= 0\n",
        "        self.tokens= load_tokens(self.shards[self.current_shard])\n",
        "        self.current_posistion= self.B * self.T * self.process_rank\n",
        "\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T= self.B, self.T\n",
        "        buf= self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "        x= (buf[:-1]).view(B, T) # inputs\n",
        "        y= (buf[1:]).view(B, T)  # targets\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B * T * self.num_processes\n",
        "        # if loading the next token would be out of bounds, reset\n",
        "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "            self.current_position= B * T * self.process_rank\n",
        "\n",
        "        return x.to(device), y.to(device)\n"
      ],
      "metadata": {
        "id": "TzlX-RMCLEx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation Loss** measures how good we are at predicting the next token in a sequence on some validation data that the model has not seen during training."
      ],
      "metadata": {
        "id": "6MOtCS2mDLHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_supervised_training_FW(model, train_loader, val_loader, optimizer, scheduler, steps,\n",
        "                                grad_accum_steps, ddp, master_process, use_compile):\n",
        "\n",
        "    eval_interval= 200\n",
        "\n",
        "    for step in range(steps):\n",
        "        start= time.time()\n",
        "        last_step= (step== steps - 1)\n",
        "\n",
        "        # --- once in a while evaluate our validation loss ---\n",
        "        if step % eval_interval== 0 or last_step:\n",
        "            model.eval()\n",
        "            val_loader.reset()\n",
        "            with torch.no_grad():\n",
        "                val_loss_accum= 0.0\n",
        "                val_loss_steps= 20\n",
        "                for _ in range(val_loss_steps):\n",
        "                    x, y= val_loader.next_batch()\n",
        "                    logits, loss= model(x, y)\n",
        "                    loss= loss / val_loss_steps\n",
        "                    val_loss_accum += loss.detach()\n",
        "            if ddp:\n",
        "                dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)\n",
        "            if master_process:\n",
        "                print(f\"Validation loss: {val_loss_accum.item():.4f}\")\n",
        "\n",
        "\n",
        "        # --- once in a while generate from the model (except step 0, which is noise) ---\n",
        "        if ((step> 0 and step % eval_interval== 0) or last_step) and (not use_compile):\n",
        "            model.eval()\n",
        "            pass\n",
        "\n",
        "\n",
        "        # --- training loop ---\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        loss_accum= 0.0\n",
        "\n",
        "        # iterating over all batches accumulating gradients\n",
        "        for micro_step in range(grad_accum_steps):\n",
        "            # --- minibatch construction ---\n",
        "            Xmb, Ymb= train_loader.next_batch()\n",
        "\n",
        "            # --- forward pass and get loss ---\n",
        "            # DDP does the backward pass and synchronizes the gradient\n",
        "            if ddp: # backward_grad_sync only turns on (True) when the micro_step is the last\n",
        "                model.require_backward_grad_sync= (micro_step== grad_accum_steps-1)\n",
        "            logits, loss= model(Xmb, Ymb)\n",
        "\n",
        "            # --- gradient pass to calculate the gradients ---\n",
        "            \"\"\" the loss reduction is the mean by default. We need to compensate it by\n",
        "            the number of gradient accumulation steps before accumulating on each backward().\n",
        "            addition of grads corresponds to a SUM in the objective, but instead of a SUM\n",
        "            we want MEAN. Scale the loss here so it comes out right \"\"\"\n",
        "            loss= loss / grad_accum_steps\n",
        "            loss_accum += loss.detach()\n",
        "            loss.backward() # this is a plus equals, i.e., accumulates the grads\n",
        "        if ddp:\n",
        "            \"\"\" reduce the accum loss over all the processes to the average over that loss\n",
        "            the loss_accum tensor exists on all the ranks, when we call all_reduce of AVG it\n",
        "            creates the average of those numbers and deposits that average on all the ranks\"\"\"\n",
        "            dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
        "        \"\"\" calculating the global norm of the parameters to preventing the model from getting\n",
        "        too big shocks in terms of gradient magnitude \"\"\"\n",
        "        norm= torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        # determine and set the learning rate for this interaction\n",
        "        scheduler.step(step)\n",
        "\n",
        "        # --- update the parameters ---\n",
        "        optimizer.step()\n",
        "\n",
        "        # --- evaluation and track stats ---\n",
        "        if Xmb.device.type== 'cuda':\n",
        "            torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "        end= time.time()\n",
        "        dt= end - start # time difference in seconds\n",
        "        tokens_processed= train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
        "        tokens_per_sec= tokens_processed / dt\n",
        "        if master_process:\n",
        "            print(f\"Step {step:4d} | loss: {loss_accum.item():.4f} | lr: {scheduler.get_last_lr():.3e} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.1f}\")\n"
      ],
      "metadata": {
        "id": "yXQI9VK-HXZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SIMILAR TO PREVIOUS CODE, CHANGING PARAMETERS TO HF FW DATA\n",
        "\n",
        "total_batch_size= 524288  # (nice number) 2**19 ~ 0.5M, in number of tokens\n",
        "B=16   # micro batch size\n",
        "T=1024 # sequence length\n",
        "\n",
        "assert total_batch_size % (B * T * ddp_world_size)== 0, \"Make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
        "grad_accum_steps= total_batch_size // (B * T * ddp_world_size)\n",
        "if master_process: # only the master_process will print it, otherwise all process will print\n",
        "    print(f\"Total desired batch size: {total_batch_size}\")\n",
        "    print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\"\"\"\n",
        "We have B x T sequences to forward and backward the Transformer but we are not going to do an\n",
        "update. We're goiing to do many forward and backwards and those gradients are all going to be\n",
        "accumulated on the parameter gradients for a single update once all that is accumulated.\n",
        "So, we have to do grad_accum_steps forward backward and then a single update\n",
        "\"\"\"\n",
        "\n",
        "train_loader= DataLoaderLite_FW(B=B, T=T, device=device, process_rank=ddp_rank,\n",
        "                                num_processes=ddp_world_size, split='train')\n",
        "val_loader= DataLoaderLite_FW(B=B, T=T, device=device, process_rank=ddp_rank,\n",
        "                              num_processes=ddp_world_size, split='val')\n",
        "\n",
        "if device== 'cuda': # TF32 computationally more efficient (slightly the same precision of FP32)\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# create model\n",
        "model= GPT2(GPTConfig(vocab_size=50304)).to(device)\n",
        "use_compile= False # # torch.compile interferes with eval and Generation. TODO fix\n",
        "if device== 'cuda' and use_compile:\n",
        "    model= torch.compile(model)\n",
        "if ddp:\n",
        "    # wrap the model into the DDP container\n",
        "    model= DDP(model, device_ids=[ddp_local_rank])\n",
        "raw_model= model.module if ddp else model # always contains the raw unwrapped model\n",
        "\n",
        "learning_rate=6e-4\n",
        "\n",
        "max_lr= learning_rate\n",
        "min_lr= max_lr * 0.1\n",
        "warmup_steps= 715\n",
        "max_steps= 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n",
        "\n",
        "optimizer= raw_model.configure_optimizers(\n",
        "    weight_decay=0.1, learning_rate=learning_rate, device=device\n",
        ")\n",
        "\n",
        "scheduler= Cosine_LR_Decay(optimizer, min_lr, max_lr, warmup_steps, max_steps)\n",
        "\n",
        "self_supervised_training_FW(model, train_loader, optimizer, scheduler, max_steps,\n",
        "                            grad_accum_steps, ddp, master_process, use_compile)\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()"
      ],
      "metadata": {
        "id": "BgVAm9UmHXUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0vvU0xxQ8Ea"
      },
      "outputs": [],
      "source": [
        "# https://www.youtube.com/watch?v=l8pRSuU81PU\n",
        "# GPT-2 paper: https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf\n",
        "\"\"\"\n",
        "Very few changes between GPT-2 and GPT-3: the context length was expanded from 1024 to 2048,\n",
        "more datails about training. GPT-2 and GPT-3 are very similar models.\n",
        "\"\"\"\n",
        "# GPT-3 paper: https://arxiv.org/abs/2005.14165"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}