{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnvueT1u39I9"
      },
      "source": [
        "# Graph Neural Networks (GNNs)\n",
        "\n",
        "From MLPs to GCNs and GATs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E_N4RFLj4VBo"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t0MFHexDsf7m"
      },
      "outputs": [],
      "source": [
        "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZKtmH6IAa_A"
      },
      "source": [
        "The Cora dataset is a benchmark dataset for graph neural networks. The dataset contains data about 2708 scientific publications. These publications are the nodes of the graph. An edge between nodes (publications) is created when a publication references the other one. The target is to predict the subject of each paper, there are seven classes in total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCfNS47i7Isc"
      },
      "outputs": [],
      "source": [
        "!pip install torch-geometric\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "dataset= Planetoid(root='.', name='Cora', force_reload=True)\n",
        "data= dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_efh5OV4OrJQ"
      },
      "outputs": [],
      "source": [
        "data_size= data.x.shape[0]\n",
        "dev_size= 500\n",
        "test_size= 500\n",
        "train_size= data_size - dev_size - test_size\n",
        "\n",
        "train_mask= torch.tensor([i< train_size for i in range(data_size)])\n",
        "dev_mask= torch.tensor([i>= train_size and i< (data_size - test_size) for i in range(data_size)])\n",
        "test_mask= torch.tensor([i>= (train_size + dev_size) for i in range(data_size)])\n",
        "\n",
        "data.train_mask= train_mask\n",
        "data.val_mask= dev_mask\n",
        "data.test_mask= test_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sCXwB8WLA2PV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9939b3f2-d4c6-4dea-8ba3-67d2c1bf339d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1433, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data= data.to(device)\n",
        "\n",
        "Xtr, Ytr= data.x[data.train_mask], data.y[data.train_mask]\n",
        "Xdev, Ydev= data.x[data.val_mask], data.y[data.val_mask]\n",
        "Xte, Yte= data.x[data.test_mask], data.y[data.test_mask]\n",
        "\n",
        "edge_idx= data.edge_index\n",
        "\n",
        "num_inputs= data.x.shape[1]                # used for input_dim\n",
        "num_labels= len(set(data.y.cpu().numpy())) # used for output_dim\n",
        "\n",
        "num_inputs, num_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE0voRYS4PWK"
      },
      "source": [
        "# Neural Network - MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Hkgvp70V32ub"
      },
      "outputs": [],
      "source": [
        "class MLPLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements one customizable CNN layer.\n",
        "    Input -> Linear -> Norm -> ReLU -> Dropout -> Output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, layer_norm=True, activation=None,\n",
        "                 dropout=0.0) -> None:\n",
        "        super(MLPLayer, self).__init__()\n",
        "        # Fully connected (FC) layer\n",
        "        self.fc_layer= nn.Linear(input_dim, output_dim)\n",
        "        # Normalization\n",
        "        self.norm= nn.LayerNorm(output_dim) if layer_norm else None\n",
        "        # Activation function\n",
        "        self.activ= activation\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout= nn.Dropout(p=dropout) if dropout> 0.0 else None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.fc_layer(x)\n",
        "        if self.norm is not None:\n",
        "            x= self.norm(x)\n",
        "        if self.activ is not None:\n",
        "            x= self.activ(x)\n",
        "        if self.dropout is not None:\n",
        "            x= self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a customizable MLP.\n",
        "    nn.ReLU(inplace=True) is the default activation function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim=[16,], output_dim=1, layer_norm=False,\n",
        "                 activation=None, dropout=0.0) -> None:\n",
        "        super(MLP, self).__init__()\n",
        "        # Activation function -- ReLU is the default\n",
        "        activation= nn.ReLU(inplace=True) if activation is None else activation\n",
        "\n",
        "        if isinstance(hidden_dim, int):\n",
        "            hidden_dim= [hidden_dim]\n",
        "        n_hidden_layers= len(hidden_dim)\n",
        "\n",
        "        if n_hidden_layers== 0:\n",
        "            raise Exception('hidden_dim cannot be an empty list')\n",
        "\n",
        "        self.fc_in= MLPLayer(input_dim, hidden_dim[0], layer_norm, activation, dropout)\n",
        "\n",
        "        if n_hidden_layers> 1:\n",
        "            self.fc_hn= nn.Sequential(*[\n",
        "                MLPLayer(d, hidden_dim[i+1], layer_norm, activation, dropout)\n",
        "                for i, d in enumerate(hidden_dim[:-1])\n",
        "            ])\n",
        "        else: self.fc_hn= None\n",
        "\n",
        "        self.fc_out= MLPLayer(hidden_dim[-1], output_dim, layer_norm=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):  # no graph structure, only node features\n",
        "        x= self.fc_in(x)\n",
        "        if self.fc_hn is not None:\n",
        "            x= self.fc_hn(x)\n",
        "        x= self.fc_out(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSF9E-O5A2Sx",
        "outputId": "513d1a73-af77-448d-8d77-daa35573cfb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 46183\n"
          ]
        }
      ],
      "source": [
        "model= MLP(input_dim=num_inputs, hidden_dim=[32,], output_dim=num_labels,\n",
        "           layer_norm=True, dropout=0.1).to(device)\n",
        "\n",
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5xEhMmpVsxw"
      },
      "source": [
        "# Graph Convolutional Network - GCN\n",
        "\n",
        "There are three common types of prediction tasks in graphs:\n",
        "- We can predict on graph level. The input of the model is many different graphs, and every graph gets one classification. For example the class a molecule belongs to: every molecule is represented by one graph, and every molecule needs a prediction. Another example is image classification. Yes, images can also be represented as graphs!\n",
        "- Another way to use GNNs is by predicting on node level. The input of the GNN is one graph, and every node needs a prediction. This prediction is a characteristic of the node. Node regression is of course possible as well. Compared to classification, we only need to change the output layer activation function, the loss function, evaluation metric, and obviously the target.\n",
        "- Finally, we can predict on edge level. The value of an edge is predicted, or the likelihood of an edge that will appear soon. An example is recommended friends on social media (a.k.a. link prediction).\n",
        "\n",
        "For understanding one node, we need to look at its neighborhood and include that information in the GCN. The idea of graph convolution is to allow each node to pass messages to neighboring nodes, such that the content of each node has the ability to interact with neighboring nodes. This happens in an iterative loop, which can be conceptualized as a \"layer\" of the model. The modified messages from one layer can be passed to another layer for further message passing. This can be done in as many layers as we desire. The number of layers we have dictates how far into the distance a particular node can \"see\".\n",
        "\n",
        "Just like how, in a traditional convolutional network, we can stack layers of convolutions on top of each other to create more in-depth understanding; in graph convolutional networks, we can stack layers of these neural networks on top of one another which modify how signals are passed."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing a GCNConv module from scratch\n",
        "\n",
        "A Graph Convolutional Network (https://arxiv.org/abs/1609.02907) is described by:\n",
        "\n",
        "$$\n",
        "H^{(l+1)} = \\sigma (\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)} W^{(l)}).\n",
        "$$\n",
        "\n",
        "Here $H$ represents the \"hidden state\" of each node in our graph, i.e. the vector associated with each node. We are creating a new hidden state for our nodes ($H^{(l+1)}$, $l+1$ symbolizing the next layer) based on some math and the previous hidden state ($H^{(l)}$, $l$ symbolizing the current layer). The hidden state, in this example, is formatted as a matrix, where each row of the matrix corresponds with a particular node.\n",
        "\n",
        "Our input hidden state is multiplied by a weight matrix and then passed through an activation function. The way matrix multiplication shakes out, this is equivalent to passing our data through a neural network where the parameters of that neural network are within the matrix $W$."
      ],
      "metadata": {
        "id": "wEPmK4y8qjqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a GCNConv module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim) -> None:\n",
        "        super(GCNConv, self).__init__()\n",
        "        # Learnable linear transformation\n",
        "        self.W= nn.Linear(input_dim, output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        num_nodes= x.size(0)\n",
        "        device= x.device\n",
        "\n",
        "        # create the adjacency matrix (including self-loops)\n",
        "        self_loops= torch.arange(num_nodes, device=device).unsqueeze(0).repeat(2, 1)\n",
        "        edge_index= torch.cat([edge_index, self_loops], dim=1)\n",
        "        # extract source (row) and destination (col) indices from edge_index\n",
        "        row, col= edge_index\n",
        "\n",
        "        # construct the adjacency matrix\n",
        "        A= torch.zeros((num_nodes, num_nodes), dtype=x.dtype, device=device)\n",
        "        A[row, col]= 1  # fill the adjacency matrix\n",
        "\n",
        "        # compute degree matrix\n",
        "        deg= torch.bincount(row, minlength=num_nodes).float()\n",
        "        # compute D^(-1/2) avoiding division by zero\n",
        "        deg_inv_sqrt= deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg == 0]= 0\n",
        "\n",
        "        # calculate D^(-1/2) * A * D^(-1/2)\n",
        "        D_inv_sqrt= torch.diag(deg_inv_sqrt)\n",
        "        A_norm= D_inv_sqrt @ A @ D_inv_sqrt  # normalized A\n",
        "\n",
        "        # propagate node features through the normalized adjacency matrix\n",
        "        x= A_norm @ x\n",
        "\n",
        "        return self.W(x)  # apply the learned transformation\n"
      ],
      "metadata": {
        "id": "icxUfah0qk7X"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is one important step we should take before actually implementing a GCN, and that is normalization. Imagine, without normalization, nodes with more connections (e.g. one node having 10 neighbors vs. another with just 1) can dominate the learning process. The node with 10 neighbors would aggregate far more information than the one with 1, leading to imbalance and unstable learning. Normalization ensures that each node's contribution is appropriately scaled, so the network learns from the graph structure rather than being skewed by uneven data distribution. In GCNs it's common to use symmetric normalization:\n",
        "\n",
        "$$\n",
        "\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}\n",
        "$$\n",
        "\n",
        "We do this because matrix multiplication is fundamentally asymmetrical. The rows of the first matrix get multiplied by the columns of the second matrix. If we only multiplied by the inverse of our degree matrix once, either before or after the adjacency matrix, we would asymmetrically be normalizing either rows or columns. By multiplying in this way, we are scaling down our adjacency values without a preference for rows or columns before using that to aggregate values in the hidden matrix.\n",
        "\n",
        "The idea is to normalize each node's aggregated features by the square root of its degree (the number of neighbors, including itself for self-loops). This helps to ensure that nodes with different degrees contribute equally during aggregation.\n",
        "\n",
        "Once we have normalized the adjacency matrix by the degree matrix, we can then multiply it by $H$ to do message passing, then we can multiply the result of that through the neural network by multiplying it by $W$ , and then through the activation function."
      ],
      "metadata": {
        "id": "m56bLykQzYRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric.nn as gnn\n",
        "\n",
        "class GCNLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements one customizable GCN layer.\n",
        "    Input -> GCNConv -> ReLU -> Dropout -> Output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, activation=None, dropout=0.0, my_gcn=False) -> None:\n",
        "        super(GCNLayer, self).__init__()\n",
        "        # GCN module\n",
        "        if my_gcn:\n",
        "            self.gcn_layer= GCNConv(input_dim, output_dim)\n",
        "        else:\n",
        "            self.gcn_layer= gnn.GCNConv(input_dim, output_dim)\n",
        "        # Activation function\n",
        "        self.activ= activation\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout= nn.Dropout(p=dropout) if dropout> 0.0 else None\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index= data\n",
        "\n",
        "        x= self.gcn_layer(x, edge_index)\n",
        "        if self.activ is not None:\n",
        "            x= self.activ(x)\n",
        "        if self.dropout is not None:\n",
        "            x= self.dropout(x)\n",
        "\n",
        "        return x, edge_index\n"
      ],
      "metadata": {
        "id": "lNliY1o3oXUL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FeDfgo0LNitv"
      },
      "outputs": [],
      "source": [
        "class GCN(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementing a Graph Convolutional Network.\n",
        "    nn.ReLU(inplace=True) is the default activation function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim=[16,], output_dim=1, activation=None,\n",
        "                 dropout=0.0, my_gcn=False) -> None:\n",
        "        super(GCN, self).__init__()\n",
        "        # Activation function -- ReLU is the default\n",
        "        activation= nn.ReLU(inplace=True) if activation is None else activation\n",
        "\n",
        "        if isinstance(hidden_dim, int):\n",
        "            hidden_dim= [hidden_dim]\n",
        "        n_hidden_layers= len(hidden_dim)\n",
        "\n",
        "        if n_hidden_layers== 0:\n",
        "            raise Exception('hidden_dim cannot be an empty list')\n",
        "\n",
        "        self.gcn_in= GCNLayer(input_dim, hidden_dim[0], activation, dropout, my_gcn)\n",
        "\n",
        "        if n_hidden_layers> 1:\n",
        "            self.gcn_hn= nn.Sequential(*[\n",
        "                GCNLayer(hidden_dim[i], hidden_dim[i+1], activation, dropout, my_gcn)\n",
        "                for i in range(n_hidden_layers - 1)\n",
        "            ])\n",
        "        else:\n",
        "            self.gcn_hn= None\n",
        "\n",
        "        self.gcn_out= GCNLayer(hidden_dim[-1], output_dim, my_gcn=my_gcn)\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        data= (x, edge_index)  # pack x and edge_index into a tuple\n",
        "\n",
        "        data= self.gcn_in(data)\n",
        "        if self.gcn_hn is not None:\n",
        "            data= self.gcn_hn(data)  # nn.Sequential forwards only one element\n",
        "        out, _= self.gcn_out(data)\n",
        "\n",
        "        return F.log_softmax(out, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR2gI40qgl0x",
        "outputId": "78ecb6cb-94b8-4679-d3fb-8800d8d681b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 46119\n"
          ]
        }
      ],
      "source": [
        "model= GCN(input_dim=num_inputs, hidden_dim=[32,], output_dim=num_labels,\n",
        "           dropout=0.1).to(device)\n",
        "\n",
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model= GCN(input_dim=num_inputs, hidden_dim=[32,], output_dim=num_labels,\n",
        "           dropout=0.1, my_gcn=True).to(device)\n",
        "\n",
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpK4HZ2O3l66",
        "outputId": "e85c6948-3476-4f48-bb70-c9b0d8cba006"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 46119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAu8nHaK_B_Q"
      },
      "source": [
        "# Training procedure\n",
        "\n",
        "Traditional neural networks can be efficiently batched during training. For graph neural networks, it's harder to batch the data because nodes have different neighbors, resulting in potentially uneven mini-batches. Efficient sampling techniques (like GraphSAGE) or mini-batch training are necessary for scalability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "obq48nTaA2WR"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "# training procedure - we train 10 times and calculate the average accuracy and standard deviation\n",
        "def supervised_training(model_config, learning_rate=1e-3, epochs=500, eval_interval=50,\n",
        "                        batches=True, batch_size=128, verbose=False):\n",
        "\n",
        "    model_class= model_config.model_class\n",
        "    input_dim= model_config.input_dim\n",
        "    hidden_dim= model_config.hidden_dim\n",
        "    output_dim= model_config.output_dim\n",
        "    dropout= model_config.dropout\n",
        "\n",
        "    if batches:\n",
        "        epoch_size= math.floor(Xtr.shape[0]/ batch_size)\n",
        "    else:\n",
        "        batch_size= Xtr.shape[0]\n",
        "        epoch_size= 1\n",
        "\n",
        "    results= []\n",
        "    best_test_acc= 0.0\n",
        "\n",
        "    for i in tqdm(range(10)):\n",
        "        if verbose: print(f'Training {model_class.__name__} iteration {i+1}')\n",
        "\n",
        "        # create a fresh model for training\n",
        "        if model_class== MLP:\n",
        "            model_tr= MLP(input_dim, hidden_dim, output_dim, layer_norm=model_config.layer_norm,\n",
        "                          dropout=dropout).to(device)\n",
        "        elif model_class== GCN:\n",
        "            my_gcn= True if model_config.gcn_type=='my_gcn' else False\n",
        "            model_tr= GCN(input_dim, hidden_dim, output_dim, dropout=dropout, my_gcn=my_gcn).to(device)\n",
        "        elif model_class== GAT:\n",
        "            model_tr= GAT(input_dim, hidden_dim, output_dim, heads=model_config.heads,\n",
        "                          dropout=dropout).to(device)\n",
        "\n",
        "        # create a PyTorch optimizer\n",
        "        optimizer= torch.optim.AdamW(model_tr.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
        "\n",
        "        # loss function\n",
        "        class_weights= torch.bincount(data.y) / len(data.y)\n",
        "        loss_fn= nn.CrossEntropyLoss(weight=1/class_weights).to(device)\n",
        "\n",
        "        # --- training loop ---\n",
        "        for epoch in range(epochs):\n",
        "            # iterating over all batches\n",
        "            for i in range(epoch_size):\n",
        "                # --- minibatch construction ---\n",
        "                Xb= Xtr[(i * batch_size):((i+1) * batch_size)]\n",
        "                Yb= Ytr[(i * batch_size):((i+1) * batch_size)]\n",
        "\n",
        "                # --- forward pass ---\n",
        "                if isinstance(model_tr, MLP):\n",
        "                    y_pred= model_tr(Xb)\n",
        "                else:\n",
        "                    y_pred= model_tr(data.x, data.edge_index)[data.train_mask]\n",
        "                tr_loss= loss_fn(y_pred, Yb)\n",
        "\n",
        "                # --- backward pass ---\n",
        "                model_tr.train(True)\n",
        "                optimizer.zero_grad()\n",
        "                tr_loss.backward()\n",
        "\n",
        "                # --- update ---\n",
        "                optimizer.step()\n",
        "\n",
        "            # --- track stats ---\n",
        "            if epoch% eval_interval== 0:\n",
        "                model_tr.eval()\n",
        "                with torch.no_grad():\n",
        "                    if isinstance(model_tr, MLP):\n",
        "                        y_pred= model_tr(Xdev)\n",
        "                    else:\n",
        "                        y_pred= model_tr(data.x, data.edge_index)[data.val_mask]\n",
        "\n",
        "                    val_loss= loss_fn(y_pred, Ydev)\n",
        "                    val_acc= (y_pred.argmax(dim=1)== Ydev).sum().item()/ Ydev.shape[0]\n",
        "                    if verbose:\n",
        "                        print(f'Epoch {epoch} | Training Loss: {tr_loss.item():.4f} | Validation Loss: {val_loss.item():.4f} | Validation Acc: {val_acc:>5.2f}')\n",
        "\n",
        "        # --- final evaluation on the test set ---\n",
        "        model_tr.eval()\n",
        "        with torch.no_grad():\n",
        "            if isinstance(model_tr, MLP):\n",
        "                y_pred= model_tr(Xte)\n",
        "            else:\n",
        "                y_pred= model_tr(data.x, data.edge_index)[data.test_mask]\n",
        "\n",
        "            test_loss= loss_fn(y_pred, Yte)\n",
        "            test_acc= (y_pred.argmax(dim=1)== Yte).sum().item()/ Yte.shape[0]\n",
        "            if best_test_acc< test_acc:\n",
        "                best_model= copy.deepcopy(model_tr)\n",
        "            del model_tr\n",
        "\n",
        "            if verbose: print(f'{model_class.__name__} Test Loss: {test_loss.item():.2f} | Test Acc: {test_acc:>5.2f}')\n",
        "            results.append([val_acc, test_acc])\n",
        "\n",
        "    return best_model, torch.tensor(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MLP --- print average on test set and standard deviation\n",
        "@dataclass\n",
        "class MLPConfig:\n",
        "    model_class= MLP\n",
        "    input_dim= num_inputs\n",
        "    hidden_dim= [32,]\n",
        "    output_dim= num_labels\n",
        "    layer_norm= True\n",
        "    dropout= 0.1\n",
        "\n",
        "model, results= supervised_training(MLPConfig, learning_rate=0.01, epochs=1000, eval_interval=100)\n",
        "print(f'{model.__class__.__name__} - Test Accuracy: {100*results[:,1].mean():.2f} ± {100*results[:,1].std():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IApLH_u63zws",
        "outputId": "8990a1aa-0266-4d39-fabf-1efd47823558"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [03:11<00:00, 19.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP - Test Accuracy: 72.50 ± 1.21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual training is relatively simple. We pass our data into the model, get some output, and compare that output to our known output via cross-entropy loss. Under the hood, the hidden states of all nodes are being affected by message-passing neural networks to ultimately create a vector of 7 numbers, one for each possible class. If the biggest number in that vector is at index 0, then that vector predicts class 0. If the biggest number is in index 1, then the vector predicts class 1."
      ],
      "metadata": {
        "id": "tUPb24b4-rYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- GCN --- print average on test set and standard deviation\n",
        "@dataclass\n",
        "class GCNConfig:\n",
        "    model_class= GCN\n",
        "    gcn_type= 'gnn'\n",
        "    input_dim= num_inputs\n",
        "    hidden_dim= [32,]\n",
        "    output_dim= num_labels\n",
        "    dropout= 0.1\n",
        "\n",
        "model, results= supervised_training(GCNConfig, learning_rate=0.01, epochs=1000, eval_interval=100,\n",
        "                                    batches=False)\n",
        "print(f'{model.__class__.__name__} - Test Accuracy: {100*results[:,1].mean():.2f} ± {100*results[:,1].std():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu87HjPx6O7S",
        "outputId": "7ddf434f-ce0d-47cc-c9f2-3a3eb0384dd4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:33<00:00,  3.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN - Test Accuracy: 84.36 ± 0.58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- GCN --- print average on test set and standard deviation\n",
        "@dataclass\n",
        "class GCNConfig:\n",
        "    model_class= GCN\n",
        "    gcn_type= 'my_gcn'  # --- using the GCNConv module implemented from scratch ---\n",
        "    input_dim= num_inputs\n",
        "    hidden_dim= [32,]\n",
        "    output_dim= num_labels\n",
        "    dropout= 0.1\n",
        "\n",
        "model, results= supervised_training(GCNConfig, learning_rate=0.01, epochs=1000, eval_interval=100,\n",
        "                                    batches=False)\n",
        "print(f'{model.__class__.__name__} - Test Accuracy: {100*results[:,1].mean():.2f} ± {100*results[:,1].std():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz7K6MNz6Ov4",
        "outputId": "6aa26730-3dfe-48af-abc1-af16e381db20"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [06:49<00:00, 40.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN - Test Accuracy: 83.94 ± 0.46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1BiiWAP_PfZ"
      },
      "source": [
        "The graph structure should really make a difference for the problem we are trying to solve. The structure should be meaningful for the prediction task at hand. Testing is important here. We can try to formulate the graph in different ways to see if one way of formulating works better than another one.\n",
        "\n",
        "Training a graph neural network takes more time than training a normal neural network. So if the results improve only a little bit and training time is important, the normal neural network can be the best choice. Also, the effectiveness among types of graph neural networks (GCN, GAT, GraphSAGE) can vary greatly based on the problem.\n",
        "\n",
        "Just like in standard neural networks, transfer learning (pre-training a GNN on a large dataset and fine-tuning on the target dataset) can be effective for GNNs. Checking for available pre-trained models for our task can be valuable.\n",
        "\n",
        "As we've seen, simply adding graph information to a basic neural network can dramatically boost performance, as was the case when we moved from a normal neural network to a GCN for the Cora dataset. By aggregating information from neighboring nodes, GCNs can provide a richer representation of the data, leading to more accurate predictions. But, it's crucial to remember that GNNs aren't a magic bullet for every problem. The graph structure must be truly meaningful to the prediction task, and the increase in training complexity might not always justify the performance boost, especially when training time is critical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6GhxIdHBFgb"
      },
      "outputs": [],
      "source": [
        "# https://towardsdatascience.com/graph-neural-networks-part-1-graph-convolutional-networks-explained-9c6aaa8a406e\n",
        "# https://medium.com/intuitively-and-exhaustively-explained/graph-convolutional-networks-intuitively-and-exhaustively-explained-2f9afae0b040"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E2HQv76_pvF"
      },
      "source": [
        "# Graph Attention Network - GAT\n",
        "\n",
        "GCNs treat all neighbors equally. For GATs, this is different. GATs allow the model to learn different importance (attention) scores for different neighbors. They aggregate neighbor information by using attention mechanisms (this might ring a bell because these mechanisms are also used in transformers).\n",
        "\n",
        "In the GCN, we only looked at the degree of the nodes. GATs on the other hand, also take the feature values into account to assign attention scores to different neighbors. So instead of treating all neighbors equally, an attention mechanism is introduced that assigns varying levels of importance to different neighbors. This allows the network to focus on the most relevant parts of the graph structure, essentially learning \"where to look\" when making predictions.\n",
        "\n",
        "**Computing Attention Scores:** For each node, we calculate an attention score for every neighboring node. This score is a measure of how important a specific neighbor's features are when updating the current node's features (https://arxiv.org/pdf/1710.10903). The score is learned during training, so the model decides which nodes matter most for each task (https://arxiv.org/abs/2105.14491), most of the time this method is more effective.\n",
        "\n",
        "Just like transformers, GATs often use multi-head attention to improve their performance. Multi-head attention refers to running several separate attention mechanisms, or heads, in parallel. Each of these heads independently computes attention scores for the neighbors of a node, learning to focus on different aspects of the graph structure or node features. After these heads process the graph, their outputs are either concatenated or averaged to form the final node representation. So one of the key reasons of using multiple heads instead of one is to learn diverse patterns, because each attention head has its own learnable parameters and can learn to focus on different parts of the neighborhood. Another reason is that it stabilizes the training process. We can compare it with an ensemble, other heads can compensate for a \"noisy head\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "enthxQHiA2hx"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GATv2Conv\n",
        "\n",
        "class GATLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements one customizable GAT layer.\n",
        "    Input -> Dropout -> GATConv -> ELU -> Output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, heads, concat=True, activation=None,\n",
        "                 dropout=0.0) -> None:\n",
        "        super(GATLayer, self).__init__()\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout= nn.Dropout(p=dropout) if dropout> 0.0 else None\n",
        "        # GAT module\n",
        "        self.gat_layer= GATv2Conv(input_dim, output_dim, heads=heads, concat=concat)\n",
        "        # Activation function\n",
        "        self.activ= activation\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index= data\n",
        "\n",
        "        if self.dropout is not None:\n",
        "            x= self.dropout(x)\n",
        "        x= self.gat_layer(x, edge_index)\n",
        "        if self.activ is not None:\n",
        "            x= self.activ(x)\n",
        "\n",
        "        return x, edge_index\n",
        "\n",
        "\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a Graph Attention Network.\n",
        "    nn.ELU(inplace=True) is the default activation function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim=[16,], output_dim=1, heads=8,\n",
        "                 activation=None, dropout=0.0) -> None:\n",
        "        super(GAT, self).__init__()\n",
        "        # Activation function -- ELU is the default\n",
        "        activation= nn.ELU(inplace=True) if activation is None else activation\n",
        "\n",
        "        if isinstance(hidden_dim, int):\n",
        "            hidden_dim= [hidden_dim]\n",
        "        n_hidden_layers= len(hidden_dim)\n",
        "\n",
        "        if n_hidden_layers== 0:\n",
        "            raise Exception('hidden_dim cannot be an empty list')\n",
        "\n",
        "        self.gat_in= GATLayer(input_dim, hidden_dim[0], heads, concat=True,\n",
        "                              activation=activation, dropout=dropout)\n",
        "\n",
        "        if n_hidden_layers> 1:\n",
        "            self.gat_hn= nn.Sequential(*[\n",
        "                GATLayer((d * heads), hidden_dim[i+1], heads, concat=True,\n",
        "                         activation=activation, dropout=dropout)\n",
        "                for i, d in enumerate(hidden_dim[:-1])\n",
        "            ])\n",
        "        else:\n",
        "            self.gat_hn= None\n",
        "        # for the last GAT layer we use concat=False to average the outputs of the heads\n",
        "        self.gat_out= GATLayer((hidden_dim[-1] * heads), output_dim, heads, concat=False,\n",
        "                               activation=None, dropout=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        data= (x, edge_index)  # pack x and edge_index into a tuple\n",
        "\n",
        "        data= self.gat_in(data)\n",
        "        if self.gat_hn is not None:\n",
        "            data= self.gat_hn(data) # nn.Sequential forwards only one element\n",
        "        out, _= self.gat_out(data)\n",
        "\n",
        "        return F.log_softmax(out, dim=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSlF1NcXt_Zc"
      },
      "source": [
        "Each attention head computes its own set of attention scores and new node features independently. For $N$ heads, and a given node $i$, we'll end up with $N$ different sets of transformed features. Next up, all outputs are concatenated (stacked) or averaged. Concatenation is more common because it increases the model's expressiveness, but on the other hand the output dimension will be larger. Averaging helps to smooth out the differences between the heads. A general rule is to use concatenation when it's a hidden layer in the network and averaging when it's the last layer. When all attention heads are combined, we hope to get a comprehensive view of the graph, because the different heads have different perspectives on the relationships in the graph."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model= GAT(input_dim=num_inputs, hidden_dim=[32,], output_dim=num_labels,\n",
        "           dropout=0.1).to(device)\n",
        "\n",
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_mWNDcF_gWT",
        "outputId": "208e9d35-2c8f-41a3-ece4-8e946c623f59"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 763567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print average on test set and standard deviation\n",
        "@dataclass\n",
        "class GATConfig:\n",
        "    model_class= GAT\n",
        "    input_dim= num_inputs\n",
        "    hidden_dim= [32,]\n",
        "    output_dim= num_labels\n",
        "    heads= 8\n",
        "    dropout= 0.1\n",
        "\n",
        "model, results= supervised_training(GATConfig, learning_rate=0.01, epochs=1000, eval_interval=100,\n",
        "                                    batches=False)\n",
        "print(f'{model.__class__.__name__} - Test Accuracy: {100*results[:,1].mean():.2f} ± {100*results[:,1].std():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ajyl_JO_pfm",
        "outputId": "0bd5087b-4ba4-4a22-fc3d-3ad918e4c66f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:20<00:00,  8.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GAT - Test Accuracy: 85.96 ± 0.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21nURmxJN04c"
      },
      "source": [
        "The GAT model takes a bit longer than the GCN... The attention mechanism in GATs adds additional complexity to the model, both in terms of computation and the number of parameters. This makes GATs more resource-intensive and slower to train than GCNs.\n",
        "\n",
        "Multi-head attention helps stabilize training, but there is still a risk of overfitting, especially when using many attention heads or deep GAT architectures. Using techniques like dropout and early stopping can help to mitigate this.\n",
        "\n",
        "Many steps in finetuning GNNs are similar to traditional neural networks: testing different values for the hyperparameters and preventing overfitting with early stopping. For example with GATs we need to tune the number of attention heads. Small changes to node and edge features can have an impact on GNN performance, so it might help to experiment with different feature combinations or to create new features. Augmenting data can improve generalization. We can do this by adding noise to edges, randomly dropping nodes, or by performing subgraph sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u0VqjtdA2mD"
      },
      "outputs": [],
      "source": [
        "# https://towardsdatascience.com/graph-neural-networks-part-2-graph-attention-networks-vs-gcns-029efd7a1d92"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}