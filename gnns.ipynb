{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Neural Networks\n",
        "\n",
        "From MLPs to GCNs and GATs."
      ],
      "metadata": {
        "id": "TnvueT1u39I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://towardsdatascience.com/graph-neural-networks-part-1-graph-convolutional-networks-explained-9c6aaa8a406e"
      ],
      "metadata": {
        "id": "N6GhxIdHBFgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "E_N4RFLj4VBo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new environment with Poetry\n",
        "#!pip install poetry\n",
        "!poetry init --no-interaction\n",
        "!poetry add torch torchvision torchaudio torch-geometric matplotlib scikit-learn"
      ],
      "metadata": {
        "id": "Uw22t8wM5TuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Cora dataset is a benchmark dataset for graph neural networks. The dataset contains data about 2708 scientific publications. These publications are the nodes of the graph. An edge between nodes (publications) is created when a publication references the other one. The target is to predict the subject of each paper, there are seven classes in total."
      ],
      "metadata": {
        "id": "7ZKtmH6IAa_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch-geometric\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "dataset= Planetoid(root='.', name='Cora', force_reload=True)\n",
        "data= dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCfNS47i7Isc",
        "outputId": "de653a6a-1318-44f2-b8f2-9db7f93deb6a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network - MLP"
      ],
      "metadata": {
        "id": "IE0voRYS4PWK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "Hkgvp70V32ub"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Activation functions implemented: relu, tanh.\n",
        "\"\"\"\n",
        "\n",
        "class MLP_Hidden(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, layer_norm, activation, dropout) -> None:\n",
        "\n",
        "        super(MLP_Hidden, self).__init__()\n",
        "        self.fc_hn= nn.Linear(input_dim, output_dim)\n",
        "        self.norm= None\n",
        "        if layer_norm:\n",
        "            self.norm= nn.LayerNorm(output_dim)\n",
        "\n",
        "        if 'relu' in activation:\n",
        "            self.activ= nn.ReLU()\n",
        "        else:\n",
        "            self.activ= nn.Tanh()\n",
        "\n",
        "        self.dropout= None\n",
        "        if dropout> 0.0:\n",
        "            self.dropout= nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.fc_hn(x)\n",
        "        if self.norm is not None:\n",
        "            x= self.norm(x)\n",
        "        x= self.activ(x)\n",
        "        if self.dropout is not None:\n",
        "            x= self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim=[16,], output_dim=1, layer_norm=False,\n",
        "                 activation='relu', dropout=0.0) -> None:\n",
        "        \"\"\"\n",
        "        Implements a customizable MLP.\n",
        "        \"\"\"\n",
        "\n",
        "        super(MLP, self).__init__()\n",
        "        if isinstance(hidden_dim, int):\n",
        "            hidden_dim= [hidden_dim]\n",
        "        n_hidden_layers= len(hidden_dim)\n",
        "\n",
        "        if n_hidden_layers== 0:\n",
        "            raise Exception('hidden_dim cannot be an empty list')\n",
        "\n",
        "        self.fc_hnin= MLP_Hidden(input_dim, hidden_dim[0], layer_norm, activation, dropout)\n",
        "\n",
        "        if n_hidden_layers> 1:\n",
        "            self.fc_hn= nn.Sequential(*[\n",
        "                MLP_Hidden(d, hidden_dim[i+1], layer_norm, activation, dropout)\n",
        "                for i, d in enumerate(hidden_dim[:-1])\n",
        "            ])\n",
        "        else: self.fc_hn= None\n",
        "\n",
        "        self.fc_hnout= nn.Linear(hidden_dim[-1], output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):  # no graph structure, only node features\n",
        "        x= self.fc_hnin(x)\n",
        "        if self.fc_hn is not None:\n",
        "            x= self.fc_hn(x)\n",
        "        x= self.fc_hnout(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_size= data.x.shape[0]\n",
        "dev_size= 500\n",
        "test_size= 500\n",
        "train_size= data_size - dev_size - test_size\n",
        "\n",
        "train_mask= torch.tensor([i< train_size for i in range(data_size)])\n",
        "dev_mask= torch.tensor([i>= train_size and i< (data_size - test_size) for i in range(data_size)])\n",
        "test_mask= torch.tensor([i>= (train_size + dev_size) for i in range(data_size)])\n",
        "\n",
        "data.train_mask= train_mask\n",
        "data.val_mask= dev_mask\n",
        "data.test_mask= test_mask"
      ],
      "metadata": {
        "id": "_efh5OV4OrJQ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "data= data.to(device)\n",
        "\n",
        "Xtr, Ytr= data.x[data.train_mask], data.y[data.train_mask]\n",
        "Xdev, Ydev= data.x[data.val_mask], data.y[data.val_mask]\n",
        "Xte, Yte= data.x[data.test_mask], data.y[data.test_mask]\n",
        "\n",
        "edge_idx= data.edge_index\n",
        "\n",
        "num_inputs= data.x.shape[1]          # used for input_dim\n",
        "num_labels= len(set(data.y.numpy())) # used for output_dim"
      ],
      "metadata": {
        "id": "sCXwB8WLA2PV"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= MLP(input_dim=num_inputs, hidden_dim=[32,], output_dim=num_labels,\n",
        "           layer_norm=True, dropout=0.1).to(device)\n",
        "\n",
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSF9E-O5A2Sx",
        "outputId": "ae84a978-a51f-44c2-96d8-a96b3e753eff"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 46183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Convolutional Network - GCN\n",
        "\n",
        "There are three common types of prediction tasks in graphs:\n",
        "- You can predict on graph level. The input of the model is many different graphs, and every graph gets one classification. For example the class a molecule belongs to: every molecule is represented by one graph, and every molecule needs a prediction. Another example is image classification. Yes, images can also be represented as graphs!\n",
        "- Another way to use GNNs is by predicting on node level. The input of the GNN is one graph, and every node needs a prediction. This prediction is a characteristic of the node. Node regression is of course possible as well. Compared to classification, you only need to change the output layer activation function, the loss function, evaluation metric, and obviously the target.\n",
        "- Finally, we can predict on edge level. The value of an edge is predicted, or the likelihood of an edge that will appear soon. An example is recommended friends on social media (a.k.a. link prediction).\n",
        "\n",
        "For understanding one node, we need to look at its neighborhood and include that information in the GNN.\n",
        "\n",
        "There is one important step we should take before actually implementing a GNN, and that is normalization. Imagine, without normalization, nodes with more connections (e.g. one node having 10 neighbors vs. another with just 1) can dominate the learning process. The node with 10 neighbors would aggregate far more information than the one with 1, leading to imbalance and unstable learning. Normalization ensures that each node's contribution is appropriately scaled, so the network learns from the graph structure rather than being skewed by uneven data distribution.\n",
        "\n",
        "In GNNs it's common to use symmetric normalization. The idea is to normalize each node's aggregated features by the square root of its degree (the number of neighbors, including itself for self-loops). This helps to ensure that nodes with different degrees contribute equally during aggregation."
      ],
      "metadata": {
        "id": "Y5xEhMmpVsxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric.nn as gnn\n",
        "\n",
        "class GCN_Hidden(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, activation) -> None:\n",
        "        super(GCN_Hidden, self).__init__()\n",
        "        self.conv_hn= gnn.GCNConv(input_dim, output_dim)\n",
        "        if 'relu' in activation:\n",
        "            self.activ= nn.ReLU()\n",
        "        else:\n",
        "            self.activ= nn.Tanh()\n",
        "\n",
        "\n",
        "    def forward(self, x, e):\n",
        "        x= self.conv_hn(x, e)\n",
        "        x= self.activ(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementing a Graph Convolutional Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim=[16,], output_dim=1, layer_norm=False,\n",
        "                 activation='relu', dropout=0.0) -> None:\n",
        "        super(GCN, self).__init__()\n",
        "        if isinstance(hidden_dim, int):\n",
        "            hidden_dim= [hidden_dim]\n",
        "        n_hidden_layers= len(hidden_dim)\n",
        "\n",
        "        if n_hidden_layers== 0:\n",
        "            raise Exception('hidden_dim cannot be an empty list')\n",
        "\n",
        "        self.conv_hnin= GCN_Hidden(input_dim, hidden_dim[0], activation)\n",
        "\n",
        "        if n_hidden_layers> 1:\n",
        "            self.conv_hn= nn.Sequential(*[\n",
        "                GCN_Hidden(d, hidden_dim[i+1], activation) for i, d in enumerate(hidden_dim[:-1])\n",
        "            ])\n",
        "        else:\n",
        "            self.conv_hn= None\n",
        "\n",
        "        self.conv_hnout= gnn.GCNConv(hidden_dim[-1], output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x, e):\n",
        "        x= self.conv_hnin(x, e)\n",
        "        if self.conv_hn is not None:\n",
        "            x= self.conv_hn(x, e)\n",
        "        x= self.conv_hnout(x, e)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "FeDfgo0LNitv"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= GCN(input_dim=num_inputs, hidden_dim=[32,], output_dim=num_labels).to(device)\n",
        "\n",
        "total_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of parameters: {total_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR2gI40qgl0x",
        "outputId": "c935615a-6af6-4b7b-9c82-91734be8bc96"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 46119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traditional neural networks can be efficiently batched during training. For graph neural networks, it's harder to batch the data because nodes have different neighbors, resulting in potentially uneven mini-batches. Efficient sampling techniques (like GraphSAGE) or mini-batch training are necessary for scalability."
      ],
      "metadata": {
        "id": "rAu8nHaK_B_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training procedure - we train 10 times and calculate the average accuracy and standard deviation\n",
        "\n",
        "def supervised_training(model_class, learning_rate=1e-3, epochs=500, eval_interval=50, batches=True,\n",
        "                        verbose=False):\n",
        "    if batches:\n",
        "        batch_size= 100\n",
        "        epoch_size= round(Xtr.shape[0]/ batch_size)-1\n",
        "    else:\n",
        "        batch_size= Xtr.shape[0]\n",
        "        epoch_size= 1\n",
        "\n",
        "    results= []\n",
        "\n",
        "    for i in tqdm(range(10)):\n",
        "        if verbose: print(f'Training {model_class} iteration {i+1}')\n",
        "\n",
        "        model= model_class(input_dim=num_inputs, hidden_dim=[32,], output_dim=num_labels,\n",
        "                           layer_norm=True, dropout=0.1).to(device)\n",
        "\n",
        "        # create a PyTorch optimizer\n",
        "        optimizer= torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
        "\n",
        "        # loss function\n",
        "        class_weights= torch.bincount(data.y) / len(data.y)\n",
        "        loss_fn= nn.CrossEntropyLoss(weight=1/class_weights).to(device)\n",
        "\n",
        "        # training loop\n",
        "        for epoch in range(epochs):\n",
        "            # iterating over all batches\n",
        "            for i in range(epoch_size):\n",
        "                # --- minibatch construction ---\n",
        "                Xb= Xtr[(i * batch_size):((i+1) * batch_size)]\n",
        "                Yb= Ytr[(i * batch_size):((i+1) * batch_size)]\n",
        "\n",
        "                # --- forward pass ---\n",
        "                if isinstance(model, MLP):\n",
        "                    y_pred= model(Xb)\n",
        "                elif isinstance(model, GCN):\n",
        "                    y_pred= model(data.x, data.edge_index)[data.train_mask]\n",
        "                tr_loss= loss_fn(y_pred, Yb)\n",
        "\n",
        "                # --- backward pass ---\n",
        "                model.train(True)\n",
        "                optimizer.zero_grad()\n",
        "                tr_loss.backward()\n",
        "\n",
        "                # --- update ---\n",
        "                optimizer.step()\n",
        "\n",
        "            # --- track stats ---\n",
        "            if epoch% eval_interval== 0:\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    if isinstance(model, MLP):\n",
        "                        y_pred= model(Xdev)\n",
        "                    elif isinstance(model, GCN):\n",
        "                        y_pred= model(data.x, data.edge_index)[data.val_mask]\n",
        "\n",
        "                    val_loss= loss_fn(y_pred, Ydev)\n",
        "                    val_acc= (y_pred.argmax(dim=1)== Ydev).sum().item()/ Ydev.shape[0]\n",
        "                    if verbose:\n",
        "                        print(f'Epoch {epoch} | Training Loss: {tr_loss.item():.2f} | Validation Loss: {val_loss.item():.2f} | Validation Acc: {val_acc:>5.2f}')\n",
        "\n",
        "        # final evaluation on the test set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            if isinstance(model, MLP):\n",
        "                y_pred= model(Xte)\n",
        "            elif isinstance(model, GCN):\n",
        "                y_pred= model(data.x, data.edge_index)[data.test_mask]\n",
        "\n",
        "            test_loss= loss_fn(y_pred, Yte)\n",
        "            test_acc= (y_pred.argmax(dim=1)== Yte).sum().item()/ Yte.shape[0]\n",
        "            if verbose: print(f'{model_class} Test Loss: {test_loss.item():.2f} | Test Acc: {test_acc:>5.2f}')\n",
        "            results.append([val_acc, test_acc])\n",
        "\n",
        "    return torch.tensor(results)\n"
      ],
      "metadata": {
        "id": "obq48nTaA2WR"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print average on test set and standard deviation\n",
        "results= supervised_training(MLP, learning_rate=0.01, epochs=1000, eval_interval=100)\n",
        "print(f'MLP - Test Accuracy: {100*results[:,1].mean():.2f} ± {100*results[:,1].std():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paAiISaoH0j2",
        "outputId": "2268bc74-402c-47bc-ca5d-90244d747cfe"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [05:56<00:00, 35.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP - Test Accuracy: 71.42 ± 1.48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print average on test set and standard deviation\n",
        "results= supervised_training(GCN, learning_rate=0.01, epochs=1000, eval_interval=100, batches=False)\n",
        "print(f'GCN - Test Accuracy: {100*results[:,1].mean():.2f} ± {100*results[:,1].std():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UylKZQGrA2b-",
        "outputId": "ff7eb9f2-29cd-49eb-b156-fe3dd92af2e1"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [03:55<00:00, 23.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN - Test Accuracy: 87.58 ± 0.18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph structure should really make a difference for the problem you are trying to solve. The structure should be meaningful for the prediction task at hand. Testing is important here. You can try to formulate the graph in different ways to see if one way of formulating works better than another one.\n",
        "\n",
        "Training a graph neural network takes more time than training a normal neural network. So if the results improve only a little bit and training time is important, the normal neural network can be the best choice. Also, the effectiveness among types of graph neural networks (GCN, GAT, GraphSAGE) can vary greatly based on the problem.\n",
        "\n",
        "Just like in standard neural networks, transfer learning (pre-training a GNN on a large dataset and fine-tuning on the target dataset) can be effective for GNNs. Checking for available pre-trained models for your task can be valuable.\n",
        "\n",
        "As we've seen, simply adding graph information to a basic neural network can dramatically boost performance, as was the case when we moved from a normal neural network to a GCN for the Cora dataset. By aggregating information from neighboring nodes, GCNs can provide a richer representation of the data, leading to more accurate predictions. But, it's crucial to remember that GNNs aren't a magic bullet for every problem. The graph structure must be truly meaningful to the prediction task, and the increase in training complexity might not always justify the performance boost, especially when training time is critical."
      ],
      "metadata": {
        "id": "d1BiiWAP_PfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Attention Network - GAT"
      ],
      "metadata": {
        "id": "-E2HQv76_pvF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "enthxQHiA2hx"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5u0VqjtdA2mD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}