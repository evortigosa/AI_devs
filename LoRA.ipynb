{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Low-Rank Adaptation (LoRA) - a type of parameter efficient fine tuning (PEFT)"
      ],
      "metadata": {
        "id": "S4XCPVAX9U0F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gR9KniwGk8DS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "bitsandbytes: for representing models using smaller datatypes, saving on memory\n",
        "datasets: for downloading datasets\n",
        "accelerate: required dependency for machine learning interoperability for some of the modules\n",
        "loralib: LoRA implementation\n",
        "peft: a general \"parameter efficient fine tuning\" module, our interface for LoRA\n",
        "transformers: for downloading and using pre-trained transformers from huggingface\n",
        "\"\"\"\n",
        "\n",
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "!pip install transformers -U"
      ],
      "metadata": {
        "id": "YKwiKnCNlAK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing dependencies and downloading pre-trained bloom model\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "# loading model\n",
        "model= AutoModelForCausalLM.from_pretrained(\n",
        "    #\"bigscience/bloom-3b\",\n",
        "    #\"bigscience/bloom-1b1\",\n",
        "    \"bigscience/bloom-560m\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "# loading tokenizer for the model (which turns text into an input for the model)\n",
        "tokenizer= AutoTokenizer.from_pretrained(\"bigscience/tokenizer\")"
      ],
      "metadata": {
        "id": "05GpXBCClAOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "r: the rank of the A and B matrices\n",
        "lora_alpha: this is a pretty controversial parameter. A lot of people have a lot of ideas about it.\n",
        "  You can consider it a scaling factor, and by default it should be equal to r, as far as I understand.\n",
        "target_modules: the portions of the model we want to optimize with LoRA. The BLOOM module has\n",
        "  parameters named query_key_value which we want to optimize.\n",
        "lora_dropout: dropout is a technique which hides inputs to suppress the model from overfitting\n",
        "  (called regularization). This is a probability of being hidden.\n",
        "bias: neural networks typically have two paramet per connection, a \"weight\" and a \"bias\".\n",
        "  We're only training weights in this example.\n",
        "task_type: not super necessary, used in the superclass PeftConfig. Setting to CAUSAL_LM because the\n",
        "  specific language model we're using is \"causal\".\n",
        "\"\"\"\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# defining how LoRA will work in this particular example\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=8,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# this actually overwrites the model in memory, so the rename is only for legibility\n",
        "peft_model= get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "s9_PTirglARE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comparing parameters before and after LoRA\n",
        "\n",
        "all_param= 0\n",
        "trainable_params= 0\n",
        "\n",
        "# iterating over all parameters\n",
        "for _, param in peft_model.named_parameters():\n",
        "\n",
        "    # adding parameters to total\n",
        "    all_param += param.numel()\n",
        "\n",
        "    # adding parameters to trainable if they require a gradient\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "\n",
        "\n",
        "# printing results\n",
        "print(f'All params: {all_param}')\n",
        "print(f'Trainable params: {trainable_params}')\n",
        "print(f'Trainable params rate: {100 * trainable_params/all_param:.2f}%')"
      ],
      "metadata": {
        "id": "78pIUh7ZlATy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c75b6b98-a3cc-4504-c659-1ac9de50a84c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All params: 560001024\n",
            "Trainable params: 786432\n",
            "Trainable params rate: 0.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading Fine Tuning Dataset - SQUAD\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "qa_dataset= load_dataset('squad_v2')"
      ],
      "metadata": {
        "id": "iyyrQhV_lAZ3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The model will expect text in this general form:\n",
        "\n",
        "**CONTEXT:**\n",
        "{context}\n",
        "\n",
        "**QUESTION:**\n",
        "{question}\n",
        "\n",
        "**ANSWER:**\n",
        "{answer}</s>\n",
        "\"\"\"\n",
        "\n",
        "# reformatting SQUAD to respect our defined structure\n",
        "# defining a function for reformatting\n",
        "def create_prompt(context, question, answer):\n",
        "\n",
        "    if len(answer['text']) < 1:\n",
        "        answer= 'Cannot find answer'\n",
        "    else:\n",
        "        answer= answer['text'][0]\n",
        "\n",
        "    prompt_template= f'CONTEXT:\\n{context}\\n\\nQUESTION:\\n{question}\\n\\nANSWER:\\n{answer}</s>'\n",
        "\n",
        "    return prompt_template\n",
        "\n",
        "\n",
        "# applying the reformatting function to the entire dataset\n",
        "mapped_qa_dataset= qa_dataset.map(lambda samples: tokenizer(create_prompt(\n",
        "    samples['context'], samples['question'], samples['answers']\n",
        ")))"
      ],
      "metadata": {
        "id": "14QiR0G9lAc-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Fine Tuning on SQUAD using LoRA\n",
        "This code is largly co-opted. In the absence of a rigid validation procedure, the best practice is\n",
        "to just copy a successful tutorial or, better yet, directly from the documentation.\n",
        "\"\"\"\n",
        "\n",
        "import transformers\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=mapped_qa_dataset[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=100,\n",
        "        max_steps=100,\n",
        "        learning_rate=1e-3,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir='outputs',\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "# silence the warnings. Please re-enable for inference!\n",
        "peft_model.config.use_cache= False\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "jHcNbDyllAg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the LoRA fine tuning locally\n",
        "\n",
        "model_id= 'BLOOM-560m-LoRA'\n",
        "peft_model.save_pretrained(model_id)\n",
        "\n",
        "# checking how large the file is in our file system\n",
        "!ls -lh {model_id}"
      ],
      "metadata": {
        "id": "RGtRTpbQ0E3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The BLOOM 560m model, in it's float 16 datatype, is over 1 gigabyte in total size. With LoRA, and\n",
        "us only needing to save the decomposed matrices, our checkpoint size is a mere 3 megabytes.\n",
        "\"\"\"\n",
        "\n",
        "# Testing - Helper Function for Comparing Results\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def make_inference(context, question):\n",
        "\n",
        "    # turn the input into tokens\n",
        "    batch= tokenizer(f'**CONTEXT:**\\n{context}\\n\\n**QUESTION:**\\n{question}\\n\\n**ANSWER:**\\n',\n",
        "                     return_tensors='pt', return_token_type_ids=False)\n",
        "    # move the tokens onto the GPU for inference\n",
        "    batch= batch.to(device='cuda')\n",
        "\n",
        "    # make an inference with both the fine tuned model and the raw model\n",
        "    with torch.cuda.amp.autocast():\n",
        "        # I think inference time would be faster if these were applied, but the fact that LoRA\n",
        "        # is not applied allows me to experiment with before and after fine tuning simultaneously\n",
        "\n",
        "        # raw model\n",
        "        peft_model.disable_adapter_layers()\n",
        "        output_tokens_raw= model.generate(**batch, max_new_tokens=200)\n",
        "\n",
        "        # LoRA model\n",
        "        peft_model.enable_adapter_layers()\n",
        "        output_tokens_qa= peft_model.generate(**batchm max_new_tokens=200)\n",
        "\n",
        "\n",
        "    # display results\n",
        "    display(Markdown('# Raw Model\\n'))\n",
        "    display(Markdown((tokenizer.decode(output_tokens_raw[0], skip_special_tokens=True))))\n",
        "    display(Markdown('\\n# QA Model\\n'))\n",
        "    display(Markdown((tokenizer.decoder(output_tokens_qa[0], skip_special_tokens=True))))"
      ],
      "metadata": {
        "id": "yfqcuI7V0FHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = 'You are a monster, and you eat yellow legos.'\n",
        "question= 'What is the best food?'\n",
        "\n",
        "make_inference(context, question)"
      ],
      "metadata": {
        "id": "_hnpHyml0FRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = 'you are a math wizard'\n",
        "question= 'what is 1+1 equal to?'\n",
        "\n",
        "make_inference(context, question)"
      ],
      "metadata": {
        "id": "c9ul_gtE0FVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = 'Answer the riddle'\n",
        "question= 'What gets bigger the more you take away?'\n",
        "\n",
        "make_inference(context, question)"
      ],
      "metadata": {
        "id": "dWF_TIvV7EuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the fine-tuned model failed to answer the question significantly more elegantly."
      ],
      "metadata": {
        "id": "CArTJRq37E40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://towardsdatascience.com/lora-intuitively-and-exhaustively-explained-e944a6bff46b"
      ],
      "metadata": {
        "id": "MfLcpofG7FB4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}