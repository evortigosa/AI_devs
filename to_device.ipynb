{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch .to(device)"
      ],
      "metadata": {
        "id": "6NZOCQLqrXB5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HUf7FXmDrLya"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your device - GPU if available, else CPU\n",
        "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWp0d3QRrc0A",
        "outputId": "a31ef314-f06f-41fa-b305-ea812cc4bd2d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Optimization: Efficiently Moving Data Between CPU and GPU\n",
        "\n",
        "You might be wondering if there's a way to speed up data transfer. Here's the answer: non-blocking transfers can make a difference in performance, especially when loading large batches of data from CPU to GPU.\n",
        "\n",
        "The non_blocking=True argument allows you to load data asynchronously, minimizing data-loading bottlenecks."
      ],
      "metadata": {
        "id": "QXM3HEu6sYsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a large tensor\n",
        "data= torch.randn(10000, 128)"
      ],
      "metadata": {
        "id": "5_1SNwLorc2r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Moving data asynchronously\n",
        "dataset= data.to(device, non_blocking=True)"
      ],
      "metadata": {
        "id": "3OH_OEUerc5h"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consider a model with multiple layers\n",
        "class MyModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.fc1= torch.nn.Linear(128, 256)\n",
        "        self.activ= nn.ReLU()\n",
        "        self.fc2= torch.nn.Linear(256, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.fc1(x)\n",
        "        x= self.activ(x)\n",
        "        x= self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Moving a model to the dynamically selected device\n",
        "model= MyModel().to(device)"
      ],
      "metadata": {
        "id": "vmuMAaxnrc-4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensuring data and model are on the same device\n",
        "assert data.device== model.fc1.weight.device, \"Data and model are on different devices!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "08QlmCqitkP_",
        "outputId": "7614e5ec-dc49-4292-f718-fdbbdce2e196"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Data and model are on different devices!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-020331e4d7c4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Ensuring data and model are on the same device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Data and model are on different devices!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: Data and model are on different devices!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensuring data and model are on the same device\n",
        "assert dataset.device== model.fc1.weight.device, \"Data and model are on different devices!\""
      ],
      "metadata": {
        "id": "AcWOG0CB3qom"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check devices of model parameters\n",
        "for param in model.parameters():\n",
        "    print(param.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA3pfdfHrdEJ",
        "outputId": "c4ee16ed-f3e2-42f8-bd97-1d09493c3469"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define DataLoader\n",
        "dataloader= DataLoader(data, batch_size=64)\n",
        "\n",
        "# Within your training loop\n",
        "for Xb in dataloader:\n",
        "    Xb= Xb.to(device, non_blocking=True)\n",
        "    # Continue with training steps"
      ],
      "metadata": {
        "id": "5258h3SGrc8P"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Applications and Case Study"
      ],
      "metadata": {
        "id": "kNSZvyjAvqED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1= nn.Conv2d(3, 32, kernel_size=3)\n",
        "        self.activ= nn.ReLU()\n",
        "        self.fc1= nn.Linear(28800, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.activ(self.conv1(x))\n",
        "        x= torch.flatten(x, 1)\n",
        "\n",
        "        return self.fc1(x)\n",
        "\n",
        "\n",
        "model= SimpleCNN().to(device)\n",
        "optimizer= optim.AdamW(model.parameters(), lr=0.001)\n",
        "criterion= nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "3_2pMZfT1fbo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's put it all together with a real-world example. Let's walk through how to efficiently manage device transfers in an image classification task on CIFAR-10, focusing on .to(device) best practices."
      ],
      "metadata": {
        "id": "OnnMILSi1kaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "# Set up transformations and load CIFAR-10\n",
        "transform= transforms.Compose([\n",
        "    transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset= datasets.CIFAR10(\n",
        "    root='./data', train=True, transform=transform, download=True\n",
        ")\n",
        "\n",
        "# Define DataLoader with pin_memory\n",
        "train_loader= DataLoader(\n",
        "    train_dataset, batch_size=64, shuffle=True, pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3gNsNvYv84u",
        "outputId": "2fb77667-0647-4c78-b4b6-c1852c15984a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:18<00:00, 9.07MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By adding non_blocking=True, you enable asynchronous operations, reducing the time your code spends waiting for data transfers. This approach works particularly well when used alongside pin_memory=True in DataLoaders.\n",
        "\n",
        "Pinning memory can speed up data transfer from CPU to GPU, as it allows the data to be directly accessed by the GPU."
      ],
      "metadata": {
        "id": "q5HpyhFY1cLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Move data and target to the GPU\n",
        "        data, target= data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        output= model(data)\n",
        "        loss= criterion(output, target)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYz4kJLsrdHT",
        "outputId": "25fd57e2-2b5e-4468-8dc3-9fb3758900ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.8261\n",
            "Epoch 2, Loss: 0.4843\n",
            "Epoch 3, Loss: 0.9790\n",
            "Epoch 4, Loss: 0.7607\n",
            "Epoch 5, Loss: 0.9187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# When to Use .detach().cpu()\n",
        "\n",
        "Sometimes, you need to retrieve data from the GPU for logging, monitoring, or evaluation without affecting the original computation graph. That's where .detach().cpu() comes in handy.\n",
        "\n",
        "Using .detach().cpu() lets you safely move data back to the CPU without altering the model's gradients or creating unnecessary device conflicts. This approach is ideal for device-agnostic code since it keeps data accessible on the CPU, making it easier to work with in mixed device environments."
      ],
      "metadata": {
        "id": "aOX84Tn0yDI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move tensor to CPU for inspection without affecting gradients\n",
        "tensor= torch.randn(10, requires_grad=True).to(device)\n",
        "cpu_tensor= tensor.detach().cpu()\n",
        "print(cpu_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ik4d42lx9YI",
        "outputId": "5d2d397a-3952-4c16-e665-26adb4b9d107"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.3866, -1.2000, -1.2601,  0.6263, -0.4033, -0.4472,  0.6083, -0.7221,\n",
            "        -0.0756,  0.2771])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://medium.com/biased-algorithms/mastering-pytorch-to-device-an-advanced-guide-for-efficient-device-management-0290b086f17e"
      ],
      "metadata": {
        "id": "Hj5LfXpOyJdq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}